\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[T1]{fontenc}
\usepackage[polish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows}
\usepackage{hyperref}
\usepackage{float}
\usepackage{changepage}
\usepackage{pdflscape}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{array}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{algorithmic}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}
\usetikzlibrary{shapes,arrows}
\selectlanguage{polish}

\author{Aleksander Sas}
\title{Szkic artykułu}
\frenchspacing

\newcolumntype{K}[1]{>{\centering\arraybackslash}p{#1}}
\DeclareMathOperator*{\argmax}{\arg\max}   % rbp
\newcounter{BlockCounter}
\newcommand{\labelBlock}[1]{%
	\smash{\raisebox{15pt}{\refstepcounter{BlockCounter}\hypertarget{#1}{}\label{#1}}}%
	(\theBlockCounter)%
}
\newcommand{\refBlock}[1]{%
	\hyperref[#1]{\ref*{#1}}% (see p. 18 of the hyperref manual)
}

\begin{document}

% Define block styles
\tikzstyle{ArmBlok} = [rectangle, draw, fill=blue!20, text width=10em, text centered, rounded corners, minimum height=4em]
\tikzstyle{word} = [rectangle, draw, text width=10em, text centered, rounded corners, minimum height=4em]
\tikzstyle{ArmBlok2} = [rectangle, draw, fill=blue!20, text width=10em, text centered, rounded corners, minimum height=4em, node distance=5cm]
\tikzstyle{path}=[circle,thick,fill=gray!0,minimum size=1mm]
\tikzstyle{hmm0}=[circle,thick,draw=gray!50,fill=gray!13,minimum size=4mm]
\tikzstyle{hmm}=[circle,thick,draw=gray!75,fill=gray!20,minimum size=6mm]
\tikzstyle{hmm2}=[circle,thick,draw=gray!75,fill=gray!20,minimum size=10mm]
\tikzstyle{model} = [ellipse, draw, fill=blue!20, text width=6em, text centered, rounded corners, minimum height=4em, node distance=5cm]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{data} = [draw, ellipse,fill=red!20, node distance=2cm,minimum height=2em]
\tikzstyle{frame} = [rectangle, draw, text centered, rounded corners, minimum height=0.7em, minimum width=4em, rotate=90,anchor=north]
\maketitle
\tableofcontents

\section{Wprowadzenie - opis problemu}
	\textbf{Automatyczne rozpoznawanie mowy}, w skrócie \textbf{ARM}, polega na rozpoznaniu i zapisaniu nagranych słów. Jest to coraz częściej i powszechniej stosowana technologia, która znajduje szerokie zastosowanie w motoryzacji, urządzeniach mobilnych, administracji państwowej i medycznej. Jest szczególnie przydatna przy sporządzani opisów, tekstowych i protokołowaniu wszelkiego rodzaju obrad. Systemy do rozpoznawanie mowy działają na dwa sposoby
	\begin{itemize}
		\item offline, w którym system wczytuje zbiór nagrań dźwiękowych i dla każdego z nich generuje transkrypcję
		\item online, w którym system na bieżąco rejestruję dźwięk i wypisuje transkrypcję.
	\end{itemize}
	Oba tryby wymagają możliwie najlepszej skuteczności rozpoznawania, jednak tryb online jest bardziej wymagający, gdyż dodatkowo wymaga szybkiego działania. Rozpoznawanie w trybie online powinno się odbywać w czasie zbliżony do rzeczywistego. 
	\\
	Rozwój kart graficznych i pojawienie się technologii CUDA zaowocowało w ostatnim dziesięcioleciu szybkim rozwojem sieci neuronowych, które w wielu problemach klasyfikacji radzą sobie znacznie lepiej niż wcześniej stosowanie algorytmy. Coraz więcej badaczy próbuje stosować sieci neuronowe w celu poprawy jakości systemów rozpoznających mowę. Można spotkać wiele różnych konfiguracji, zarówno łączących konwencjonalne systemy, jaki i autonomiczne sieci rekurencyjne.
	\\
	Celem niniejszej pracy magisterskiej jest przetestowanie możliwości wykorzystania sieci neuronowych w procesie rozpoznawania mowy. Duży nacisk położony jest na możliwość rozpoznawanie trifonów z ograniczonym kontekstem, co jest moim rozwinięciem podejścia spotykanego w wielu publikacjach. W pracy zbadano wpływ liczby rozpoznawanych trifonów na skuteczność rozpoznawania mowy.
	
\section{Omówienie istniejących podejść}
	Klasyczne podejście do rozpoznawania mowy, opierające się na wykorzystaniu ukrytych modli markowa oraz wielowymiarowych mikstur gaussowskich, zostało opisane w \cite{book1}. 
	\\
	Autorzy \cite{article1} wykorzystali konwolucyjne sieci neuronowe oraz bramki LSTM do modelowania fonów. W tym podejściu, wyjście sieci rozpoznającej wszystkie trifony jest traktowane, jako wejście do modelu Markova. 
	
	Tutaj znajdzie się krótki spis książek i publikacji dotyczących podobnych zagadnień.

\section{Proces automatycznego rozpoznawania mowy (ARM) }

	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[node distance = 2cm, auto]
		% Place nodes
		\node [data] (etap0) {Mowa};
		\node [ArmBlok,  below of=etap0] (etap1) {\labelBlock{zbieranie_sygnalu} sygnału akustycznego};
		\node [ArmBlok,  below of=etap1] (etap2) {\labelBlock{ekstrakcja_cech} Ekstrakcja cech};
		\node [ArmBlok,  below of=etap2] (etap3) {\labelBlock{klasyfikator} Klasyfikacja stanów};
		\node [ArmBlok,  below of=etap3] (etap4) {\labelBlock{lancuch_markova} Wyszukiwanie najlepszej ścieżki w modelu Markova};
		\node [ArmBlok2, right of=etap3] (etap5) {\labelBlock{n_gramy} Estymacja prawdopowobieństw słów};
		\node [model,    right of=etap2] (model_jezykowy) {Model językowy};
		\node [model,    left  of=etap2] (model_akustyczny) {\labelBlock{model_akustyczny} Model językowy};
		\node [data,     below of=etap4] (etap6) {Rozpoznanie};
		\node [model,   right  of=etap6] (slownik) {\labelBlock{slownik} Słownik};
		
		\path [line] (etap0) -- (etap1);
		\path [line] (etap1) -- (etap2);
		\path [line] (etap2) -- (etap3);
		\path [line] (etap3) -- (etap4);
		\path [line] (etap5) |- (etap4);
		\path [line] (etap4) -- (etap6);
		\path [line, dashed] (model_jezykowy) -- (etap5);
		\path [line, dashed] (slownik) |- (etap4);
		\path [line, dashed] (model_akustyczny) |- (etap3);
		\path [line, dashed] (model_akustyczny) |- (etap4);
		
		\end{tikzpicture}
		\caption{Etapy automatycznego rozpoznawania mowy}
		\label{fig:ARM_schemat}
	\end{figure}

	\textbf{Automatyczne rozpoznawanie mowy} możemy formalnie zdefiniować, jako znajdowanie ciągu słów $\hat{W}$ nad pewnym alfabetem $\Sigma$, o maksymalnym prawdopodobieństwie, pod warunkiem obserwacji $O$.
	
	\begin{equation}
		\hat{W}=\argmax_{W \in \Sigma^{*}}{P(W \mid O)}
		\label{equation:ASR_definicja1}
	\end{equation}
	
	Niestety wyliczanie formuły \ref{equation:ASR_definicja1} okazuję się niemożliwe do wykonania, ale korzystając ze wzory Bayesa możemy dojść do postaci \ref{equation:ASR_definicja2}, która jest już wygoda do obliczenia.
	
	\begin{equation}
		\hat{W}=\argmax_{W \in \Sigma^{*}}{P(W \mid O)} = \argmax_{W \in \Sigma^{*}}{\frac{P(O \mid W)P(W)}{P(O)}} = \argmax_{W \in \Sigma^{*}}{P(O \mid W)P(W)}
		\label{equation:ASR_definicja2}
	\end{equation}
	
	Ostatnie przejście wynika, z faktu, że $P(O)$ się nie zmienia.
	Implementując formułę \ref{equation:ASR_definicja2} możemy wydzielić 5 kluczowych etapów, są one zilustrowane na rysunku \ref{fig:ARM_schemat}. Opis poszczególnych etapów znajduje się w kolejnych podrozdziałach.
	\\
	Pierwszy etap, określony jako \textit{Zbieranie sygnału akustycznego} (blok \refBlock{zbieranie_sygnalu} na rysunku \ref{fig:ARM_schemat} ), jest realizowany sprzętowo poprzez peryferyjne urządzenia. Obejmuje analogowe przetwarzanie sygnału, cyfryzację oraz opcjonalny post-processing wykonywany przez kartę dźwiękową. Na tym etapie ważne jest, aby dostroić poziom dźwięku, tak aby uniknąć przesterowań, dobrać częstotliwość próbkowania i zakres częstotliwości. Niepoprawna konfiguracja tego etapu, może skutkować zakłóceniami oraz wycięciem cech, które mogą być istotne na kolejnym etapie, a w konsekwencji obniżeniem skuteczności rozpoznawania. W niniejszej pracy wykorzystany był gotowy, powszechnie dostępny korpus nagrań (patrz rozdział \ref{sec:opis_danych}), dlatego nie będę się skupiał na tym etapie.

	\subsection{ Fony }
	\label{sec:Phones}
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[node distance=1.7cm]
			
			\begin{scope}
			\node [hmm] (hmm1) {$s_1$};
			\node [hmm, right of=hmm1] (hmm2) {$s_2$};
			\node [hmm, right of=hmm2] (hmm3) {$s_2$};
			
			\draw[thick,->,shorten >=1pt] (hmm1) to [out=0,in=180] (hmm2);
			\draw[thick,->,shorten >=1pt] (hmm2) to [out=0,in=180] (hmm3);
			
			\draw[thick,->] (hmm1.70) arc (-60:245:4mm);
			\draw[thick,->] (hmm2.70) arc (-60:245:4mm);
			\draw[thick,->] (hmm3.70) arc (-60:245:4mm);
			
			\draw[thick,<-,shorten <=1pt] (hmm1) -- +(180:1cm);
			\draw[thick,->,shorten <=1pt] (hmm3) -- +(0:1cm);
			\end{scope}
			
			\end{tikzpicture}
			\caption{Reprezentacja fonów}
			\label{fig:fon_hmm}
			
		\end{figure}

		Fony są podstawową koncepcją przy modelowaniu dźwięków w mowie, będącą rozszerzeniem pojęcia głoski. Typowo rozróżniają dźwięczne i bezdźwięczne warianty głosek oraz wprowadzają nowe dźwięki, takie jak przykładowo \textit{cisza}. W tabeli \ref{tab:phone_list} znajduje się lista fonów zamodelowanych w wykorzystanych modelach akustycznych, łącznie jest ich 40. Słowa, które mają być rozpoznawane przez system muszą mieć przypisaną transkrypcję na fony. Lista słów wraz z transkrypcją znajduje się w \textit{słowniku} (blok \refBlock{slownik} na rysunku \ref{fig:ARM_schemat}).
		\\
		Lingwistyka <<?>>
		\\
		Parametry modelujące wszystkie rozpoznawane fony są częścią \textit{modelu akustycznego} (blok \refBlock{model_akustyczny} na rysunku \ref{fig:ARM_schemat}). 

		\begin{figure}
			\begin{tabular}{|K{1cm}K{1cm}K{1cm}K{1cm}K{1cm}K{1cm}K{1cm}K{1cm}|}
				\hline
				a  & o\~ & b & c & cz & ć & d & dz \\ 
				dź & dż & e & e\~ & f & g & g\^ & h \\
				i & j & k & k\^ & l & ł & m & n \\
				nn & ń & o & p  & r & s & sz & ś \\
				t & u & v & y & z & ź & ż & sil \\
				\hline
			\end{tabular}
			\caption{\label{tab:phone_list}Lista modelowanych fonów}
		\end{figure}
 

	\subsection{ Modelowanie fonów, modele kontekstowe i bezkontekstrowe }
		
		Przy modelowaniu fonów wyróżnia się trzy fazy:
		\begin{itemize}
			\item początkową, podczas której aparat mowy zmienia swój kształt
			\item środkową, podczas której aparat mowy jest już w ustabilizowanej pozycji
			\item końcową, podczas której aparat mowy przechodzi do układu dla kolejnego fonu, ale dźwięk jaki wydaje jest jeszcze bliższy aktualnemu fonowi. 
		\end{itemize}
		Uwzględniając powyższe spostrzeżenia, wszystkie fony modeluje się za pomocą trzech stanów, tak jak na rysunku \ref{fig:fon_hmm}. Ze stanu można przejść jedynie do następnego stanu lub powrócić do samego siebie. Dzięki przejściu zwrotnemu, możliwe jest modelowanie dźwięków o różnej długości. 
		\\
		Rozwinięciem fonów są \textbf{tri-fony}, uwzględniają one lewy i prawy kontekst poprzez dodanie informacji o fonach stojących obok. Typowo tri-fony zapisuje się zgodnie z notacją na rysunku \ref{fig:tri-fony_notacja}, gdzie \textit{a} jest poprzednim fonem, \textit{b} aktualnym, natomiast \textit{c} następnym. 
		
		\begin{figure}[H]
			\begin{center}
			{a-b+c}
			\end{center}
			\caption{Konwencja zapisu tri-fonów}
			\label{fig:tri-fony_notacja}
		\end{figure}
	
		W języku polskim niektóre głoski różnie się wymawia w zależności od kontekstu, czyli fonów stojących obok. Przykładowo, inaczej brzmi głoska \textit{w} w słowie \textit{wersja}, gdzie jest dźwięczna, a inaczej w zwrocie \textit{w pracy}, gdzie jest bezdźwięczna i bardziej przypomina głoskę \textit{f}. Model tri-fonowy pozwala na uchwycenie takiej różnicy, dzięki czemu umożliwia osiągnięcie wyższej skuteczności niż model uni-fonowy.
		\\
		Należy zauważyć, że korzystając z \textit{tri-fonów} zamiast \textit{uni-fonów} wprowadzamy dodatkową informację, zarówno do modelu językowego, jak i do modelu akustycznego, ale jednocześnie znacznie zwiększamy liczbę parametrów do estymacji i czas działania silnika rozpoznającego. W modelu \textit{tri-fonowym} występuje $O(n^3)$ tri-fonów, gdzie $n$ to liczba fonów w modelu uni-fonowym.  
		
	\subsection{ Rozpoznawanie }
		Ten rozdział ma na pokazać, jak wcześniej opisane elementy składają się na działający system do rozpoznawani.
	\subsection{ Ekstrakcja cech }
	\label{sec:Feature_vec}

	 Ekstrakcja cech, odpowiadająca blokowi \refBlock{ekstrakcja_cech} z rysunku \ref{fig:ARM_schemat}, polega na zamianie sygnału akustycznego na ciąg wektorów, które mogą być następnie sklasyfikowane na kolejnym etapie. Przy rozpoznawaniu mowy najczyściej wykorzystuje się cechy \textit{MFCC}, które składają się z 5 etapów przetwarzania:
	 
	 \begin{itemize}
	 	\item Podział sygnału akustycznego.
	 	\item Transformata Fouriera.
	 	\item Nałożenie zestawu filtrów.
	 	\item Nałożenie logarytmu. 
	 	\item Dyskretna transformata Fouriera.
	 \end{itemize}
 
	 Pierwszym etapem ekstrakcji jest podzielenie sygnału na fragmenty zwane \textit{ramkami}. Typowo ramki mają $25ms$ i są przesunięte co $10ms$. Ramki o takiej długości zawierają wystarczająco dużo informacji, a jednocześnie można założyć, że są stochastycznie stacjonarne. 
	 
	 W drugim etapie każdą ramkę poddaje się transformacie Fouriera. W tej operacji efekcie otrzymuje się widmo sygnału akustycznego, opisujące ilość energii w zależności o częstotliwości.
	 
	 W trzecim etapie na widmo nakłada się zestaw filtrów trójkątnych. Filtry się przesunięte względem siebie, każdy kolejny filer jest coraz szerszy, co odpowiada ludzkiej percepcji. Ludzie dobrze rozróżniają sygnały o małej częstotliwości, natomiast te o wysokiej są podobnie odbierane przez ludzkie ucho. Aby określić zestaw filtrów należy podać ich liczbę, niech będzie nią $n$ oraz zdefiniować $n+1$ częstotliwości, które wyznaczają zakresy filtrów. Załóżmy, że częstotliwości sa zapisane w ciągu $f(1), f(2), \ldots f(n+1)$, wtedy $m$-ty filtr jest zdefiniowany równaniem \ref{eqn:filter}. Efektem nałożenia filtrów jest nowy ciąg wartości $v_1, v_2, \ldots \vDash_n$, które stanowy wejście do kolejnego etapu przetwarzania.
	 
	 \begin{equation}
		 H_m(x)= 
			 \begin{cases}
				 0,				 					 & x < f(m-1) 		       \\
				 \frac{x - f(m-1)}{f(m) - (f(m-1))}, & f(m-1) \leq x \leq f(m) \\
				 \frac{f(m+1) - k}{f(m+1) - (f(m))}, & f(m) \leq x \leq f(m+1) \\
				 0,				 					 & x > f(m+1)
			 \end{cases}
		 \label{eqn:filter}
	 \end{equation}
	 
	  \begin{figure}[H]
	 	\centering
	 	\begin{tikzpicture}[node distance=1.7cm]
	 	
	 	\begin{scope}
	 	
	 	\def\q{2.8};
	 	\def\w{3.5}
	 	
	 	\draw[-stealth] (0cm,0cm)--(9.4cm,0cm) node[right]{Hz}; 
	 	\draw[-stealth] (0cm,0cm)--(0cm,\q cm); 
	 	
	 	\def\a{0.15}; \node at (\a, -0.5) {f(1)};
	 	\def\s{1.0};  \node at (\s, -0.5) {f(2)};
	 	\def\d{2.0};  \node at (\d, -0.5) {f(3)};
	 	\def\f{3.6};  \node at (\f, -0.5) {f(4)};
	 	\def\g{5.8};  \node at (\g, -0.5) {f(5)};
	 	\def\h{8.8};  \node at (\h, -0.5) {f(6)};
	    
    	\path [line, dashed] (\s, 1.0) -- (\s, \w) node[above] {$v_1$};
   		\path [line, dashed] (\d, 1.0) -- (\d, \w) node[above] {$v_2$};
 		\path [line, dashed] (\f, 1.0) -- (\f, \w) node[above] {$v_3$}; 
		\path [line, dashed] (\g, 1.0) -- (\g, \w) node[above] {$v_4$};
	 	
	 	\pgfmathsetmacro{\z}{1.0 * \q / (\d - \a) * (\d - \a)};
	 	\draw[-] (\a cm,0cm)--(\s cm,\z cm); 
	 	\draw[-] (\d cm,0.0cm)--(\s cm,\z cm); 
	 	
	 	\pgfmathsetmacro{\z}{1.0 * \q / (\f - \s) * (\d - \a)};
	 	\draw[-] (\s cm,0.0cm)--(\d cm,\z cm); 
	 	\draw[-] (\f cm,0.0cm)--(\d cm,\z cm); 
	 	
	 	\pgfmathsetmacro{\z}{1.0 * \q / (\g - \d) * (\d - \a)};
	 	\draw[-] (\d cm,0cm)--(\f cm,\z cm); 
	 	\draw[-] (\g cm,0.0cm)--(\f cm,\z cm); 
	 	
	 	\pgfmathsetmacro{\z}{1.0 * \q / (\h - \f) * (\d - \a)};
	 	\draw[-] (\f cm,0cm)--(\g cm,\z cm); 
	 	\draw[-] (\h cm,0.0cm)--(\g cm,\z cm);
	 	
	 	\end{scope}
	 	
	 	\end{tikzpicture}
	 	\caption{Wizualizacja jednej iteracji algorytmu Viterbiego}
	 	\label{fig:feature_extraction}
	 	
	 \end{figure}
	 
	Tutaj omówić cechy MFCC i MFSC i krótko wspomnieć o innych np PLP, RASTA-PLP.
	

\section {Rozpoznawanie mowy z zastosowaniem ukrytych modeli markowa}
	\label{sec:ASR_HMM}
    \subsection{Ukryte modele Markova - definicja}
    
	    \textbf{Ukryte modele Markova}, w skrócie \textbf{HMM}, to automat, który przechodzi pomiędzy stanami z pewnym prawdopodobieństwem $p1$ i wraz z każdym przejściem, emituje obserwację z prawdopodobieństwem $p2$. Formalnie HMM można zdefiniować jako krotkę \ref{equation:HMM_def}.
	    
	    \begin{equation}
		    HMM = (Q, O, A, B, q_0, q_F)
		    \label{equation:HMM_def}
	    \end{equation}
	    gdzie
	    \begin{align*}
		    \mathbf{Q}=\{q_1, q_2,\ldots,q_n\} & &&  \text{Zbiór stanów automatu} \\
	 	    \mathbf{O}=\{o_1, o_2,\ldots,o_k\} & &&  \text{Zbiór obserwacji} \\
	 	    \mathbf{A} =
	 	    \left| \begin{array}{ccc}
		 	    a_{1,1} & \ldots & a_{1,n} \\
		 	    \vdots  & \ddots & \vdots\\
		 	    a_{n,1} & \ldots & a_{n,n}
	 	    \end{array} \right|
												 	    & &&  \text{Macierz przejścia pomiędzy stanami} \\
												 	    & && \\
	 	    \mathbf{B}=\{B_1(o),\ldots,B_n(o)\},o \in O & && \text{zbiór rozkłądów prawdopodobieństwa emisji} \\ 
													 	& && \text{obsetwacji \textit{o} w stanie \textit{i}} \\
		 	\mathbf{q_0, q_F}				  & && \text{stany początkowy i końcowy}
	    \end{align*}
	    
    	
	    	
	   \subsubsection{Modele Markova}
	   \subsubsection{Algorytmy Viterbiego}
		   \label{sec:Viterbi}
		   Algorytmy Viterbiego sa dynamicznymi algorytmami do znajdowania najbardziej prawdopodobnej ścieżki w automacie oraz prawdopodobieństwa stanu $s_i$ po $t$ krokach. 
		   
		   \begin{itemize}
			   	\item Aby znaleźć najbardziej prawdopodobną ścieżkę, należy skorzystać z rekurencyjnego równania \ref{equation:viterbi_path}.
			   	\item Aby znaleźć prawdopodobieństwo stanu po $k$ krokach, należy skorzystać z rekurencyjnego równania \ref{equation:viterbi_node}.
		   \end{itemize}
	   	   
		   \begin{equation}
		   P_t^q=\max_{q \in Q} \bigg( P_{t-1}^q\cdot a_{q,w}\cdot b_q(o) \bigg)
		   \label{equation:viterbi_path}
		   \end{equation}
		   
		   \begin{equation}
		   P_t^q=\sum_{q \in Q} \bigg( P_{t-1}^q\cdot a_{q,w}\cdot b_q(o) \bigg)
		   \label{equation:viterbi_node}
		   \end{equation}
		   
		  Oba równania (\ref{equation:viterbi_path} oraz \ref{equation:viterbi_node}) odwołując się jedynie do wartości z poprzedniej iteracji, zatem stosując techniki programowania dynamicznego można wyznaczyć najbardziej prawdopodobną ścieżkę stosując algorytm \ref{alg:viterbi}. Algorytm ten przechodzi przez wszystkie ramki czasowe $T$, w najbardziej zewnętrznej pętli. Następnie dla każdego stany $q1$, w najbardziej wewnętrznej pętli, wyszukuje przejścia o największym prawdopodobieństwie, które doprowadziło ze stanu $q2$ do $q1$. Rysunek \ref{fig:viterbi} ilustruje jedną iterację zewnętrznej pętli. Pod koniec działania algorytmu zwracany jest końcowy stan najbardziej prawdopodobnej ścieżki. \\
		  Algorytm \ref{alg:viterbi2} pokazuje analogiczny sposób znajdowania najbardziej prawdopodobnego stanu.
		
		  \begin{figure}
			   \begin{algorithmic}[1]
				   	
					   	\REQUIRE {tablica obserwacji $O$ o rozmiarze T}
					   	\REQUIRE {macierz przejścia $A$ o rozmiarze $|Q| \times |Q|$}
					   	
					   	\STATE $T_a[0 \ldots Q] = 1$;
					   	\COMMENT {inicjalizacja pierwszej tablicy pomocniczej}
					   	\STATE $T_b[0 \ldots Q] = 0$;
					   	\COMMENT {inicjalizacja drugiej tablicy pomocniczej}
					   	
					   	\FOR{$t=1$ to $T$}
						   	\FOR{$q1=1$ to $|Q|$}
							   	\STATE $p_{tmp} = 0$;
							   	\FOR{$q2=1$ to $|Q|$}
								   	\IF {$ \bigg( T_a[q2]\cdot a_{q,w}\cdot b_q(O_t) \bigg) > p_{tmp}$}
									   	\STATE $p_{tmp} = \bigg( T_a[q2]\cdot a_{q,w}\cdot b_q(O_t) \bigg)$;
								   	\ENDIF
							   	\ENDFOR
							   	\STATE $T_b[q1] = p_{tmp}$
						   	\ENDFOR
						   	\STATE $swap(T_a, T_b)$;
					   	\ENDFOR
					   	
					   	\RETURN $\argmax{T_a}$;
			   \end{algorithmic}
		  \caption{Algorytmy Viterbiego znajdujący najbardziej prawdopodobną ścieżkę}
		  \label{alg:viterbi}
		  \end{figure}		
	  
		  \begin{figure}
			  \begin{algorithmic}[1]
		  		
		  		  \REQUIRE {tablica obserwacji $O$ o rozmiarze T}
		  		  \REQUIRE {macierz przejścia $A$ o rozmiarze $|Q| \times |Q|$}
		  		
		  	  	  \STATE $T_a[0 \ldots Q] = 1$;
		  		  \COMMENT {inicjalizacja pierwszej tablicy pomocniczej}
		  		  \STATE $T_b[0 \ldots Q] = 0$;
		  		  \COMMENT {inicjalizacja drugiej tablicy pomocniczej}
		  		
		  		  \FOR{$t=1$ to $T$}
			  		  \FOR{$q1=1$ to $|Q|$}
				  		  \STATE $T_b[q1] = 0$;
				  		  \FOR{$q2=1$ to $|Q|$}
					  		  \STATE $T_b[q1] += \bigg( T_a[q2]\cdot a_{q,w}\cdot b_q(O_t) \bigg)$					  		  
				  		  \ENDFOR
				  		  \STATE $T_b[q1] = p_{tmp}$
			  		  \ENDFOR
			  		  \STATE $swap(T_a, T_b)$;
		  		  \ENDFOR
		  		
		  		  \RETURN $\argmax{T_a}$;
			  \end{algorithmic}
			  \caption{Algorytmy Viterbiego znajdujący najbardziej prawdopodobną stan}
		  	  \label{alg:viterbi2}
		  \end{figure}	   
		   
		   \begin{figure}[H]
		   	\centering
		   	\begin{tikzpicture}[node distance=1.7cm]
		   	
		   	\begin{scope}
		   	\node [hmm] (hmm1) {$s_1$};
		   	\node[left] at (hmm1.west) {$P_{t-1}^{s1}$};
		   	
		   	\node [hmm, below of=hmm1] (hmm2) {$s_2$};
		   	\node[left] at (hmm2.west) {$P_{t-1}^{s2}$};
		   	
		   	\node [hmm, below of=hmm2] (hmm3) {$s_3$};
		   	\node[left] at (hmm3.west) {$P_{t-1}^{s3}$};
		   	
		   	\node [hmm, below of=hmm3] (hmm4) {$s_4$};
		   	\node[left] at (hmm4.west) {$P_{t-1}^{s4}$};
		   	
		   	\node [hmm, below of=hmm4] (hmm5) {$s_5$};
		   	\node[left] at (hmm5.west) {$P_{t-1}^{s5}$};
		   	
		   	\node [hmm, right = 3.5 cm of hmm3] (hmm6) {$s_3$};
		   	\node[right] at (hmm6.east) {$P_{t}^{s3}$};
		   	
		   	\draw [->] (hmm1) [out=0, in=140]  to node[midway,right,rotate=0] {$a_{s1,s3}$} (hmm6);
		   	\draw [->] (hmm2) [out=0, in=160]  to node[midway,above,rotate=0] {$a_{s2,s3}$} (hmm6);
		   	\draw [->] (hmm3) [out=0, in=180]  to node[midway,above,rotate=0] {$a_{s3,s3}$} (hmm6);
		   	\draw [->] (hmm4) [out=0, in=200]  to node[midway,above,rotate=0] {$a_{s4,s3}$} (hmm6);
		   	\draw [->] (hmm5) [out=0, in=220]  to node[midway,right,rotate=0] {$a_{s5,s3}$} (hmm6);
		   	
		   	\draw[-stealth] (-2cm,-8cm)--(6cm,-8cm) node[right]{T}; 
		   	\draw[-stealth] (-2cm,-8cm)--(-2cm,0.5cm) node[right]{Q}; 
		   	\draw[dashed] (1cm,-8cm)--(1cm,0.5cm);
		   	
		   	\end{scope}
		   	
		   	\end{tikzpicture}
		   	\caption{Wizualizacja jednej iteracji algorytmu Viterbiego}
		   	\label{fig:viterbi}
		   	
		   \end{figure}
		   
	   \subsubsection{Algorytm Bauma-Welcha}
	   \subsubsection{Estymacja parametrów rozkładu normalnego}
	 
	
    \subsection{Modelowanie fonetyki z wykorzystaniem HMM }
    W klasycznych systemach rozpoznających mowę, wykorzystujących ukryte modele Markova, zbiór stanów reprezentuje fony. Każdy fon ma trzy stany w HMM, tak jak opisano w rozdziale \ref{sec:Phones}. Zbiorem obserwacji są wektory cech wyznaczane zgodnie z opisem w rozdziale \ref{sec:Feature_vec}. Przejścia pomiędzy stanami jednego fonemu są zgodne z opisem z rozdziału \ref{sec:Phones}, natomiast przejścia pomiędzy różnymi fonami są tworzone na podstawie słownika (blok \refBlock{slownik} na rysunku \ref{fig:ARM_schemat}). Dla każdego słowa generowany jest ciąg stanów odpowiadający kolejnym fonemom, z których składa się słowo. \\
    Na rysunku \ref{fig:AutomatExample} zamieszczono przykład automatu dla słowa \textit{jabłko}. Zgodnie z wykorzystanymi regułami leksykalnymi słowo \textit{jabłko} zapisuje się za pomocą fonemów jako $[j a p ł k o]$. \\
    
    \begin{figure}[H]
    	\centering
		\begin{tabular}{|c|}
			\hline
			\textit{jabłko} = [j a p ł k o] \\ 
			\hline \\
	
			\begin{tikzpicture}[node distance=1.7cm]
			
				\begin{scope}
				
				\def\x{0.65}
				\def\y{1.0}
				\def\z{2.5}
				
				\node [hmm0] (hmm1) {};
				\node [below] at (hmm1.south) {$j_1$};
				
				\node [hmm0, right = \x cm of hmm1] (hmm2) {};
				\node [below] at (hmm2.south) {$j_2$};
				
				\node [hmm0, right = \x cm of hmm2] (hmm3) {};
				\node [below] at (hmm3.south) {$j_3$};
				
				
				\node [hmm0, right = \y cm of hmm3] (hmm4) {};
				\node [below] at (hmm4.south) {$a_1$};
				
				\node [hmm0, right = \x cm of hmm4] (hmm5) {};
				\node [below] at (hmm5.south) {$a_2$};
				
				\node [hmm0, right = \x cm of hmm5] (hmm6) {};
				\node [below] at (hmm6.south) {$a_3$};
				
				
				\node [hmm0, right = \y cm of hmm6] (hmm7) {};
				\node [below] at (hmm7.south) {$p_1$};
				
				\node [hmm0, right = \x cm of hmm7] (hmm8) {};
				\node [below] at (hmm8.south) {$p_2$};
				
				\node [hmm0, right = \x cm of hmm8] (hmm9) {};
				\node [below] at (hmm9.south) {$p_3$};
				
				
				
				
				\node [hmm0, below = \z cm of hmm9] (hmm10) {};
				\node [below] at (hmm10.south) {$ł_1$};
				
				\node [hmm0, left = \x cm of hmm10] (hmm11) {};
				\node [below] at (hmm11.south) {$ł_2$};
				
				\node [hmm0, left = \x cm of hmm11] (hmm12) {};
				\node [below] at (hmm12.south) {$ł_3$};
				
				
				\node [hmm0, left = \y cm of hmm12] (hmm13) {};
				\node [below] at (hmm13.south) {$k_1$};
				
				\node [hmm0, left = \x cm of hmm13] (hmm14) {};
				\node [below] at (hmm14.south) {$k_2$};
				
				\node [hmm0, left = \x cm of hmm14] (hmm15) {};
				\node [below] at (hmm15.south) {$k_3$};
				
				
				\node [hmm0, left = \y cm of hmm15] (hmm16) {};
				\node [below] at (hmm16.south) {$o_1$};
				
				\node [hmm0, left = \x cm of hmm16] (hmm17) {};
				\node [below] at (hmm17.south) {$o_2$};
				
				\node [hmm0, left = \x cm of hmm17] (hmm18) {};
				\node [below] at (hmm18.south) {$o_3$};
				
				
				
				\draw[thick,->,shorten >=1pt] (hmm1) to [out=0,in=180] (hmm2);
				\draw[thick,->,shorten >=1pt] (hmm2) to [out=0,in=180] (hmm3);
				
				\draw[thick,->,shorten >=1pt] (hmm3) to [out=0,in=180] (hmm4);
				
				\draw[thick,->,shorten >=1pt] (hmm4) to [out=0,in=180] (hmm5);
				\draw[thick,->,shorten >=1pt] (hmm5) to [out=0,in=180] (hmm6);
				
				\draw[thick,->,shorten >=1pt] (hmm6) to [out=0,in=180] (hmm7);
				
				\draw[thick,->,shorten >=1pt] (hmm7) to [out=0,in=180] (hmm8);
				\draw[thick,->,shorten >=1pt] (hmm8) to [out=0,in=180] (hmm9);
				
				\draw[thick,->,shorten >=1pt] (hmm9) to [out=0,in=0,looseness=1.1] (hmm10);
				
				\draw[thick,->,shorten >=1pt] (hmm10) to [out=180,in=0] (hmm11);
				\draw[thick,->,shorten >=1pt] (hmm11) to [out=180,in=0] (hmm12);
				
				\draw[thick,->,shorten >=1pt] (hmm12) to [out=180,in=0] (hmm13);
				
				\draw[thick,->,shorten >=1pt] (hmm13) to [out=180,in=0] (hmm14);
				\draw[thick,->,shorten >=1pt] (hmm14) to [out=180,in=0] (hmm15);
				
				\draw[thick,->,shorten >=1pt] (hmm15) to [out=180,in=0] (hmm16);
				
				\draw[thick,->,shorten >=1pt] (hmm16) to [out=180,in=0] (hmm17);
				\draw[thick,->,shorten >=1pt] (hmm17) to [out=180,in=0] (hmm18);
				
		
				\draw[thick,->] (hmm1.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm2.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm3.70) arc (-60:245:4mm);
				
				\draw[thick,->] (hmm4.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm5.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm6.70) arc (-60:245:4mm);
				
				\draw[thick,->] (hmm7.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm8.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm9.70) arc (-60:245:4mm);
				
				
				
				\draw[thick,->] (hmm10.110) arc (240:-65:4mm);
				\draw[thick,->] (hmm11.110) arc (240:-65:4mm);
				\draw[thick,->] (hmm12.110) arc (240:-65:4mm);
				
				\draw[thick,->] (hmm13.110) arc (240:-65:4mm);
				\draw[thick,->] (hmm14.110) arc (240:-65:4mm);
				\draw[thick,->] (hmm15.110) arc (240:-65:4mm);
				
				\draw[thick,->] (hmm16.110) arc (240:-65:4mm);
				\draw[thick,->] (hmm17.110) arc (240:-65:4mm);
				\draw[thick,->] (hmm18.110) arc (240:-65:4mm);
				
				\draw[thick,<-,shorten <=1pt] (hmm1) -- +(180:1cm);
				\draw[thick,->,shorten <=1pt] (hmm18) -- +(180:1cm);
				\end{scope}			
			\end{tikzpicture} \\
			
			\hline
		\end{tabular}
    	\caption{Automat dla słowa \textit{jabłko}}
    	\label{fig:AutomatExample}
    	
    \end{figure}

    Przy generowaniu automatu dla wielu słów, w najprostszej implementacji, dla każdego słowa ze słownika, można stworzyć osobny ciąg stanów. Rysunek \ref{fig:Graph_simple} pokazuje przykład prostego grafu przejść dla słów: \textit{kot, kos, kasa, masa, ma}. Widać na nim dodatkowe dwa stany \textit{start} oraz \textit{stop}, z którymi nie wiąże się emisja żadnej obserwacji. Funkcją tych stanów jest połączenie wielu słów w jeden graf, odpowiadają one stanom \textit{$q_0$} oraz \textit{$q_F$} z definicji \ref{equation:HMM_def}. Dodane jest również jeszcze jedno techniczne przejście, ze stanu \textit{stop} do stanu \textit{start}, jego zadaniem jest umożliwienie przejścia na początek automatu w celu rozpoznawania kolejnego słowa. \\
    Opisany powyżej automat dobrze oddaje koncepcję ukrytego modelu Markova, który jest wykorzystywany przy rozpoznawaniu mowy, jednak jest bardzo nie efektywny. Nieefektywność wynika z wielokrotnego powtarzania tych samych obliczeń, dla słów o wspólnym prefiksie. Naturalną optymalizacją jest więc połączenie wspólnych prefiksów. Rysunek \ref{fig:Graph_complex} ilustruje zoptymalizowany automat \ref{fig:Graph_simple}. Widać, że dzięki optymalizacji otrzymujemy znacznie mniejszy graf o bardziej zwięzłej strukturze. 
    
    \begin{landscape}
	    \begin{figure}
	    	\centering
	    	\begin{tabular}{|c|}
	    		\hline
	    		\textit{kot} = [k o t],  \textit{kos} = [k o s],  \textit{kasa} = [k a s a], \textit{masa} = [m a s a], \textit{ma} = [m a]\\ 
	    		\hline \\
	    		
	    		\begin{tikzpicture}[node distance=1.13cm]
	    		
	    		\begin{scope} 		
	    		\def \h{1.5}
	    		\def \t{3.2}
	    		\def \d{0.7}
	    		
	   			\node [hmm] (hmm1) {$k_1$};
	   			\node [hmm, right of=hmm1] (hmm2) {$k_2$};
	   			\node [hmm, right of=hmm2] (hmm3) {$k_2$};
	   			
	   			\node [hmm, right = \d cm of hmm3] (hmm4) {$o_1$};
	   			\node [hmm, right of=hmm4] (hmm5) {$o_2$};
	   			\node [hmm, right of=hmm5] (hmm6) {$o_3$};
	   			
	   			\node [hmm, right = \d cm of hmm6] (hmm7) {$t_1$};
	   			\node [hmm, right of=hmm7] (hmm8) {$t_2$};
	   			\node [hmm, right of=hmm8] (hmm9) {$t_3$};
	   			
	   			
	   			
	   			\node [hmm, above = \h cm of hmm1] (hmm10) {$k_1$};
	   			\node [hmm, right of=hmm10] (hmm11) {$k_2$};
	   			\node [hmm, right of=hmm11] (hmm12) {$k_3$};
	   			
	   			\node [hmm, right = \d cm of hmm12] (hmm13) {$o_1$};
	   			\node [hmm, right of=hmm13] (hmm14) {$o_2$};
	   			\node [hmm, right of=hmm14] (hmm15) {$o_3$};
	   			
	   			\node [hmm, right = \d cm of hmm15] (hmm16) {$s_1$};
	   			\node [hmm, right of=hmm16] (hmm17) {$s_2$};
	   			\node [hmm, right of=hmm17] (hmm18) {$s_3$};
	   			
	   			
	   			
	   			\node [hmm, below = \h cm of hmm1] (hmm19) {$k_1$};
	   			\node [hmm, right of=hmm19] (hmm20) {$k_2$};
	   			\node [hmm, right of=hmm20] (hmm21) {$k_3$};
	   			
	   			\node [hmm, right = \d cm of hmm21] (hmm22) {$a_1$};
	   			\node [hmm, right of=hmm22] (hmm23) {$a_2$};
	   			\node [hmm, right of=hmm23] (hmm24) {$a_3$};
	   			
	   			\node [hmm, right = \d cm of hmm24] (hmm25) {$s_1$};
	   			\node [hmm, right of=hmm25] (hmm26) {$s_2$};
	   			\node [hmm, right of=hmm26] (hmm27) {$s_3$};
	   			
	   			\node [hmm, right = \d cm of hmm27] (hmm28) {$a_1$};
	   			\node [hmm, right of=hmm28] (hmm29) {$a_2$};
	   			\node [hmm, right of=hmm29] (hmm30) {$a_3$};
	   			
	   			
	   			\node [hmm, below = \h cm of hmm19] (hmm31) {$m_1$};
	   			\node [hmm, right of=hmm31] (hmm32) {$m_2$};
	   			\node [hmm, right of=hmm32] (hmm33) {$m_3$};
	   			
	   			\node [hmm, right = \d cm of hmm33] (hmm34) {$a_1$};
	   			\node [hmm, right of=hmm34] (hmm35) {$a_2$};
	   			\node [hmm, right of=hmm35] (hmm36) {$a_3$};
	   			
	   			\node [hmm, right = \d cm of hmm36] (hmm37) {$s_1$};
	   			\node [hmm, right of=hmm37] (hmm38) {$s_2$};
	   			\node [hmm, right of=hmm38] (hmm39) {$s_3$};
	   			
	   			\node [hmm, right = \d cm of hmm39] (hmm40) {$a_1$};
	   			\node [hmm, right of=hmm40] (hmm41) {$a_2$};
	   			\node [hmm, right of=hmm41] (hmm42) {$a_3$};
	   			
	   			
	   			\node [hmm, below = \h cm of hmm31] (hmm43) {$m_1$};
	   			\node [hmm, right of=hmm43] (hmm44) {$m_2$};
	   			\node [hmm, right of=hmm44] (hmm45) {$m_3$};
	   			
	   			\node [hmm, right = \d cm of hmm45] (hmm46) {$a_1$};
	   			\node [hmm, right of=hmm46] (hmm47) {$a_2$};
	   			\node [hmm, right of=hmm47] (hmm48) {$a_3$};
	   			
	   			
	   			\node [hmm, left = 1.3 cm of hmm19] (start) {$start$};
	   			\node [hmm, right = 1.3 cm of hmm30] (stop) {$stop$};
	   			
	   			\coordinate [below = \t cm of hmm42] (P1);
	   			\coordinate [below of = hmm43] (P2);
	   			\coordinate [below = 2.0 cm of hmm42] (P3);
	   			\coordinate [above = 2.0 cm of hmm30] (P4);
	   			\coordinate [above = 2.4 cm of P4] (P5);
	   			%\coordinate [below of=hmm25] (P4);
	   			
	   			\draw[thick,->,shorten >=1pt] (hmm1) to [out=0,in=180] (hmm2);
	   			\draw[thick,->,shorten >=1pt] (hmm2) to [out=0,in=180] (hmm3);
	   			\draw[thick,->,shorten >=1pt] (hmm3) to [out=0,in=180] (hmm4);
	   			\draw[thick,->,shorten >=1pt] (hmm4) to [out=0,in=180] (hmm5);
	   			\draw[thick,->,shorten >=1pt] (hmm5) to [out=0,in=180] (hmm6);
	   			\draw[thick,->,shorten >=1pt] (hmm6) to [out=0,in=180] (hmm7);
	   			\draw[thick,->,shorten >=1pt] (hmm7) to [out=0,in=180] (hmm8);
	   			\draw[thick,->,shorten >=1pt] (hmm8) to [out=0,in=180] (hmm9);
	   			
	   			\draw[thick,->,shorten >=1pt] (hmm10) to [out=0,in=180] (hmm11);
	   			\draw[thick,->,shorten >=1pt] (hmm11) to [out=0,in=180] (hmm12);
	   			\draw[thick,->,shorten >=1pt] (hmm12) to [out=0,in=180] (hmm13);
	   			\draw[thick,->,shorten >=1pt] (hmm13) to [out=0,in=180] (hmm14);
	   			\draw[thick,->,shorten >=1pt] (hmm14) to [out=0,in=180] (hmm15);
	   			\draw[thick,->,shorten >=1pt] (hmm15) to [out=0,in=180] (hmm16);
	   			\draw[thick,->,shorten >=1pt] (hmm16) to [out=0,in=180] (hmm17);
	   			\draw[thick,->,shorten >=1pt] (hmm17) to [out=0,in=180] (hmm18);
	   			
	   			\draw[thick,->,shorten >=1pt] (hmm19) to [out=0,in=180] (hmm20);
	   			\draw[thick,->,shorten >=1pt] (hmm20) to [out=0,in=180] (hmm21);
	 			\draw[thick,->,shorten >=1pt] (hmm21) to [out=0,in=180] (hmm22);
	 			\draw[thick,->,shorten >=1pt] (hmm22) to [out=0,in=180] (hmm23);
	 			\draw[thick,->,shorten >=1pt] (hmm23) to [out=0,in=180] (hmm24);
	 			\draw[thick,->,shorten >=1pt] (hmm24) to [out=0,in=180] (hmm25);
	 			\draw[thick,->,shorten >=1pt] (hmm25) to [out=0,in=180] (hmm26);
	 			\draw[thick,->,shorten >=1pt] (hmm26) to [out=0,in=180] (hmm27);
	 			\draw[thick,->,shorten >=1pt] (hmm27) to [out=0,in=180] (hmm28);
	 			\draw[thick,->,shorten >=1pt] (hmm28) to [out=0,in=180] (hmm29);
	 			\draw[thick,->,shorten >=1pt] (hmm29) to [out=0,in=180] (hmm30);

				\draw[thick,->,shorten >=1pt] (hmm31) to [out=0,in=180] (hmm32);
				\draw[thick,->,shorten >=1pt] (hmm32) to [out=0,in=180] (hmm33);
				\draw[thick,->,shorten >=1pt] (hmm33) to [out=0,in=180] (hmm34);
				\draw[thick,->,shorten >=1pt] (hmm34) to [out=0,in=180] (hmm35);
				\draw[thick,->,shorten >=1pt] (hmm35) to [out=0,in=180] (hmm36);
				\draw[thick,->,shorten >=1pt] (hmm36) to [out=0,in=180] (hmm37);
				\draw[thick,->,shorten >=1pt] (hmm37) to [out=0,in=180] (hmm38);
				\draw[thick,->,shorten >=1pt] (hmm38) to [out=0,in=180] (hmm39);
				\draw[thick,->,shorten >=1pt] (hmm39) to [out=0,in=180] (hmm40);
				\draw[thick,->,shorten >=1pt] (hmm40) to [out=0,in=180] (hmm41);
				\draw[thick,->,shorten >=1pt] (hmm41) to [out=0,in=180] (hmm42);
				
				\draw[thick,->,shorten >=1pt] (hmm43) to [out=0,in=180] (hmm44);
				\draw[thick,->,shorten >=1pt] (hmm44) to [out=0,in=180] (hmm45);
				\draw[thick,->,shorten >=1pt] (hmm45) to [out=0,in=180] (hmm46);
				\draw[thick,->,shorten >=1pt] (hmm46) to [out=0,in=180] (hmm47);				
				\draw[thick,->,shorten >=1pt] (hmm47) to [out=0,in=180] (hmm48);
	   			
	   			\draw[thick,->] (hmm1.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm2.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm3.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm4.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm5.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm6.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm7.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm8.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm9.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm10.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm11.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm12.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm13.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm14.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm15.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm16.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm17.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm18.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm19.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm20.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm21.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm22.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm23.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm24.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm25.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm26.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm27.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm28.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm29.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm30.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm31.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm32.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm33.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm34.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm35.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm36.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm37.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm38.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm39.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm40.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm41.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm42.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm43.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm44.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm45.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm46.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm47.70) arc (-60:245:4mm);
	   			\draw[thick,->] (hmm48.70) arc (-60:245:4mm);
	   			
	   			\draw[thick,->,shorten >=1pt] (start) to [out=45,in=180] (hmm10);
	   			\draw[thick,->,shorten >=1pt] (start) to [out=20,in=180] (hmm1);
	   			\draw[thick,->,shorten >=1pt] (start) to [out=0,in=180] (hmm19);
   				\draw[thick,->,shorten >=1pt] (start) to [out=-20,in=180] (hmm31);
   				\draw[thick,->,shorten >=1pt] (start) to [out=-45,in=180] (hmm43);
	   			
	   			\draw[thick,->,shorten >=1pt] (hmm18) to [out=0,in=180] (P5);
	   			\draw[thick,->,shorten >=1pt] (P5) to [out=0,in=135] (stop);
	   			\draw[thick,->,shorten >=1pt] (hmm9) to [out=0,in=180] (P4);
	   			\draw[thick,->,shorten >=1pt] (P4) to [out=0,in=160] (stop);
	   			\draw[thick,->,shorten >=1pt] (hmm30) to [out=0,in=180] (stop);
	   			\draw[thick,->,shorten >=1pt] (hmm42) to [out=0,in=230] (stop);
	   			
	   			\draw[thick,->,shorten >=1pt] (stop) to [out=-45,in=0] (P1);
	   			\draw[thick,->,shorten >=1pt] (P1) to [out=180,in=0] (P2);
	   			\draw[thick,->,shorten >=1pt] (P2) to [out=180,in=225] (start);
	   			
	   			\draw[thick,->,shorten >=1pt] (hmm48) to [out=0,in=180] (P3);
	   			\draw[thick,->,shorten >=1pt] (P3) to [out=0,in=-110] (stop);
	   			%\draw[thick,->,shorten >=1pt] (P3) to [out=0,in=240] (stop);
	    		
	    		\end{scope}			
	    		\end{tikzpicture} \\
	    		
	    		\hline
	    	\end{tabular}
	    	\caption{Niezoptymalizowany automat dla słów: \textit{kot}, \textit{kos}, \textit{kasa}, \textit{masa} oraz \textit{ma}}
	    	\label{fig:Graph_simple}
	    	
	    \end{figure}
    \end{landscape}

	\begin{landscape}
		\begin{figure}
			\centering
			
			\begin{tabular}{|c|}
				\hline
				\textit{kot} = [k o t],  \textit{kos} = [k o s],  \textit{kasa} = [k a s a], \textit{masa} = [m a s a], \textit{ma} = [m a]\\ 
				\hline \\
				
				\begin{tikzpicture}[node distance=1.13cm]
				
				\begin{scope}  		
				
				\def \h{1.5}
				\def \d{0.7}
				
				\node [hmm] (hmm1) {$k_1$};
				\node [hmm, right of=hmm1] (hmm2) {$k_2$};
				\node [hmm, right of=hmm2] (hmm3) {$k_3$};
				
				\node [hmm, right = \d cm of hmm3] (hmm13) {$a_1$};
				\node [hmm, right of=hmm13] (hmm14) {$a_2$};
				\node [hmm, right of=hmm14] (hmm15) {$a_3$};
				
				\node [hmm, right = \d cm of hmm15] (hmm16) {$s_1$};
				\node [hmm, right of=hmm16] (hmm17) {$s_2$};
				\node [hmm, right of=hmm17] (hmm18) {$s_3$};
				
				\node [hmm, right = \d cm of hmm18] (hmm19) {$a_1$};
				\node [hmm, right of=hmm19] (hmm20) {$a_2$};
				\node [hmm, right of=hmm20] (hmm21) {$a_3$};
				
				
				\node [hmm, above = \h cm of hmm13] (hmm4) {$o_1$};
				\node [hmm, right of=hmm4] (hmm5) {$o_2$};
				\node [hmm, right of=hmm5] (hmm6) {$o_3$};
				
				\node [hmm, right = \d cm of hmm6] (hmm7) {$t_1$};
				\node [hmm, right of=hmm7] (hmm8) {$t_2$};
				\node [hmm, right of=hmm8] (hmm9) {$t_3$};
				
				\node [hmm, above = \h cm of hmm7] (hmm10) {$s_1$};
				\node [hmm, right of=hmm10] (hmm11) {$s_2$};
				\node [hmm, right of=hmm11] (hmm12) {$s_3$};
				
				
				\node [hmm, below = \h cm of hmm1] (hmm31) {$m_1$};
				\node [hmm, right of=hmm31] (hmm32) {$m_2$};
				\node [hmm, right of=hmm32] (hmm33) {$m_3$};
				
				\node [hmm, right = \d cm of hmm33] (hmm34) {$a_1$};
				\node [hmm, right of=hmm34] (hmm35) {$a_2$};
				\node [hmm, right of=hmm35] (hmm36) {$a_3$};
				
				\node [hmm, right = \d cm of hmm36] (hmm37) {$s_1$};
				\node [hmm, right of=hmm37] (hmm38) {$s_2$};
				\node [hmm, right of=hmm38] (hmm39) {$s_3$};
				
				\node [hmm, right = \d cm of hmm39] (hmm40) {$a_1$};
				\node [hmm, right of=hmm40] (hmm41) {$a_2$};
				\node [hmm, right of=hmm41] (hmm42) {$a_3$};
				
				\node [hmm, left = 1.3 cm of hmm1] (start) {$start$};
				\node [hmm, right = 1.3 cm of hmm21] (stop) {$stop$};
				
				\coordinate [below = \h cm of hmm42] (P1);
				\coordinate [below = \h cm of hmm31] (P2);
				\coordinate [above = 1.9 cm of hmm21] (P3);
				\coordinate [above = 2.3 cm of P3] (P4);
				\coordinate [below of = hmm37] (P5);
				\coordinate [below of = hmm42] (P6);
				
				\draw[thick,->,shorten >=1pt] (hmm1) to [out=0,in=180] (hmm2);
				\draw[thick,->,shorten >=1pt] (hmm2) to [out=0,in=180] (hmm3);
				\draw[thick,->,shorten >=1pt] (hmm3) to [out=15,in=195] (hmm4);
				\draw[thick,->,shorten >=1pt] (hmm4) to [out=0,in=180] (hmm5);
				\draw[thick,->,shorten >=1pt] (hmm5) to [out=0,in=180] (hmm6);
				\draw[thick,->,shorten >=1pt] (hmm6) to [out=0,in=180] (hmm7);
				\draw[thick,->,shorten >=1pt] (hmm7) to [out=0,in=180] (hmm8);
				\draw[thick,->,shorten >=1pt] (hmm8) to [out=0,in=180] (hmm9);
				
				\draw[thick,->,shorten >=1pt] (hmm6) to [out=15,in=196] (hmm10);
				\draw[thick,->,shorten >=1pt] (hmm10) to [out=0,in=180] (hmm11);
				\draw[thick,->,shorten >=1pt] (hmm11) to [out=0,in=180] (hmm12);
				
				\draw[thick,->,shorten >=1pt] (hmm3) to [out=0,in=180] (hmm13);
				\draw[thick,->,shorten >=1pt] (hmm13) to [out=0,in=180] (hmm14);
				\draw[thick,->,shorten >=1pt] (hmm14) to [out=0,in=180] (hmm15);
				\draw[thick,->,shorten >=1pt] (hmm15) to [out=0,in=180] (hmm16);
				\draw[thick,->,shorten >=1pt] (hmm16) to [out=0,in=180] (hmm17);
				\draw[thick,->,shorten >=1pt] (hmm17) to [out=0,in=180] (hmm18);
				\draw[thick,->,shorten >=1pt] (hmm18) to [out=0,in=180] (hmm19);
				\draw[thick,->,shorten >=1pt] (hmm19) to [out=0,in=180] (hmm20);
				\draw[thick,->,shorten >=1pt] (hmm20) to [out=0,in=180] (hmm21);
				
				\draw[thick,->,shorten >=1pt] (hmm31) to [out=0,in=180] (hmm32);
				\draw[thick,->,shorten >=1pt] (hmm32) to [out=0,in=180] (hmm33);
				\draw[thick,->,shorten >=1pt] (hmm33) to [out=0,in=180] (hmm34);
				\draw[thick,->,shorten >=1pt] (hmm34) to [out=0,in=180] (hmm35);
				\draw[thick,->,shorten >=1pt] (hmm35) to [out=0,in=180] (hmm36);
				\draw[thick,->,shorten >=1pt] (hmm36) to [out=0,in=180] (hmm37);
				\draw[thick,->,shorten >=1pt] (hmm37) to [out=0,in=180] (hmm38);
				\draw[thick,->,shorten >=1pt] (hmm38) to [out=0,in=180] (hmm39);
				\draw[thick,->,shorten >=1pt] (hmm39) to [out=0,in=180] (hmm40);
				\draw[thick,->,shorten >=1pt] (hmm40) to [out=0,in=180] (hmm41);
				\draw[thick,->,shorten >=1pt] (hmm41) to [out=0,in=180] (hmm42);
				
				\draw[thick,->] (hmm1.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm2.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm3.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm4.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm5.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm6.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm7.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm8.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm9.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm10.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm11.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm12.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm13.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm14.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm15.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm16.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm17.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm18.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm19.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm20.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm21.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm31.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm32.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm33.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm34.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm35.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm36.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm37.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm38.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm39.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm40.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm41.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm42.70) arc (-60:245:4mm);
				
				
				\draw[thick,->,shorten >=1pt] (start) to [out=0,in=180] (hmm1);
				\draw[thick,->,shorten >=1pt] (start) to [out=-20,in=160] (hmm31);
				
				\draw[thick,->,shorten >=1pt] (hmm9) to [out=0,in=180] (P3);
				\draw[thick,->,shorten >=1pt] (P3) to [out=0,in=160] (stop);
				\draw[thick,->,shorten >=1pt] (hmm12) to [out=0,in=180] (P4);
				\draw[thick,->,shorten >=1pt] (P4) to [out=0,in=135] (stop);
				\draw[thick,->,shorten >=1pt] (hmm21) to [out=0,in=180] (stop);
				
				\draw[thick,->,shorten >=1pt] (hmm36) to [out=-20,in=180] (P5);
				\draw[thick,->,shorten >=1pt] (P5) to [out=0,in=180] (P6);
				\draw[thick,->,shorten >=1pt] (P6) to [out=0,in=230] (stop);
				
				\draw[thick,->,shorten >=1pt] (hmm42) to [out=0,in=210] (stop);
				
				\draw[thick,->,shorten >=1pt] (stop) to [out=-45,in=0] (P1);
				\draw[thick,->,shorten >=1pt] (P1) to [out=180,in=0] (P2);
				\draw[thick,->,shorten >=1pt] (P2) to [out=180,in=225] (start);
				
				\end{scope}			
				\end{tikzpicture} \\
				
				\hline
			\end{tabular}
			\caption{Zoptymalizowany automat dla słów: \textit{kot}, \textit{kos}, \textit{kasa}, \textit{masa} oraz \textit{ma}}
			\label{fig:Graph_complex}
			
		\end{figure}
	\end{landscape}
    
	   \subsubsection{ Rozpoznawanie}
		   Mając zamodelowany słownik, za pomocą ukrytych modeli Markova zgodnie z opisem w rozdziale \ref{sec:Phones}, można przejść do rozpoznawania mowy. Proces rozpoznawania jest w rzeczywistości wyszukiwaniem najbardziej prawdopodobnej ścieżki w automacie, wraz z którą wiąże się sekwencja wypowiedzianych słów (blok \refBlock{lancuch_markova} na rysunku \ref{fig:ARM_schemat}).  W rozdziale \ref{sec:Viterbi} opisano algorytm \ref{alg:viterbi}, który wylicza ścieżkę której szukamy. Chcąc go wykorzystać do rozpoznawania, należy zmodyfikować algorytm o uwzględnianie prawdopodobieństwa słów z modelu językowego, które wyznaczamy za pomocą n-gramów, tak jak opisano w rozdziale \ref{sec:ngramm}. Ponadto należy dodać fragment kodu zapamiętującego słowa przez, które prowadzi ścieżka.\\
		   Model językowy uwzględnia się poprzez pomnożenie prawdopodobieństwa ścieżki przez prawdopodobieństwo rozpoznanego słowa, przy przechodzeniu do stany \textit{stop}.\\
		   W rzeczywistości wiele systemów rozpoznawania mowy, w szczególności \textit{Julius}, wykorzystują algorytm \textit{token passing} (\ref{alg:token_passing}), który jest wariancją algorytmu Viterbiego. Czas działania algorytmu \ref{fig:viterbi} to $O(T\cdot |Q|^2)$, co przy słowniku zawierającym kilkaset tysięcy słów przekłada się na bardzo wolny czas działania. W algorytmie \textit{token passing} uwzględnia się jedynie stany do których jest przejście, co znacząco przyśpiesza procedurę wyznaczania ścieżki. \\
		   Ponadto w algorytmie \ref{alg:token_passing} można zastosować heurystyczną metodę przyśpieszania, polegającą na przycinaniu zbiory $Q2$ do zadanej wielkości, zwanej \textit{szerokością wiązki}. Podczas przycinania pozostawia się jedynie $n$ par o największym prawdopodobieństwie, gdzie $n$ jest empirycznie wyznaczanym parametrem. W praktyce, znaczące przyśpieszenie przy jednoczesnym znikomym spadku skuteczności rozpoznawania otrzymuje się dla $n \in [500, 2000]$.
		    
		    \begin{figure}
		    	\begin{algorithmic}[1]
		    		
		    		\REQUIRE {tablica obserwacji $O$ o rozmiarze T}
		    		\REQUIRE {macierz przejścia $A$ o rozmiarze $|Q| \times |Q|$}
		    		\REQUIRE {zbiór krawędzi $E$, które występują w automacie}
		    		
		    		\STATE $Q1 = \{(start, 1)\}$; 
		    		\STATE $Q2 = \phi$;		    
		    		
		    		\FOR{$t=1$ to $T$}
			    		\STATE $Q2 = \phi$
			    		\FOR{$(q1, p1)$ in $Q1$}
				    		\FOR{$\{q2: (q1, q2) \in E \} $}
					    		\STATE $p_{q_2} = \bigg( p1\cdot a_{q,w}\cdot b_q(O_t) \label{line:b_q} \bigg)$;				    	
					    		\STATE $Q2 = Q2 \cup \{(q2, p_{q_2})\}$
				    		\ENDFOR
			    		\ENDFOR
			    		\STATE $swap(Q1, Q2)$;
		    		\ENDFOR
		    		
		    		\RETURN $\argmax{Q1}$;
		    	\end{algorithmic}
	    	
		    	Zbiory $Q1$, $Q2$ przechowują pary $(stan, parwdopodobieństwo)$ 
		    	
		    	\caption{Algorytmy token passing}
		    	\label{alg:token_passing}
		    \end{figure}		
		    
		    
       \subsubsection{ Modele bezkontekstowe i kontekstowe}
       \subsubsection{ Metody redukcji liczby stanów modeli kontekstowych }	

\section{N-gramowy model językowy}
	\label{sec:ngramm}
	Dosłownie kilka słów o n-gramach i wygładzaniu (to nie jest temat tej pracy)
    \subsection{ Rodzaje modeli językowych }
      Omówienie modeli stochastycznych, gramatycznych, moodeli językowych wykorzystujących sieci głębokiego uczenia (krótko)
    \subsection{ Estymacja parametrów modelu językowego na podstawie korpusu tekstów }
    \subsection{ Metody wygładzania stochastycznych modeli językowych }


\section{Rozpoznawanie mowy z wykorzystaniem sieci neuronowych}
	\label{sec:ASR_NN}
	W prezentowanym podejściu do rozpoznawania mowy z wykorzystaniem sieci neuronowych, sieć jest wykorzystywana do wyznaczania prawdopodobieństw stanów na podstawie wektora cech, co odpowiada blokwi \refBlock{klasyfikator} z rysunku \ref{fig:ARM_schemat}).
	
	\subsection{Sieć jako estymator prawdopodobieństw stanów }
	\label{sec:nn_estymator}
		Sieć neuronowa wykorzystywana jako klasyfikator, dostaje na wejściu $k$-elementowy wektor cech $v \in \mathbb{R}^k$, natomiast na wyjściu zwraca $n$-elementowy wektor $w \in \mathbb{R}_+^n$, gdzie $n$ jest liczbą rozpoznawanych klas. W idealnym klasyfikatorze $w \in \{0,1\}^n$ i dokładnie jeden element ma wartość $1$, a pozostałe $0$. W praktyce jednak uznajemy, że klasyfikator wybiera klasę o największej wartości. Aby nadać wyjściu klasyfikatora charakter prawdopodobieństwa, należy nałożyć na niego funkcję \textit{softmax} opisaną równaniem \ref{eqn:softmax}. Po złożeniu z funkcją \ref{eqn:softmax} klasyfikator będzie wyznaczał $P(w_i|v)$, czyli prawdopodobieństwo $i$-tej klasy pod warunkiem wektora wejściowego $v$.
		
		\begin{equation}
			\sigma(w)_k=\frac{\mathrm{e}^{w_k}}{\sum_{i=1}^{n} \mathrm{e}^{w_i}}
			\text{,   gdzie $n$ to szerokości wektora $w$}
			\label{eqn:softmax}
		\end{equation}
		
		Zgodnie z równaniem \ref{equation:ASR_definicja2} do rozpoznawani mowy potrzeba $P(O|W)$, co oznacza, że klasyfikator musi wyznaczać $P(v|w_i)$. Korzystając ze wzoru Bayes i obserwacji, że $P(v)$ nie ma wpływu na rozpoznawanie, gdyż jest takie samo dla wszystkich hipotez, otrzymujemy:
		
		\begin{equation}
			P(v|w_i) = \frac{P(w_i|v) \cdot P(w_i)}{P(v)}\simeq P(w_i|v) \cdot P(w_i)
			\label{eqn:softmax2}
		\end{equation}
		
		 W proponowanym podejściu zastosowania sieci neuronowej przy rozpoznawaniu mowy, wejściem sieci jest $k$ $123$-elementowych wektorów cech, wyznaczanych zgodnie z opisem w rozdziale \ref{sec:Feature_vec}. Parametr $k$ określa liczbę kolejnych ramek dostarczanych sieci w wektorze wejściowym. Poprzez \textit{szerokość kontekstu} rozumieć będziemy liczbę ramek na prawo i lewo od rozpoznawanej obserwacji, zatem $k = 2 \cdot kontekst + 1$. Zwiększanie kontekstu wiąże się ze zmniejszeniem liczby obserwacji, gdyż pierwsza obserwacja może wystąpić dopiero po liczbie ramek równej $kontekst + 1$. Rysunek \ref{fig:nn_context} pokazuje, jak kolejne ramki są agregowane przy tworzeniu obserwacji. W przykładzie z rysunku szerokość kontekstu wynosi $2$, zatem każda obserwacja jest tworzona z $5$ kolejnych ramek. Wyjściem sieci jest wektor o szerokości równej liczbie stanów modelu Markova. Uwzględniając równanie \ref{eqn:softmax2}, wartość funkcji $b_q(O_t)$ z \ref{line:b_q}. linii algorytmu \ref{alg:token_passing} odpowiada wyrażeniu:
		 
		 \begin{equation}
			 b_q(O_t) = NN(O_t) \cdot P(w_q)			 
		 \end{equation}
		 ,gdzie NN to sieć neuronowa rozumiana jako funkcja.
		
	\subsection{Prawdopodobieństwo stanów \textit{apriori} }
		W rozdziale \ref{sec:nn_estymator} opisano, jak wykorzystać sieć neuronową do wyznaczania prawdopodobieństw stanów, jednak wymaga to wcześniejszego wyznaczenia prawdopodobieństw stanów apriori, czyli $P(w_i)$ ze wzoru \ref{eqn:softmax2}. 
		$P(w_i)$ może być łatwo wyznaczone podczas przygotowywani danych do uczenia sieci neuronowej. Na etapie generowania wektorów wyjściowych do treningu, możliwe jest zliczanie wystąpień stanu $q_i$.
		Następnie wystarczy dla każdego stanu obliczyć:
		
		\begin{equation}
			P(w_k) = \frac{c_k}{\sum_{i=1}^{|Q|} c_i}
			\text{,   gdzie $c_i$ to liczba wystąpień stanu $q_i$}
		\end{equation}
		
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[node distance=0.8cm]
			
			\begin{scope}
			\node [frame] (fr1) {$t_1$};
			\node [frame, below of=fr1] (fr2) {$t_2$};
			\node [frame, below of=fr2] (fr3) {$t_3$};
			\node [frame, below of=fr3] (fr4) {$t_4$};
			\node [frame, below of=fr4] (fr5) {$t_5$};
			\node [frame, below of=fr5] (fr6) {$t_6$};
			\node [frame, below of=fr6] (fr7) {$t_7$};
			\node [frame, below of=fr7] (fr8) {$t_8$} node [black,midway,yshift=-2.3cm, xshift=5.8cm] {\footnotesize $\ldots$};;
			\node [frame, below of=fr8] (fr9) {$t_9$};
			\node [frame, below of=fr9] (fr10) {$t_{10}$};
			\node [frame, below of=fr10] (fr11) {$t_{11}$};
			\node [frame, below of=fr11] (fr12) {$t_{12}$};
			\node [frame, below of=fr12] (fr13) {$t_{13}$};
			\node [frame, below of=fr13] (fr14) {$t_{14}$};
			\node [frame, below of=fr14] (fr15) {$t_{15}$};
			
			
			
			
			\draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=5pt},yshift=10pt]
			(fr1.north west) -- (fr5.south west) node [black,midway,yshift=-1.5cm] {\footnotesize $o_3$};
			\draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=12pt},yshift=10pt]
			(fr2.north west) -- (fr6.south west) node [black,midway,yshift=-1.5cm] {\footnotesize $o_4$};
			\draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=19pt},yshift=10pt]
			(fr3.north west) -- (fr7.south west) node [black,midway,yshift=-1.5cm] {\footnotesize $o_5$};
			\draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=26pt},yshift=10pt]
			(fr4.north west) -- (fr8.south west) node [black,midway,yshift=-1.5cm] {\footnotesize $o_6$};
			
			\draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=5pt},yshift=10pt]
			(fr8.north west) -- (fr12.south west) node [black,midway,yshift=-1.5cm] {\footnotesize $o_{10}$};
			\draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=12pt},yshift=10pt]
			(fr9.north west) -- (fr13.south west) node [black,midway,yshift=-1.5cm] {\footnotesize $o_{11}$};
			\draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=19pt},yshift=10pt]
			(fr10.north west) -- (fr14.south west) node [black,midway,yshift=-1.5cm] {\footnotesize $o_{12}$};
			\draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=26pt},yshift=10pt]
			(fr11.north west) -- (fr15.south west) node [black,midway,yshift=-1.5cm] {\footnotesize $o_{13}$};
		
			\end{scope}
			
			\end{tikzpicture}
			\caption{Tworzenie obserwacji dla sieci neuronowej przy kontekście równym $2$.}
			\label{fig:nn_context}
			
		\end{figure}
		
	\subsection{Architektura sieci i układ danych}
		
	\subsection{Zastosowanie sieci DNN w ARM}
	\subsection{Zastosowanie sieci konwolucyjnych w ARM}
	\subsection{Skuteczność ARM przy zastosowaniu sieci neuronowych}
	
	Ten rozdział ma na pokazać, jak wcześniej opisane elementy składają się na działający system do rozpoznawani.

\section{ Zastosowanie modelowania fonemów z ograniczonym kontekstem w ASR }	
	Wykorzystując ukryte modele Markova, tak jak opisano w rozdziale \ref{sec:ASR_HMM}, wymaga policzenia, dla każdej ramki, prawdopodobieństwa każdego ze zdefiniowanych fonemów. Wykorzystując tri-fony przy rozpoznawaniu mowy trzeba zdefiniować $O(\sigma^3)$ modeli. Jednak tylko niektóre z tri-fonów o wspólnym środkowym fonie różnią się w istotny sposób. Oczywiście wraz ze wzrostem liczby fonemów, rośnie liczba obliczeń, które trzeba wykonać, co ma negatywny wpływ na szybkość rozpoznawania. Ponadto, przy wykorzystaniu sieci neuronowych zgodnie z podejściem opisanym w rozdziale \ref{sec:ASR_NN} trzeba wytrenować więcej parametrów, co z kolei, w ogóle utrudnia lub wręcz uniemożliwia proces uczenia. Aby ograniczyć negatywny wpływ dużej liczby fonemów, zastosowano technikę opisaną w rozdziale \ref{sec:tri-fone_mapping}. Dzięki niej można było znacznie ograniczyć szerokość warstwy wyjściowej, które jest równa liczbie fizycznych fonemów.
	
	\subsection{ Problemy wynikające z zastosowania kontekstowych modeli fonemów }
	\subsection{ Definicja modelu z ograniczonym kontekstem }
		\label{sec:tri-fone_mapping}
		Korzystając ze spostrzeżenia, że tylko niektóre tri-fony o wspólnym środkowym fonie istotnie się różnią, można zastosować mechanizm współdzielenia \textit{fizycznych modeli} przez wiele tri-fonów. Poprzez \textit{fizyczne tri-fony} będziemy tu rozumieć modele, które mają bezpośrednio zdefiniowane parametry, czyli przypisane stany modelu Markova oraz macierz przejścia pomiędzy nimi. Poprzez \textit{tri-fony wirtualne} będziemy rozumieć modele, które są zdefiniowane poprzez jakiś tri-fon fizyczny. Innymi słowy, istnieje mapowanie z tri-fonu wirtualnego na tri-fon fizyczny. Rysunek \ref{fig:virtual_phisical_tri-phones} pokazuje przykładowe mapowanie. Znajdujące się na nim tri-fony $d-b+d$, $k-l+m$ oraz $u-y+t$ sa fizyczne, natomiast $d-b+e$, $a-b+c$, $x-y+z$ oraz $d-y+t$ sa wirtualne. Ponadto fizyczne modele mogą współdzielić stany, przy czym drugi, środkowy stan jest zawsze współdzielony przez wszystkie tri-fony o takim samym środkowym fonie. Oba mechanizmy umożliwiają bardzo skuteczne ograniczenie liczby parametrów, które trzeba wyliczyć. W tabeli \ref{tab:acustic_models} umieszczono zestawienie modeli akustycznych wytrenowanych na potrzeby eksperymentu, w kolumnie \textit{Liczba stanów} wpisano liczbę stanów, do jakiej ograniczono modele tri-fonowe. \\
		Poprzez \textit{Model z Ograniczonym Kontekstem} będziemy rozumieć akustyczny model tri-fonowy, w którym ograniczono liczbę stanów modelu Markova poprzez oba opisane powyżej mechanizmy.
		
		\begin{landscape}
			\begin{figure}[H]
			\centering
			\begin{tikzpicture}[node distance=1.7cm]
			
			\begin{scope}
						
			\def\x{1.5}
			\def\w{\x * 5}
			\def\z{\x * 2}
			\def\y{2.5}
			\def\j{4.0}
			
			\node [hmm2] (p1) {d-b+e};
			\node [hmm2, right = \x cm of p1] (p2) {a-b+c};
			\node [hmm2, below = \y cm of p2] (p3) {c-b+d};
			
			\node [hmm2, right = \z cm of p3] (p4) {k-l+m};
			
			\node [hmm2, right = \w cm of p2] (p5) {x-y+z};
			\node [hmm2, right = \x cm of p5] (p6) {w-y+t};
			\node [hmm2, below = \y cm of p5] (p7) {u-y+t};
			

			\draw [->] (p1) [out=270, in=140] to  (p3);
			\draw [->] (p2) [out=280, in=80] to  (p3);
			
			\draw [->] (p6) [out=270, in=40] to  (p7);
			\draw [->] (p5) [out=260, in=100] to  (p7);
			
			\node [hmm, below = \j cm of p3] (s1) {s1};
			\node [hmm, right = \x cm of s1] (s2) {s2};
			\node [hmm, right = \x cm of s2] (s3) {s3};
			\node [hmm, right = \x cm of s3] (s4) {s4};
			\node [hmm, right = \x cm of s4] (s5) {s5};
			
			\draw [->] (p3) [out=240, in=100] to (s1);
			\draw [->] (p3) [out=270, in=90]  to (s2);
			\draw [->] (p3) [out=300, in=130] to (s5);
			
			\draw [->] (p4) [out=240, in=80]  to (s1);
			\draw [->] (p4) [out=270, in=90]  to (s3);
			\draw [->] (p4) [out=300, in=100] to (s5);
			
			\draw [->] (p7) [out=240, in=50]  to (s1);
			\draw [->] (p7) [out=270, in=90]  to (s4);
			\draw [->] (p7) [out=300, in=80] to (s5);
			\end{scope}
			
			\end{tikzpicture}
			\caption{Mapowanie tri-fonów wirtualnych na tri-fony fizyczne}
			\label{fig:virtual_phisical_tri-phones}
			
		\end{figure}
	\end{landscape}
		
		
	\subsection{ Uczenie sieci NN estymującej prawdopodobieństwa stanów dla modeli z ograniczonym kontekstem }
		\subsubsection{ Redukowanie liczby stanów fizycznych w modelu }
		\subsubsection{ Wiązanie stanów logicznych ze stanami zisycznymi }
		\subsubsection{ Przygotowanie danych treningowych }
		\subsubsection{ Półautomatyczna weryfikacja poprawności danych treningowych }
		\subsubsection{ Wyznaczanie prawdopodobieństwa stanów apriori }
		\subsubsection{ Architektura konwolucyjnej sieci neuronowej }
		\subsubsection{ Kompletny algorytm przygotowania danych i uczenia sieci CNN }
		\subsubsection{ Znaczenie optymalizacji metaparametrów sieci/uczenia dla końcowej skuteczności ARM }	
	\subsection{ Teza pracy }

\section{ Wykorzystane technologie i biblioteki }
	\subsection{ Komponenty do budowy modeli akustycznych i językowych z pakietu HTK }
	\subsection{ Dekoder ARM Julius }
	\subsection{ Theano }
	\subsection{ Architektura systemu do eksperymentowania z wykorzystaniem sieci CNN }
		
\section{Opis danych}
	\label{sec:opis_danych}

	W celu przetestowania proponowanych metod wykorzystano studyjny korpus Clarin\footnote{http://mowa.clarin-pl.eu/korpusy/}. Składa się on z 56 godzin polskich nagrań o różnej tematyce, nagranych przez 554 różnych mówców. Każdy mówca nagrał 20 lub 30 wypowiedzi o długości od 6 do 22 sekund. Korpus nie jest zbalansowany pod względem płci. Wypowiedzi mają przypisane transkrypcje, jednak znajduje się w wiele błędów i artefaktów, takich jak zająknięcia (4. wypowiedz 191. mówcy), seplenienie (200. mówca) czy błędne odczytanie tekstu (18. wypowiedz 294. mówcy) skutkujące różnicą pomiędzy transkrypcja a faktycznie wypowiedzianymi słowami. Artefakty utrudniają lub wręcz uniemożliwiają wykorzystanie niektórych nagrań do uczenia modelu akustycznego. Dźwięk został nagrany z próbkowaniem 16 kHz i jakością 16 bitów na próbkę, a następnie zapisany w niekompresowanym formacie wav. Wypowiedzi zaczynają się i kończą ciszą. 
\\
W fazie wstępnego przetwarzania danych, nagrania zostały poddane następującym procesom:
\begin{itemize}
	\item automatyczne wyrównanie poziomu nagrań to stałego poziomu -6dB (1/2 maksymalnego możliwego do osiągnięcia poziomu) dla najgłośniejszych miejsc całego nagrania
	\item odrzucenie fragmentów ciszy/szumów tła o długości przekraczającej 0.7 sek,
	\item odrzucenie tych nagrań dla których faktycznie wypowiedziana fraza w znaczący sposób różniła się od frazy zadeklarowanej w dostarczonej transkrypcji.
\end{itemize}
W celu odrzucenia nagrań o niepoprawnej transkrypcji, przepuszczono je przez system do rozpoznawania mowy. Nagrania, których transkrypcje różniła się znacząco od wyniku rozpoznawania zostały odrzucone.

\section{Eksperymenty}


	\begin{figure}
	\begin{tabular}{|l|c|c|c|c|} \hline
		
		Nazwa modelu & \vtop{\hbox{\strut Min liczba}\hbox{\strut obserwacji}} &
		\vtop{\hbox{\strut Próg}\hbox{\strut poprawy}}& \vtop{\hbox{\strut Liczba}\hbox{\strut stanów /modeli}}& Skuteczność \\ \hline
		
		MODEL\_CL\_SUPERPURE & 100   & 350 & 4192 / 41309 & 81.58 \\
		MODEL\_CL\_25000     & 25000 & 350 & 304 / 8074   & 76.90 \\
		MODEL\_CL\_33000     & 33000 & 350 & 231          &   -   \\
		MODEL\_CL\_37000     & 37000 & 350 & 214 / 7775   & 74.97 \\
		???????              &   -   & 350 & 147          &   -   \\
		???????              &   -   & 350 & 122          &   -   \\
		MODEL\_UNIFONOWY     &  N/A  & N/A &     N/A      & 71.86 \\
		\hline
	\end{tabular}
	\caption{\label{tab:acustic_models}Wytrenowane modele akustyczne}
	\end{figure}

	 \begin{figure}
	 \begin{adjustwidth}{-1cm}{}
	\begin{tabular}{|c|c|c|c|c|c|c|} \hline
		
		Nazwa modelu & Liczba wyjść & Kontekst & \vtop{\hbox{\strut Parametry}\hbox{\strut pierwszego}\hbox{\strut przejścia}}  & 
		\vtop{\hbox{\strut Parametry}\hbox{\strut drugiego}\hbox{\strut przejścia}} & \vtop{\hbox{\strut Skuteczność}\hbox{\strut corr}} & \vtop{\hbox{\strut Skuteczność}\hbox{\strut acc}} \\
		\hline
		\multirow{7}{*}{Model 240424} &
		\multirow{7}{*}{-}            &
		\multirow{7}{*}{5}            & 13 -8 & 13 -9 & 84.88 & 81.09 \\	
									  &&& \textbf{12 -7} & \textbf{15 -9} & \textbf{85.47} & \textbf{81.55}\\
									  &&& 12 -7 & 12 -7 & 85.31 & 81.32 \\
									  &&& 11 -7 & 11 -7 & 85.16 & 81.03 \\
									  &&& 10 -7 & 10 -7 & 84.98 & 80.72 \\
									  &&& 7 -8  & 7 -8  & 82.8  & 78.19 \\
									  &&& 5 -8  & 5 -8  & 78.98 & 73.37 \\
		\hline
		\multirow{8}{*}{Model 268154} &
		\multirow{7}{*}{122}            &
		\multirow{8}{*}{5}          & \textbf{13 -8} & \textbf{13 -9} & \textbf{85.56} & \textbf{81.69} \\
									  &&& 12 -7 & 15 -9 & 86.92 & 81.48 \\
									  &&& 12 -7 & 12 -7 & 85.71 & 81.61 \\
									  &&& 11 -7 & 11 -7 & 85.72 & 81.50 \\
									  &&& 10 -7 & 10 -7 & 85.57 & 81.23 \\
									  &&& 7 -8  & 7 -8  & 83.24 & 78.47 \\
									  &&& 5 -8  & 5 -8  & 79.62 & 73.9  \\
									  &&& 18 -7 & 16 -12& 82.33 & 79.04 \\
		\hline					
		\multirow{7}{*}{Model 268151} &	
		\multirow{7}{*}{214}            &	  
		\multirow{7}{*}{5}          & 13 -8 & 13 -9 & 85.19 & 81.44 \\
									  &&& \textbf{12 -7} & \textbf{15 -9} & \textbf{88.58} & \textbf{83.39} \\
									  &&& 12 -7.5 & 15 -9.5&  88.44 & 83.33 \\
									  &&& 12 -7 16& -9.5   & 88.78  & 83.04 \\
									  &&& 12 -7 12& -7  & 84.98 & 81.09 \\ 
									  &&& 11 -7 11& -7  & 84.54 & 80.61 \\
									  &&& 18 -7 16& -12 & 82.88 & 79.76 \\
		\hline					
		\multirow{5}{*}{Model 268148} &	
		\multirow{7}{*}{304}            &	  
		\multirow{5}{*}{5} & 13 -8 & 13 -9 & 64.85 & 54.30 \\
									  &&& \textbf{12 -7} & \textbf{15 -9} & \textbf{71.22} & \textbf{57.42} \\
									  &&& 12 -7 & 12 -7 & 64.07 & 52.58 \\
									  &&& 7 -8  & 7 -8  & 54.39 & 39.58 \\
									  &&& 18 -7 & 16 -12& 62.64 & 55.14 \\
		\hline
		
	\end{tabular}
	\caption{\label{tab:text1}Wyniki cz.1}
	\end{adjustwidth}
	\end{figure}

	\begin{figure}
	\begin{adjustwidth}{-3.8cm}{}
	\begin{tabular}{|c|c|c|c|c|c|c|} \hline
		Nazwa modelu & Liczba wyjść & Kontekst & \vtop{\hbox{\strut Parametry}\hbox{\strut pierwszego}\hbox{\strut przejścia}}  & 
		\vtop{\hbox{\strut Parametry}\hbox{\strut drugiego}\hbox{\strut przejścia}} & \vtop{\hbox{\strut Skuteczność}\hbox{\strut corr}} & \vtop{\hbox{\strut Skuteczność}\hbox{\strut acc}} \\
		\hline
		\multirow{9}{*}{Model 268159} &
		\multirow{7}{*}{231}            &
		\multirow{9}{*}{5}          & 18 -7 & 16 -12 & 83.43 & 79.90 \\
		&&& 12 -7 & 15 -9  & 89.25 & 83.37 \\
		&&& 12 -7 & 15 -9  & 89.11 & 83.40 \\
		&&& \textbf{12 -10}& \textbf{15 -10} & \textbf{88.80} & \textbf{83.48} \\
		&&& 13 -10& 15 -10 & 88.10 & 83.37 \\
		&&& 7 -8  & 7 -8   & 79.79 & 74.63 \\
		&&& 12 -7 & 12 -7  & 85.22 & 80.68 \\
		&&& 13 -11& 15 -11 & 87.92 & 83.34 \\
		&&& 14 -12& 15 -12 & 86.24 & 82.19 \\
		\hline
		\multirow{7}{*}{Model 268158} &
		\multirow{7}{*}{147}            &
		\multirow{7}{*}{5}          & 18 -7 & 16 -12 & 82.88 & 79.64 \\
		&&& 12 -7 & 15 -9  & 88.56 & 83.18 \\
		&&& \textbf{12 -10}& \textbf{15 -10} & \textbf{88.17} & \textbf{83.24} \\
		&&& 7 -8  & 7 -8   & 73.31 & 78.59 \\
		&&& 11 -10& 11 -10 & 84.95 & 81.12 \\
		&&& 11 -9 & 11 -9  & 85.32 & 81.32 \\
		&&& 12 -9 & 12 -9  & 85.51 & 81.55 \\
		\hline
		MODEL\_CL\_33000, rectify        & 231 & 5 & 12 -10 & 15 -10 & 88.84 & 83.50 \\
		\hline
		MODEL\_CL\_33000, leaky\_rectify & 231 & 5 & 12 -10 & 15 -10 & 89.19 & 83.89 \\
		\hline
		MODEL\_CL\_33000, leaky\_rectify & 231 & 7 & 12 -10 & 15 -10 & 88.95 & 83.56 \\
		\hline
		MODEL\_CL\_33000, leaky\_rectify & 231 & 9 & 12 -10 & 15 -10 & 88.90 & 83.49 \\
		\hline
		MODEL\_CL\_33000, leaky\_rectify & 231 & 2 & 12 -10 & 15 -10 & 89.24 & 83.64 \\
		\hline
		
	\end{tabular}
	\end{adjustwidth}
	\caption{\label{tab:text2}Wyniki cz.2}
	\end{figure}


	\begin{figure}
		\begin{tabular}{|c|c|c|c|c|c|} \hline
			Nazwa modelu & \vtop{\hbox{\strut Liczba neuronów}\hbox{\strut w warstwie ukrytej}}  & 
			\vtop{\hbox{\strut Liczba }\hbox{\strut filtrów}} & \vtop{\hbox{\strut Skuteczność}\hbox{\strut corr}} & \vtop{\hbox{\strut Skuteczność}\hbox{\strut acc}} \\
			\hline
			siec 16 & 1200 & 32 & 88.54 & 83.10 \\
			\hline
			siec 17 & 600 & 32 & 87.52 & 82.01 \\
			\hline
			siec 18 & 300 & 32 & 86.90 & 81.36 \\
			\hline
			
		\end{tabular}
		\caption{\label{tab:text2}Wpływ liczby neuronów na skuteczność rozpoznawania.}
	\end{figure}


    Należy tutaj jasno określić cel przeprowadzanych badań i go uzasadnić. Uzasadnić potrrzebę dobrania optymalnych parametrów dekodowania, osobno dla GMM-HMM i CNN-HMM. Każdy z podpunktów musi kończyć się krótką analizą zaprezentowanych wyników oraz próbą ich uzasadnienia.
	\subsection{ Środowisko sprzętowo-programisytyczne }
	\subsection{ Optymalizacja parametrów procesu dekodowania }
	\subsection{ Wpływ sposobu wyznaczania cech na skuteczność rozpoznawania }
	Tutaj należy porównać skuteczność rozpoznawania dla cech MFCC i MFSC dla GMM-HMM i CNN-HMM oraz uzasadnić otrzymamy wynik (dla GMM lepsze MFCC, dla CNN lepsze MFSC)
	\subsection{Wpływ liczby fizycznych stanów na skuteczność rozpoznawania }
	Należy tutaj przetestować równocześnie (dla tych samych danych) model GMM-HMM i CNN-HMM, dla neutralnych parametrów lmp/lmp2 i dla parametrów optymalizowanych na zbiorze walidacyjnym
	\subsection{ Wpływ głębokości/architektury sieci na skuteczność rozpoznawania }
	\subsection{ Wpływ szerokości kontekstu na skuteczność rozpoznawania }
	\subsection{ Porównanie efektywności czasowej modeli GMM i CNN }

\section{ Podsumowanie }
  Tej sekcji zwykle nie dzieli się już na podsekcje Zqamieścić tutaj ocenę uzyskanych wyników, praktycznego znaczenia uzyskanych wyników, napotkane problemy, możliwe dalsze prace rozwojowe
   
   
	\nocite{*}
	\bibliographystyle{plain}
	\bibliography{bibliografia}
\end{document}