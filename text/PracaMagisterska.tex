\documentclass[shortabstract, mgr]{iithesis}

\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage[polish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{hyperref}
\usepackage{float}
\usepackage{changepage}
\usepackage{pdflscape}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{array}
\usepackage{algorithmic}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}

\author{Aleksander Sas}
\polishtitle{Wykorzystanie sieci neuronowych rozpoznających trifony w procesie analizy mowy}
\englishtitle{Using neural network recognizing triphones in speech analyze}
\advisor{dr Paweł Rychlikowski}
\polishabstract{W niniejszej pracy magisterskiej przedstawiono nowe podejście do wykorzystanie splotowych sieci neuronowych przy 		rozpoznawaniu mowy. W pierwszej części omówiono klasyczne metody wykorzystujące wielowymiarowe mikstury Gaussowskie do estymacji prawdopodobieństwo stanów modelu Markowa. W drugiej zaproponowano i opisano podejście, w którym mikstury są zastąpione splotową siecią neuronową rozpoznającą bardzo mocno ograniczony zbiór tri-fonów. Przedstawiono zarówno sposób pozyskiwania danych do treningu sieci, jak i metodą budowania wektorów cech, będących wejściem sieci. W ostatniej części zaprezentowano wyniki przeprowadzonych eksperymentów. Główny nacisk położono na przebadanie wpływu liczby rozpoznawanych przez sieć stanów na skuteczność rozpoznawania mowy}

\englishabstract{The following paper presents a new approach to using neural networks in speech recognition. In the first part classic approach, using multidimensional Gaussian mixture to estimate probability of hidden Markov states, is described. The second part presents new method using convolutional neural network recognizing limited set of triphones, instead of Gaussian mixture. Both, method to generate training data and building of input feature vector for network are described. In the last part, experiment results are presented. The main focus in experiments is on examination impact of number of recognized states to speech recognition accuracy.}

\newcolumntype{K}[1]{>{\centering\arraybackslash}p{#1}}
\DeclareMathOperator*{\argmax}{\arg\max}   % rbp
\newcounter{BlockCounter}
\newcommand{\labelBlock}[1]{%
	\smash{\raisebox{15pt}{\refstepcounter{BlockCounter}\hypertarget{#1}{}\label{#1}}}%
	(\theBlockCounter)%
}
\newcommand{\refBlock}[1]{
	\hyperref[#1]{\ref*{#1}}
}

\begin{document}

% Define block styles
\tikzstyle{hmm0}=[circle,thick,draw=gray!50,fill=gray!13,minimum size=4mm]
\tikzstyle{hmm}=[circle,thick,draw=gray!75,fill=gray!20,minimum size=6mm]
\tikzstyle{line} = [draw, -latex']


\chapter{Wprowadzenie - opis problemu}
	\textbf{Automatyczne rozpoznawanie mowy}, w skrócie \textbf{ARM}, polega na rozpoznaniu i zapisaniu nagranych słów. Jest to coraz częściej i powszechniej stosowana technologia, która znajduje szerokie zastosowanie w motoryzacji, urządzeniach mobilnych, administracji państwowej i medycznej. Jest szczególnie przydatna przy sporządzani opisów, tekstowych i protokołowaniu wszelkiego rodzaju obrad. Systemy do rozpoznawanie mowy działają na dwa sposoby
	\begin{itemize}
		\item offline, w którym system wczytuje zbiór nagrań dźwiękowych i dla każdego z nich generuje transkrypcję
		\item online, w którym system na bieżąco rejestruję dźwięk i wypisuje transkrypcję.
	\end{itemize}
	Oba tryby wymagają możliwie najlepszej skuteczności rozpoznawania, jednak tryb online jest szczególnie wymagający, gdyż dodatkowo wymaga szybkiego działania. Rozpoznawanie w trybie online powinno się odbywać w czasie zbliżony do rzeczywistego. 
	
	Rozwój kart graficznych i pojawienie się technologii CUDA zaowocowało w ostatnim dziesięcioleciu szybkim rozwojem sieci neuronowych, które w wielu problemach klasyfikacji radzą sobie znacznie lepiej niż wcześniej stosowanie algorytmy. Coraz więcej badaczy próbuje stosować sieci neuronowe w celu poprawy jakości systemów rozpoznających mowę. Można spotkać wiele różnych konfiguracji, zarówno łączących konwencjonalne systemy, jaki i autonomiczne sieci rekurencyjne.
	Jedną z pierwszych skutecznych prób zastosowania sieci neuronowych przy rozpoznawaniu mowy zaprezentowano w artykule \cite{NN_ASR_monophones}. Autorzy wykorzystują w nim klasyczną sieć typy \textit{feed-forward} do estymacji prawdopodobieństw uni-fonów. W kolejnych latach zaproponowano zastąpienie klasycznych sieci architekturą splotową oraz zamianę szeroko stosowanych cech \textit{MFCC} na \textit{MFSC}. W artykule \cite{article1} zaprezentowano właśnie takie podejście. Autorzy inspirując się wykorzystaniem głębokich, splotowych sieci neuronowych przy rozpoznawaniu obrazów, proponują przeniesie tej samej architektury do systemów rozpoznających mowę.
	
	Celem niniejszej pracy magisterskiej jest przetestowanie możliwości wykorzystania sieci neuronowych w procesie rozpoznawania mowy. Duży nacisk położony jest na możliwość rozpoznawanie tri-fonów, co jest rozwinięciem podejścia proponowanego w publikacji \cite{NN_ASR_monophones}. Przeprowadzono szereg eksperymentów w celu sprawdzenia możliwości poprawy skuteczności rozpoznawania mowy, poprzez zastosowanie sieci neuronowych rozpoznających ograniczony zbiór tri-fonów. Pokazano, jak liczba tri-fonów wpływa na skuteczność oraz porównano otrzymane wyniki z klasycznymi modelami gaussowskimi oraz sieciami neuronowymi rozpoznającymi uni-fony.

\chapter{Proces automatycznego rozpoznawania mowy (ARM) }

	\begin{figure}[H]
		\hspace*{-0.5cm}
		\begin{tikzpicture}[node distance = 1.7cm, auto]
		
		\tikzstyle{ArmBlok} = [rectangle, draw, fill=blue!20, text width=12em, text centered, rounded corners, minimum height=3em]
		\tikzstyle{model} = [ellipse, draw, fill=blue!20, text width=7em, text centered, rounded corners, minimum height=3em, node distance=5cm]
		\tikzstyle{data} = [draw, ellipse,fill=red!20, node distance=2cm,minimum height=2em]
		
		% Place nodes
		\node [data] (etap0) {Mowa};
		\node [ArmBlok,  below of=etap0] (etap1) {\labelBlock{zbieranie_sygnalu}Zbieranie sygnału akustycznego};
		\node [ArmBlok,  below of=etap1] (etap2) {\labelBlock{ekstrakcja_cech} Ekstrakcja cech};
		\node [ArmBlok,  below of=etap2] (etap3) {\labelBlock{klasyfikator} Klasyfikacja stanów};
		\node [ArmBlok,  below of=etap3] (etap4) {\labelBlock{lancuch_markova} Wyszukiwanie najlepszej ścieżki w modelu Markowa};
		\node [model, right of=etap3] (etap5) {\labelBlock{n_gramy} Model językowy};
		\node [model,    left  of=etap2] (model_akustyczny) {\labelBlock{model_akustyczny} Model akustyczny};
		\node [data,     below of=etap4] (etap6) {Rozpoznanie};
		\node [model,   right  of=etap6] (slownik) {\labelBlock{slownik} Słownik};
		
		\path [line] (etap0) -- (etap1);
		\path [line] (etap1) -- (etap2);
		\path [line] (etap2) -- (etap3);
		\path [line] (etap3) -- (etap4);
		\path [line, dashed] (etap5) |- (etap4);
		\path [line] (etap4) -- (etap6);
		\path [line, dashed] (slownik) |- (etap4);
		\path [line, dashed] (model_akustyczny) |- (etap3);
		\path [line, dashed] (model_akustyczny) |- (etap4);
		
		\end{tikzpicture}
		\caption{Etapy automatycznego rozpoznawania mowy}
		\label{fig:ARM_schemat}
	\end{figure}

	\textbf{Automatyczne rozpoznawanie mowy} możemy formalnie zdefiniować, jako znajdowanie ciągu słów $\hat{W}$ nad pewnym alfabetem $\Sigma$, o maksymalnym prawdopodobieństwie, pod warunkiem obserwacji $O$.
	
	\begin{equation}
		\hat{W}=\argmax_{W \in \Sigma^{*}}{P(W \mid O)}
		\label{equation:ASR_definicja1}
	\end{equation}
	
	Niestety wyliczanie formuły \ref{equation:ASR_definicja1} okazuję się niemożliwe do wykonania, ale korzystając ze wzory Bayesa możemy dojść do postaci \ref{equation:ASR_definicja2}, która jest już wygoda do obliczenia.
	
	\begin{equation}
		\hat{W}=\argmax_{W \in \Sigma^{*}}{P(W \mid O)} = \argmax_{W \in \Sigma^{*}}{\frac{P(O \mid W)P(W)}{P(O)}} = \argmax_{W \in \Sigma^{*}}{P(O \mid W)P(W)}
		\label{equation:ASR_definicja2}
	\end{equation}
	
	Ostatnie przejście wynika, z faktu, że $P(O)$ nie zależy od $W$, zatem możne jest pominąć.
	Implementując formułę \ref{equation:ASR_definicja2} możemy wydzielić 5 kluczowych etapów, są one zilustrowane na rysunku \ref{fig:ARM_schemat}. Opis poszczególnych bloków ze schematu znajduje się w kolejnych podrozdziałach.
	\\
	Pierwszy etap, określony jako \textit{Zbieranie sygnału akustycznego} (blok \refBlock{zbieranie_sygnalu} na rysunku \ref{fig:ARM_schemat} ), jest realizowany sprzętowo poprzez peryferyjne urządzenia. Obejmuje analogowe przetwarzanie sygnału, cyfryzację oraz opcjonalny post-processing wykonywany przez kartę dźwiękową. Na tym etapie ważne jest, aby dostroić poziom dźwięku, tak aby uniknąć przesterowań, dobrać częstotliwość próbkowania i zakres częstotliwości. Niepoprawna konfiguracja tego etapu, może skutkować zakłóceniami oraz wycięciem informacji, które mogą być istotne na kolejnych etapach przetwarzania, a w konsekwencji obniżeniem skuteczności rozpoznawania. W niniejszej pracy wykorzystany był gotowy, powszechnie dostępny korpus nagrań (patrz rozdział \ref{sec:opis_danych}), dlatego nie będę się skupiał na tym etapie.

	\section{ Fony }
	\label{sec:Phones}
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[node distance=1.7cm]
			
			\begin{scope}
			\node [hmm] (hmm1) {$s_1$};
			\node [hmm, right of=hmm1] (hmm2) {$s_2$};
			\node [hmm, right of=hmm2] (hmm3) {$s_2$};
			
			\draw[thick,->,shorten >=1pt] (hmm1) to [out=0,in=180] (hmm2);
			\draw[thick,->,shorten >=1pt] (hmm2) to [out=0,in=180] (hmm3);
			
			\draw[thick,->] (hmm1.70) arc (-60:245:4mm);
			\draw[thick,->] (hmm2.70) arc (-60:245:4mm);
			\draw[thick,->] (hmm3.70) arc (-60:245:4mm);
			
			\draw[thick,<-,shorten <=1pt] (hmm1) -- +(180:1cm);
			\draw[thick,->,shorten <=1pt] (hmm3) -- +(0:1cm);
			\end{scope}
			
			\end{tikzpicture}
			\caption{Reprezentacja fonemów}
			\label{fig:fon_hmm}
			
		\end{figure}

		Fony są podstawową koncepcją przy modelowaniu dźwięków w mowie, będącą rozszerzeniem pojęcia głoski. Typowo rozróżniają dźwięczne i bezdźwięczne warianty głosek oraz wprowadzają nowe dźwięki, takie jak przykładowo \textit{cisza}. W tabeli \ref{tab:phone_list} znajduje się lista fonów zamodelowanych w wykorzystanych modelach akustycznych, łącznie jest ich 40. Każdy zamodelowany fonem ma swój zestaw parametrów, które go opisują. Parametry te tworzą \textit{model akustyczny} odpowiadający blokowi \ref{model_akustyczny} z rysunku \ref{fig:ARM_schemat}. Słowa, które mają być rozpoznawane przez system, muszą mieć przypisaną transkrypcję na fonemy. Lista słów wraz z transkrypcją znajduje się w \textit{słowniku}, który odpowiada blokowi \refBlock{slownik} na rysunku \ref{fig:ARM_schemat}. Aby umożliwić poprawne rozpoznawanie, każda forma gramatyczne danego słowa musi się znaleźć w słowniku, gdyż jest traktowana jak niezależne słowo. Konsekwencją tego, jest bardzo duża liczba wpisów w słownikach dla języków o rozbudowanej fleksji, takich jak język polski. 
		W wykorzystanym słowniku, zawierającym $1209017$ wpisów, transkrypcje zostały zbudowane na podstawie zasad sformułowanych przez Marię Steffen-Batogową w książce \textit{Automatyzacja transkrypcji fonematycznej tekstów polskich} \cite{transkrypcja}. Zasady te nie są jednak jednoznaczne i w sytuacji, gdy dla jednego słowa można wyprowadzić wiele różnych transkrypcji, wszystkie zostają umieszczone w słowniku. Przykładem wyrazu o wielu transkrypcjach jest słowo \textit{wystrzygł}, które zgodnie z regułami, zapisuje się jako \textit{v y s t sz y k} lub \textit{v y s t sz y k ł}.  

		\begin{figure}
			\centering
			\begin{tabular}{|K{1cm}K{1cm}K{1cm}K{1cm}K{1cm}K{1cm}K{1cm}K{1cm}|}
				\hline
				a  & o\~ & b & c & cz & ć & d & dz \\ 
				dź & dż & e & e\~ & f & g & g\^ & h \\
				i & j & k & k\^ & l & ł & m & n \\
				nn & ń & o & p  & r & s & sz & ś \\
				t & u & v & y & z & ź & ż & sil \\
				\hline
			\end{tabular}
			\caption{\label{tab:phone_list}Lista modelowanych fonemów}
		\end{figure}
 

	\section{ Modelowanie fonemów, modele kontekstowe i bezkontekstrowe }
		\label{sec:trifones_definition}
		
		Przy modelowaniu fonemów wyróżnia się trzy fazy:
		\begin{itemize}
			\item początkową, podczas której aparat mowy zmienia swój kształt
			\item środkową, podczas której aparat mowy jest już w ustabilizowanej pozycji
			\item końcową, podczas której aparat mowy przechodzi do układu dla kolejnego fonemu, ale dźwięk jaki wydaje jest jeszcze bliższy aktualnemu fonemowi. 
		\end{itemize}
		Uwzględniając powyższe spostrzeżenia, wszystkie fony modeluje się za pomocą trzech stanów, tak jak na rysunku \ref{fig:fon_hmm}. Ze stanu można przejść jedynie do następnego stanu lub powrócić do samego siebie. Dzięki przejściu zwrotnemu, możliwe jest modelowanie dźwięków o różnej długości. 
		\\
		Rozwinięciem fonemów są \textbf{tri-fony}, uwzględniają one lewy i prawy kontekst poprzez dodanie informacji o fonemach stojących obok. Typowo tri-fony zapisuje się zgodnie z notacją na rysunku \ref{fig:tri-fony_notacja}, gdzie \textit{a} jest poprzednim fonemem, \textit{b} aktualnym, natomiast \textit{c} następnym. 
		
		\begin{figure}[H]
			\begin{center}
			{a-b+c}
			\end{center}
			\caption{Konwencja zapisu tri-fonów}
			\label{fig:tri-fony_notacja}
		\end{figure}
	
		W języku polskim niektóre głoski różnie się wymawia w zależności od kontekstu, czyli fonemów stojących obok. Przykładowo, inaczej brzmi głoska \textit{w} w słowie \textit{wersja}, gdzie jest dźwięczna, a inaczej w zwrocie \textit{w pracy}, gdzie jest bezdźwięczna i bardziej przypomina głoskę \textit{f}. Model tri-fonowy pozwala na uchwycenie takiej różnicy, dzięki czemu umożliwia osiągnięcie wyższej skuteczności niż model uni-fonowy.
		\\
		Należy zauważyć, że korzystając z \textit{tri-fonów} zamiast \textit{uni-fonów} wprowadzamy dodatkową informację, zarówno do modelu językowego, jak i do modelu akustycznego, ale jednocześnie znacznie zwiększamy liczbę parametrów do estymacji i czas działania silnika rozpoznającego. W modelu \textit{tri-fonowym} występuje $O(n^3)$ tri-fonów, gdzie $n$ to liczba fonemów w modelu uni-fonowym.  
		
	\section{ Ekstrakcja cech }

	 Ekstrakcja cech, odpowiadająca blokowi \refBlock{ekstrakcja_cech} z rysunku \ref{fig:ARM_schemat}, polega na zamianie sygnału akustycznego na ciąg wektorów, które mogą być następnie sklasyfikowane na kolejnym etapie. Porównanie powszechnie stosowanych metod ekstrakcji cech takich jak \textit{MFCC} czy \textit{PLP} zostało opisane w artykule \cite{feature_comparision}.
	 \subsection{Cechy \textit{MFCC}}
	 \label{sec:Feature_vec_mfcc}
	 Przy rozpoznawaniu mowy najczyściej wykorzystuje się cechy \textit{MFCC} zaproponowane przez \textit{S. Davisa} i \textit{P. Mermelsteina} \cite{mfcc}, składające się z 5 etapów przetwarzania:
	 
	 \begin{itemize}
	 	\item Podział sygnału akustycznego.
	 	\item Transformata Fouriera.
	 	\item Nałożenie zestawu filtrów.
	 	\item Nałożenie logarytmu. 
	 	\item Dyskretna transformata kosinusowa.
	 \end{itemize}
 
	 Pierwszym etapem ekstrakcji jest podzielenie sygnału na fragmenty zwane \textit{ramkami}. Typowo ramki mają $25ms$ i są przesunięte co $10ms$, przez co nachodzą na siebie. Ramki o takiej długości zawierają wystarczająco dużo informacji, a jednocześnie można założyć, że są stochastycznie stacjonarne. 
	 
	 W drugim etapie każdą ramkę poddaje się transformacie Fouriera. W efekcie tej operacji, otrzymuje się widmo sygnału akustycznego, opisujące ilość energii w zależności o częstotliwości.
	 
	 W trzecim etapie na widmo nakłada się zestaw filtrów trójkątnych, zwanych \textit{MEL-owym zestawem filtrów}. Filtry te są przesunięte względem siebie, każdy kolejny filer jest coraz szerszy, co odpowiada ludzkiej percepcji. Ludzie dobrze rozróżniają sygnały o małej częstotliwości, natomiast te o wysokiej są podobnie odbierane przez ludzkie ucho. Aby określić zestaw filtrów należy podać ich liczbę, niech będzie nią $N$ oraz zdefiniować $N+1$ częstotliwości, które wyznaczają zakresy filtrów. Załóżmy, że częstotliwości sa zapisane w ciągu $f(1), f(2), \ldots f(N+1)$, wtedy $m$-ty filtr jest zdefiniowany równaniem \ref{eqn:filter}. Na rysunku \ref{fig:filter_bank} zwizualizowano $4$ kolejne filtry. Efektem nałożenia filtrów jest nowy ciąg wartości $v_1, v_2, \ldots v_N$, które stanowy wejście do kolejnego etapu przetwarzania.
	 
	 \begin{equation}
		 H_m(x)= 
			 \begin{cases}
				 0,				 					 & x < f(m-1) 		       \\
				 \frac{x - f(m-1)}{\big(f(m) - f(m-1)\big)^2}, & f(m-1) \leq x \leq f(m) \\
				 \frac{f(m+1) - k}{\big(f(m+1) - f(m)\big)^2}, & f(m) \leq x \leq f(m+1) \\
				 0,				 					 & x > f(m+1)
			 \end{cases}
		 \label{eqn:filter}
	 \end{equation}
	 
	  \begin{figure}[H]
	 	\centering
	 	\begin{tikzpicture}[node distance=1.7cm]
	 	
	 	\begin{scope}
	 	
	 	\def\q{2.8};
	 	\def\w{3.5}
	 	
	 	\draw[-stealth] (0cm,0cm)--(9.4cm,0cm) node[right]{Hz}; 
	 	\draw[-stealth] (0cm,0cm)--(0cm,\q cm); 
	 	
	 	\def\a{0.15}; \node at (\a, -0.5) {f(1)};
	 	\def\s{1.0};  \node at (\s, -0.5) {f(2)};
	 	\def\d{2.0};  \node at (\d, -0.5) {f(3)};
	 	\def\f{3.6};  \node at (\f, -0.5) {f(4)};
	 	\def\g{5.8};  \node at (\g, -0.5) {f(5)};
	 	\def\h{8.8};  \node at (\h, -0.5) {f(6)};
	    
    	\path [line, dashed] (\s, 1.0) -- (\s, \w) node[above] {$v_1$};
   		\path [line, dashed] (\d, 1.0) -- (\d, \w) node[above] {$v_2$};
 		\path [line, dashed] (\f, 1.0) -- (\f, \w) node[above] {$v_3$}; 
		\path [line, dashed] (\g, 1.0) -- (\g, \w) node[above] {$v_4$};
	 	
	 	\pgfmathsetmacro{\z}{1.0 * \q / (\d - \a) * (\d - \a)};
	 	\draw[-] (\a cm,0cm)--(\s cm,\z cm); 
	 	\draw[-] (\d cm,0.0cm)--(\s cm,\z cm); 
	 	
	 	\pgfmathsetmacro{\z}{1.0 * \q / (\f - \s) * (\d - \a)};
	 	\draw[-] (\s cm,0.0cm)--(\d cm,\z cm); 
	 	\draw[-] (\f cm,0.0cm)--(\d cm,\z cm); 
	 	
	 	\pgfmathsetmacro{\z}{1.0 * \q / (\g - \d) * (\d - \a)};
	 	\draw[-] (\d cm,0cm)--(\f cm,\z cm); 
	 	\draw[-] (\g cm,0.0cm)--(\f cm,\z cm); 
	 	
	 	\pgfmathsetmacro{\z}{1.0 * \q / (\h - \f) * (\d - \a)};
	 	\draw[-] (\f cm,0cm)--(\g cm,\z cm); 
	 	\draw[-] (\h cm,0.0cm)--(\g cm,\z cm);
	 	
	 	\end{scope}
	 	
	 	\end{tikzpicture}
	 	\caption{Wizualizacja $4$ kolejnych filtrów trójkątnych.}
	 	\label{fig:filter_bank}
	 	
	 \end{figure}
	 
	 Czwartym etapem przetwarzania jest nałożenia logarytmu na wektor cech otrzymany w poprzednim etapie. Ma to również związek z percepcją człowieka, gdyż ludzkie zmysły, w szczególności słuch, odbierają bodźce w skali logarytmicznej. 
	 
	 Piąty etap tworzenia cech \textit{MFCC} polega na nałożeniu dyskretnej transformaty kosinusowej, zgodnie z równaniami \ref{eqn:DCT}. W efekcie jej działania otrzymamy ciąg wartości $W=(w_1, w_2, \ldots, w_N)$. Celem tego etapu jest dekorelacja wcześniej otrzymanych wartości, wynikająca z nakładania się na siebie ramek, filtrów z etapu trzeciego oraz samej natury sygnału akustycznego, zawierającego kolejne składowe harmoniczne dźwięków.
	 
	 \begin{equation}
		w_k = 
		\begin{cases}
			\sqrt{\frac{1}{N}}\sum_{m=1}^{N}v_m, & k = 1 \\
			&\\
			\sqrt{\frac{2}{N}}\sum_{m=1}^{N}{v_m \cos(\frac{\pi k \cdot (2m - 1)}{2N})}, & k = 2,3,\ldots,N
		\end{cases}
		\label{eqn:DCT}
	 \end{equation}
	 
	 Ostatni etap przetwarzania polega na przybliżeniu pierwszej i drugiej pochodnej, co daje informację o dynamicznej zmianie sygnału akustycznego. Pierwsza pochodna, dla $t$-tej ramki, może być przybliżona wzorem \ref{eqn:d1}, natomiast druga wzorem \ref{eqn:d2}.
	 
	 \begin{equation}
	 	 w_{k,t}'=\frac{w_{k,t+1}-w_{k,t-1}}{2}
	 	 \label{eqn:d1}
	 \end{equation}
	 \begin{equation}
		 w_{k,t}''=\frac{2w_{k,t+2}+w_{k,t+1}-w_{k,t-1}-2w_{k,t-2}}{10}
		 \label{eqn:d2}
	 \end{equation}
	 \begin{equation}
	 	\epsilon = \log \sum_{i=1}^N {v_i^2}
	 	\label{eqn:energy}
	 \end{equation}
	 
	 Ostatecznie, wektor cech jest składany z $M$ pierwszych wartości ciągu $W$, całkowitej energii sygnału akustycznego $\epsilon$ liczonej według wzory \ref{eqn:energy} oraz pierwszej i drugiej pochodnej wspomnianych wartości liczonych według wzorów \ref{eqn:d1} oraz \ref{eqn:d2}. Łącznie otrzymuje się wektor cech o $3(M+1)$ elementach. Rysunek \ref{fig:feature_vector} pokazuje ostateczną formę wektora cech dla $t$-tej ramki.
	 
	\begin{figure}[H]
		\begin{center}
			\begin{tabular}{l|ccccc}
				zerowa pochodna   & $\epsilon_t$   & $w_{1,t}$   & $w_{2,t}$   & $\ldots$ & $w_{M,t}$   \\
				pierwsza pochodna & $\epsilon_t'$  & $w_{1,t}'$  & $w_{2,t}'$  & $\ldots$ & $w_{M,t}'$  \\
				druga pochodna    & $\epsilon_t''$ & $w_{1,t}''$ & $w_{2,t}''$ & $\ldots$ & $w_{M,t}''$ \\
			\end{tabular}
			\label{fig:feature_vector}
			\caption{Ostateczne forma wektora cech.}
		\end{center}
	\end{figure}

	Ponadto, opcjonalnie, wektory cech mogą zostać poddane normalizacji, co uniezależnia je od niektórych czynników zewnętrznych, takich jak przykładowo głośność wypowiedzi. Normalizacji dokonuje się poprzez odjęcie średniej i podzielenie przez wariancję zgodnie ze wzorem \ref{eqn:norm}, gdzie $W$ jest zbiorem wektorów, a $W_i$ $i$-tym wektorem ze zbioru. Każdy z parametrów wektora jest normalizowany względem odpowiadających mu parametrów w innych wektorach cech ze zbiory $W$. Typowo normalizacji dokonuje się dla pojedynczej frazy, co oznacza ze zbiór $W$ zawiera jedynie wektory z jednej wypowiedzi.
	
	\begin{equation}
		\hat{W_i} = \frac{W_i - mean(W)}{var(W)}
		\label{eqn:norm}
	\end{equation}

	\subsection{Cechy \textit{MFSC}}
	\label{sec:Feature_vec_mfsc}
	
	Cechy \textit{MFSC} są uproszczoną wariacją cech MFCC, zaprojektowaną specjalnie na potrzeby \textit{splotowych sieci neuronowych}. W cechach tych nie nakłada się logarytmu ani dyskretnej transformaty kosinusowej z czwartego i piątego kroku. W efekcie otrzymuje się wektory, w których kolejne elementy są pomiędzy sobą skorelowane, co jest korzystne ze względu na splotową architekturę sieci. Cechy MFSC też mogą zostać poddane normalizacji opisanej wzorem \ref{eqn:norm}. W tabeli \ref{fig:feature_comparision} znajduje się porównanie cech MFCC oraz MFSC wraz z parametrami, jakie zostały wykorzystane w przeprowadzonych eksperymentach.
	
	\begin{figure}
		\begin{center}
			\begin{tabular}{|l|c|c|}
				\hline
				                                          & \textit{MFCC} & \textit{MFSC} \\ 
                \hline
				liczba filtrów $(N)$                      &       26      &      40       \\
				liczba filtrów w wektorze wynikowym $(M)$ &       12      &      40       \\  
				logarytmowanie wartości filtrów           &       tak     &      nie      \\
				całkowita energia                         &       tak     &      tak      \\
				pierwsza pochodna                         &       tak     &      tak      \\
				druga pochodna                            &       tak     &      tak      \\
				całkowita liczba cech                     &       39      &      123      \\
				\hline
				
			\end{tabular}			
		\end{center}
		\label{fig:feature_comparision}
		\caption{Zestawienie cech \textit{MFCC} orz \textit{MFSC}.}
	\end{figure}
	

\section {Rozpoznawanie mowy z zastosowaniem ukrytych modeli Markowa}
	\label{sec:ASR_HMM}
    \subsection{Ukryte modele Markowa - definicja}
	    \label{sec:hmm_def}
    
	    \textbf{Ukryte modele Markowa}, w skrócie \textbf{HMM}, to automat, który przechodzi pomiędzy stanami z pewnym prawdopodobieństwem $p1$ i wraz z każdym przejściem, emituje obserwację z prawdopodobieństwem $p2$. Formalnie HMM można zdefiniować jako krotkę \ref{equation:HMM_def}.
	    
	    \begin{equation}
		    HMM = (Q, O, A, B, q_0, q_F)
		    \label{equation:HMM_def}
	    \end{equation}
	    gdzie
	    \begin{align*}
		    \mathbf{Q}=\{q_1, q_2,\ldots,q_n\} & &&  \text{Zbiór stanów automatu} \\
	 	    \mathbf{O}=\{o_1, o_2,\ldots,o_k\} & &&  \text{Zbiór obserwacji} \\
	 	    \mathbf{A} =
	 	    \left| \begin{array}{ccc}
		 	    a_{1,1} & \ldots & a_{1,n} \\
		 	    \vdots  & \ddots & \vdots\\
		 	    a_{n,1} & \ldots & a_{n,n}
	 	    \end{array} \right|
												 	    & &&  \text{Macierz przejścia pomiędzy stanami} \\
												 	    & && \\
	 	    \mathbf{B}=\{B_1(o),\ldots,B_n(o)\},o \in O & && \text{zbiór rozkłądów prawdopodobieństwa emisji} \\ 
													 	& && \text{obsetwacji \textit{o} w stanie \textit{i}} \\
		 	\mathbf{q_0, q_F}				  & && \text{stany początkowy i końcowy}
	    \end{align*}
	    
	   \subsection{Algorytmy Viterbiego}
		   \label{sec:Viterbi}
		   Algorytmy Viterbiego sa dynamicznymi algorytmami do znajdowania najbardziej prawdopodobnej ścieżki w automacie oraz prawdopodobieństwa stanu $s_i$ po $t$ krokach. Znajdują zastosowanie w algorytmie \textit{Bauma-Welcha} służącym estymacji parametrów modelu, opisanym w rozdziale \ref{sec:Baum_Welch}.
		   
		   \begin{itemize}
			   	\item Aby znaleźć najbardziej prawdopodobną ścieżkę, należy skorzystać z rekurencyjnego równania \ref{equation:viterbi_path}.
			   	\item Aby znaleźć prawdopodobieństwo stanu po $k$ krokach, należy skorzystać z rekurencyjnego równania \ref{equation:viterbi_node}.
		   \end{itemize}
	   	   
		   \begin{equation}
		   P_t^q=\max_{q \in Q} \bigg( P_{t-1}^q\cdot a_{q,w}\cdot b_q(o) \bigg)
		   \label{equation:viterbi_path}
		   \end{equation}
		   
		   \begin{equation}
		   P_t^q=\sum_{q \in Q} \bigg( P_{t-1}^q\cdot a_{q,w}\cdot b_q(o) \bigg)
		   \label{equation:viterbi_node}
		   \end{equation}
		   
		  Oba równania (\ref{equation:viterbi_path} oraz \ref{equation:viterbi_node}) odwołując się jedynie do wartości z poprzedniej iteracji, zatem stosując techniki programowania dynamicznego można wyznaczyć najbardziej prawdopodobną ścieżkę stosując algorytm \ref{alg:viterbi}. Algorytm ten, w zewnętrznej pętli, przechodzi przez wszystkie ramki czasowe $T$. Następnie dla każdego stany $q1$, wyszukuje przejścia o największym prawdopodobieństwie, które doprowadziło ze stanu $q2$ do $q1$. Rysunek \ref{fig:viterbi} ilustruje jedną iterację zewnętrznej pętli. Pod koniec działania algorytmu zwracany jest końcowy stan najbardziej prawdopodobnej ścieżki. \\
		  Algorytm \ref{alg:viterbi2} pokazuje analogiczny sposób znajdowania najbardziej prawdopodobnego stanu.
		
		  \begin{figure}
			   \begin{algorithmic}[1]
				   	
					   	\REQUIRE {tablica obserwacji $O$ o rozmiarze T}
					   	\REQUIRE {macierz przejścia $A$ o rozmiarze $|Q| \times |Q|$}
					   	
					   	\STATE $T_a[0 \ldots |Q|] = 1$;
					   	\COMMENT {inicjalizacja pierwszej tablicy pomocniczej}
					   	\STATE $T_b[0 \ldots |Q|] = 0$;
					   	\COMMENT {inicjalizacja drugiej tablicy pomocniczej}
					   	
					   	\FOR{$t=1$ to $T$}
						   	\FOR{$q1=1$ to $|Q|$}
							   	\STATE $p_{tmp} = 0$;
							   	\FOR{$q2=1$ to $|Q|$}
								   	\IF {$ \bigg( T_a[q2]\cdot a_{q2,q1}\cdot b_{q1}(O_t) \bigg) > p_{tmp}$}
									   	\STATE $p_{tmp} = \bigg( T_a[q2]\cdot a_{q2,q1}\cdot b_{q1}(O_t) \bigg)$;
								   	\ENDIF
							   	\ENDFOR
							   	\STATE $T_b[q1] = p_{tmp}$
						   	\ENDFOR
						   	\STATE $swap(T_a, T_b)$;
					   	\ENDFOR
					   	
					   	\RETURN $\argmax{T_a}$;
			   \end{algorithmic}
		  \caption{Algorytmy Viterbiego znajdujący najbardziej prawdopodobną ścieżkę}
		  \label{alg:viterbi}
		  \end{figure}		
	  
		  \begin{figure}
			  \begin{algorithmic}[1]
		  		
		  		  \REQUIRE {tablica obserwacji $O$ o rozmiarze T}
		  		  \REQUIRE {macierz przejścia $A$ o rozmiarze $|Q| \times |Q|$}
		  		
		  	  	  \STATE $T_a[0 \ldots |Q|] = 1$;
		  		  \COMMENT {inicjalizacja pierwszej tablicy pomocniczej}
		  		  \STATE $T_b[0 \ldots |Q|] = 0$;
		  		  \COMMENT {inicjalizacja drugiej tablicy pomocniczej}
		  		
		  		  \FOR{$t=1$ to $T$}
			  		  \FOR{$q1=1$ to $|Q|$}
				  		  \STATE $T_b[q1] = 0$;
				  		  \FOR{$q2=1$ to $|Q|$}
					  		  \STATE $T_b[q1] += \bigg( T_a[q2]\cdot a_{q2,q1}\cdot b_{q1}(O_t) \bigg)$					  		  
				  		  \ENDFOR
				  		  \STATE $T_b[q1] = p_{tmp}$
			  		  \ENDFOR
			  		  \STATE $swap(T_a, T_b)$;
			  		  \STATE $T_b[0 \ldots |Q|] = 0$;
		  		  \ENDFOR
		  		
		  		  \RETURN $\argmax{T_a}$;
			  \end{algorithmic}
			  \caption{Algorytmy Viterbiego znajdujący najbardziej prawdopodobną stan}
		  	  \label{alg:viterbi2}
		  \end{figure}	   
		   
		   \begin{figure}[H]
		   	\centering
		   	\begin{tikzpicture}[node distance=1.3cm]
		   	
		   	\begin{scope}
		   	\node [hmm] (hmm1) {$s_1$};
		   	\node[left] at (hmm1.west) {$P_{t-1}^{s1}$};
		   	
		   	\node [hmm, below of=hmm1] (hmm2) {$s_2$};
		   	\node[left] at (hmm2.west) {$P_{t-1}^{s2}$};
		   	
		   	\node [hmm, below of=hmm2] (hmm3) {$s_3$};
		   	\node[left] at (hmm3.west) {$P_{t-1}^{s3}$};
		   	
		   	\node [hmm, below of=hmm3] (hmm4) {$s_4$};
		   	\node[left] at (hmm4.west) {$P_{t-1}^{s4}$};
		   	
		   	\node [hmm, below of=hmm4] (hmm5) {$s_5$};
		   	\node[left] at (hmm5.west) {$P_{t-1}^{s5}$};
		   	
		   	\node [hmm, right = 3.5 cm of hmm3] (hmm6) {$s_3$};
		   	\node[right] at (hmm6.east) {$P_{t}^{s3}$};
		   	
		   	\draw [->] (hmm1) [out=0, in=140]  to node[midway,right,rotate=0] {$a_{s1,s3}$} (hmm6);
		   	\draw [->] (hmm2) [out=0, in=160]  to node[midway,above,rotate=0] {$a_{s2,s3}$} (hmm6);
		   	\draw [->] (hmm3) [out=0, in=180]  to node[midway,above,rotate=0] {$a_{s3,s3}$} (hmm6);
		   	\draw [->] (hmm4) [out=0, in=200]  to node[midway,above,rotate=0] {$a_{s4,s3}$} (hmm6);
		   	\draw [->] (hmm5) [out=0, in=220]  to node[midway,right,rotate=0] {$a_{s5,s3}$} (hmm6);
		   	
		   	\draw[-stealth] (-2cm,-6cm)--(6cm,-6cm) node[right]{T}; 
		   	\draw[-stealth] (-2cm,-6cm)--(-2cm,0.5cm) node[right]{Q}; 
		   	\draw[dashed] (1cm,-6cm)--(1cm,0.5cm);
		   	
		   	\end{scope}
		   	
		   	\end{tikzpicture}
		   	\caption{Wizualizacja jednej iteracji algorytmu Viterbiego}
		   	\label{fig:viterbi}
		   	
		   \end{figure}
		   
	   \subsection{Algorytm Bauma-Welcha}
	   \label{sec:Baum_Welch}
	   Algorytm \textit{Bauma-Welcha} jest techniką klasy \textit{Expectation-Maximization} służącą estymacji prawdopodobieństw przejścia pomiędzy stanami $A$ oraz funkcji emisji $b$. Został on zaproponowany przez Leonarda E. Bauma, szczegółowy opis teorii dotyczącej algorytmu znajduje się w artykule \cite{BaumWelch_tutorial}. Koncepcyjnie, podczas treningu wykorzystującego \textit{algorytm Bauma-Welcha}, w $i-tej$ iteracji, rozpoznaje się zbiór treningowy z wykorzystaniem modelu $M_i$, otrzymując w efekcie dopasowanie stanów do obserwacji. Następnie buduje się nowy model $M_{i+1}$, maksymalizujący wcześniej dopasowane obserwacje. Wraz z kolejnymi iteracjami parametry modelu powinny zbiegać do sub-optymalnych wartości.
	   
	   Chcą wyznaczyć prawdopodobieństwo przejścia $q_i \rightarrow q_j$ należy policzyć:
	   \begin{equation}
		    a_{i,j}=\frac{\text{oczekiwana liczba przejść } q_i \rightarrow q_j}{\text{oczekiwana liczba przejść ze stanu } q_i} = \frac{\sum_{t=1}^T 	\xi_t(i,j)}{\sum_{t=1}^T \sum_{k=1}^{|Q|}	\xi_t(i,k)}
		    \label{eqn:qi_qj}
	   \end{equation}
	   Aby to osiągnąć zdefiniujemy następujące pomocnicze prawdopodobieństwa:
	   \begin{equation}
			\xi_t(i,j)=P(q_{i,t}, q_{j,t+1}|O,M)
			\label{eqn:ksi}
	   \end{equation}
	   \begin{equation}
	   	    \beta_t(i)=P(o_{t+1}, \ldots, o_T|q_{i,t}|M)
	   	    \label{eqn:beta}
	   \end{equation}
	   \begin{equation}
			\alpha_t(i)=P(o_1, \ldots, o_{t},q_{i,t}|M)
			\label{eqn:alfa}
	   \end{equation}
	   
	   $\alpha_t(i)$ jest dokładnie tym, co wyznacza opisany w rozdziale \ref{sec:Viterbi} \textit{algorytm Viterbiego}, natomiast $\beta_t(i)$ dale się wyznaczyć za pomocą analogicznej procedury. Warto zauważyć, że:
	   \begin{equation}
		   P(O|M) = \sum_{i=1}^{|Q|}\alpha_t(i)\beta_t(i)
	   \end{equation}
	   Dodatkowo, zdefiniujemy $\hat{\xi}$ zgodnie ze wzorem \ref{eqn:quite_ksi}, które jest prawie tym samym co $\xi$. Korzystając ze wzory Bayesa w postaci \ref{eqn:bayes2}, przejdziemy zgodnie z równaniem \ref{eqn:quite_ksi_2_ksi} do interesującego nas $\xi$.
	   \begin{equation}
	       \hat{\xi} = P(q_{i,t}, q_{j,t+1},O|M)
	       \label{eqn:quite_ksi}
	   \end{equation}
	   \begin{equation}
	        P(X|Y,W) = \frac{P(X,Y|W)}{P(Y|W)}
	        \label{eqn:bayes2}
	   \end{equation}
	   \begin{equation}
			\xi_t(i,j)=P(q_{i,t}, q_{j,t+1}|O,M) = \frac{P(q_{i,t}, q_{j,t+1},O|M)}{P(O|M)}= \frac{\hat{\xi_t}(i,j)}{P(O|M)}
			\label{eqn:quite_ksi_2_ksi}
		\end{equation}
		
		Następnie wykorzystując \ref{eqn:alfa} oraz \ref{eqn:beta} zapiszemy $\hat{\xi_t}(i,j)$ w następujący sposób:
		\begin{equation}
			\hat{\xi_t}(i,j) = \alpha_t(i) a_{i,j} b_j(o_{t+1}) \beta_{t+1}(j)
		\end{equation}
		
		Powyższe równości pozwalają nam skutecznie wyznaczać prawdopodobieństwo przejścia $q_i \rightarrow q_j$ ze wzory \ref{eqn:qi_qj}. Kolejną rzeczą potrzebną w \textit{algorytmie Bauma-Welcha} jest prawdopodobieństwo bycia w stanie $j$ w momencie $t$, opisane wzorem \ref{eqn:gamma}. Jest ono potrzebne do wyznaczenia funkcji emisji obserwacji. Na potrzeby tego rozdziały posłużymy się uproszczeniem, w którym występuje jedynie ograniczona liczba różnych obserwacji. Pozwala nam to na wyznaczenie funkcji $b$ ze pomocą wzorem \ref{eqn:b}. Bardziej uogólnione podejście, w którym funkcja $b$ nie ma ograniczeń, przedstawione jest w kolejnym rozdziale.
		\begin{equation}
			\gamma_t(j) = P(q_{t,j}|O,M) = \frac{P(q_{t,j},O|M)}{P(O|M)} = \frac{\aleph_t(j)\beta_t(j)}{P(O|M)}
			\label{eqn:gamma}
		\end{equation}
		\begin{equation}
			b_j(o)=\frac{\text{oczekiwana liczba obserwacji $o$ w stanie $j$}}{\text{oczekiwana liczba odwiedzeń stanu $j$}} = \frac{\sum_{t=1,o_t=o}^T\gamma_t(j)}{\sum_{t=1}^T\gamma_t(j)}
			\label{eqn:b}
		\end{equation}
		
		Składając razem wszystkie otrzymane równości dostajemy algorytm \ref{alg:Baum_Welch}. W uzyskanym algorytmie linie \ref{line:E_begin}, \ref{line:E_end} odpowiadają fazie estymacji, natomiast \ref{line:M_begin}, \ref{line:M_end} odpowiadają fazie maksymalizacji. $K$ określa liczbę epok treningu.
		
		\begin{figure}
			\begin{algorithmic}[1]				
				\STATE $\langle A, B\rangle \leftarrow \text{zainicjalizuj}$
				
				\FOR{$k = (1\ldots K)$}
					\STATE $\forall_{t,j} \gamma_t(j) = \frac{\alpha_t(j)\beta_t(j)}{P(O|M)} \label{line:E_begin}$
					\STATE $\forall_{t,i,j} \xi_t(i,j) = \frac{\alpha_t(i)a_{i,j}b_j(o_{t+1})\beta_{t+1}(j)}{P(O|M)} \label{line:E_end}$
					\STATE
					\STATE $\forall_{i,j} \xi_t(i,j) a_{i,j} = \frac{\sum_{t=1}^T 	\xi_t(i,j)}{\sum_{t=1}^T \sum_{k=1}^{|Q|}	\xi_t(i,k)} \label{line:M_begin}$
					\STATE $\forall_{j,o} 	b_j(o) = \frac{\sum_{t=1,o_t=o}^T\gamma_t(j)}{\sum_{t=1}^T\gamma_t(j)} \label{line:M_end}$
				\ENDFOR							
				
				\RETURN $\langle A, B\rangle$;
			\end{algorithmic}
	
			\caption{Algorytmy Bauma-Welcha}
			\label{alg:Baum_Welch}
		\end{figure}	
		
	   \subsection{Estymacja parametrów rozkładu normalnego}
	   	   
	   \begin{figure}[H]
		   \centering
		   \begin{tikzpicture}
			   \begin{axis}[
			   axis lines = left
			   ]
			   %Below the red parabola is defined
			   \addplot [
			   domain=0:20, 
			   samples=100, 
			   color=red,
			   ]
			   {0.3 * 1/(2.5 * 2) * e ^ (-(x - 5)^2 / (2 * 2 ^ 2))};
			    
			   \addplot [
			   domain=0:20, 
			   samples=100, 
			   color=blue,
			   ]
			   {0.3 * 1/(2.5 * 4) * e ^ (-(x - 17)^2 / (2 * 4 ^ 2))};
			   
			   \addplot [
			   domain=0:20, 
			   samples=100, 
			   color=yellow,
			   ]
			   {0.4 * 1/(2.5 * 1.5) * e ^ (-(x - 10)^2 / (2 * 1.5 ^ 2))};
			   
			   \addplot [
			   domain=0:20, 
			   samples=100, 
			   color=black,
			   ]
			   {0.3 * 1/(2.5 * 2) * e ^ (-(x - 5)^2 / (2 * 2 ^ 2)) + 0.3 * 1/(2.5 * 4) * e ^ (-(x - 17)^2 / (2 * 4 ^ 2)) + 0.4 * 1/(2.5 * 1.5) * e ^ (-(x - 10)^2 / (2 * 1.5 ^ 2))};
			   
			   \end{axis}
		   \end{tikzpicture}
		   \label{fig:gmm}
		   \caption{Jednowymiarowa mikstura Gaussowska składająca się z trzech komponentów.}
	\end{figure}
	   
	   \label{sec:gmm_estimation}
	   W rozdziale \ref{sec:Baum_Welch} posłużono się uproszczeniem zakładającym, że zbiór obserwacji jest skończony i dyskretny. W rzeczywistości przy rozpoznawaniu mowy do określania $b_i(o)$ wykorzystuje się \textit{wielowymiarowe mikstury Gaussowkie} będące sumą kilku rozkładów normalnych. Mikstura opisane jest wzorem \ref{eqn:GMM}, w którym $D$ oznacza liczbę wymiarów wektora cech, $\mu$ jest wektorem średnich, natomiast $\Sigma$ macierzą kowariancji. Rysunek \ref{fig:gmm} pokazuje przykład jednowymiarowej mikstury Gaussowskiej składającej się z trzech komponentów. Korzystając z nisko skorelowanych cech \textit{MFCC} opisanych w rozdziale \ref{sec:Feature_vec_mfcc}, można założyć, że macierz kowariancji $\Sigma$ jest diagonalna. Uproszczenie to znacząco przyśpiesza obliczenia, które nie wymagają kosztownego odwracania macierzy oraz liczenie jej wyznacznika. Ponadto wzór na rozkład normalny upraszcza się do postaci \ref{eqn:normal_distribution_simple}
	   
	   \begin{equation}
		   	b_j(o) = \sum_{m=1}^N c_j N_{j,m}(o)
		   	\label{eqn:GMM}
	   \end{equation}
	   \begin{equation}
		   N_{j,m}(o)=\frac{1}{(2\pi)^{\frac{D}{2}}||\Sigma_{j,m}|^{\frac{1}{2}}}\exp\bigg( -\frac{1}{2}(o-\mu_{j,m})^T\Sigma_{j,m}^{-1}(o-\mu_{j,m}) \bigg)
		   \label{eqn:normal_distribution}
	   \end{equation}	
	   \begin{equation}
		   N_{j,m}(o)\simeq\frac{1}{(2\pi)^{\frac{D}{2}}(\prod_{i=1}^D\sigma_{j,m,i})^{\frac{1}{2}}}\exp\bigg( -\frac{1}{2}\sum_{i=1}^D(o_i-\mu_{j,m,i})^2\sigma_{j,m,i} \bigg)
	   \label{eqn:normal_distribution_simple}
	   \end{equation}
	   
	   W celu wytrenowania parametrów mikstur Guassowkich należy zmodyfikować wzór \ref{eqn:b}. Teraz zamiast liczyć bezpośrednio prawdopodobieństwo obserwacji pod warunkiem stanu, wyznaczać będziemy średnią, macierze kowariancji oraz wagi komponentów tworzących miksturę Gaussowską. Do wyznaczania tych wartości posłużymy się wzorami \ref{eqn:b_u_c}, \ref{eqn:b_s_c} oraz \ref{eqn:b_c_c}. Występujący w nich symbol $\delta_{t,m}(j)$ oznacza prawdopodobieństwo bycia w stanie $j$, w momencie $t$ oraz emisję z komponentu Gaussowskiego $m$. W prostym przypadku, gdy mikstura składa się tylko z jednego komponentu, wzór na $\delta$ upraszcza się do postaci $\delta_{j,m} \equiv \gamma_j$, gdzie $\gamma_j$ jest zdefiniowana wzorem \ref{eqn:gamma}. W przypadku wielu komponentów do wyznaczania $\delta$ skorzystamy z równanie \ref{eqn:delta}, warto zauważyć, że jest to wzór bardzo podobny do końcowej definicji $\xi _t(i,j)$ wyprowadzonej w poprzednim rozdziale i zapisanej w \ref{line:E_end} linii algorytmu \ref{alg:Baum_Welch}. W rzeczywistości wylicza on szansę przejścia do stanu $q_j$ wraz z emisja obserwacji przez $m-ty$ komponent, pod warunkiem całej obserwacji $O$ i aktualnie wyestymowanego modelu.
	   
	   \begin{equation}
	   \mu_{j,m}=\frac{\sum_{t=1}^T\delta_{t,m}(j)o_t}{\sum_{t=1}^T\sum_{m=1}^M\delta_{t,m}(j)}
	   \label{eqn:b_u_c}
	   \end{equation}
	   
	   \begin{equation}
	   \Sigma_{j,m}=\frac{\sum_{t=1}^T\delta_{t,m}(j)(o_t-\mu_{j,m})(o_t-\mu_{j,m})^T}{\sum_{t=1}^T\sum_{m=1}^M\delta_{t,m}(j)}
	   \label{eqn:b_s_c}
	   \end{equation}
	   
	   \begin{equation}
	   c_{j,m}=\frac{\sum_{t=1}^T\delta_{t,m}(j)}{\sum_{t=1}^T\sum_{m=1}^M\delta_{t,m}(j)}
	   \label{eqn:b_c_c}
	   \end{equation}
	   
	   \begin{equation}
	   \delta_t(j)= \frac{\sum_{i=1}^{|Q|}\alpha_{t-1}(j)a_{i,j}c_{j,m}N_{j,m}(o_{t})\beta_{t}(j)}{P(O|M)}
	   \label{eqn:delta}
	   \end{equation}
	   
    \subsection{Modelowanie fonetyki z wykorzystaniem HMM }
    \label{sec:hmm_in_asr}
    W klasycznych systemach rozpoznających mowę, wykorzystujących ukryte modele Markowa, zbiór stanów reprezentuje fonemy. Każdy fonem ma trzy stany w HMM, tak jak opisano w rozdziale \ref{sec:Phones}. Zbiorem obserwacji są wektory cech wyznaczane zgodnie z opisem w rozdziale \ref{sec:Feature_vec_mfcc}. Przejścia pomiędzy stanami jednego fonemu są zgodne z opisem z rozdziału \ref{sec:Phones}, natomiast przejścia pomiędzy różnymi fonemami są tworzone na podstawie słownika (blok \refBlock{slownik} na rysunku \ref{fig:ARM_schemat}). Dla każdego słowa generowany jest ciąg stanów odpowiadający kolejnym fonemom, z których składa się słowo. \\
    Na rysunku \ref{fig:AutomatExample} zamieszczono przykład automatu dla słowa \textit{jabłko}. Zgodnie z wykorzystanymi regułami leksykalnymi słowo \textit{jabłko} zapisuje się za pomocą fonemów jako $[j a p ł k o]$. \\
    
    \begin{figure}[H]
    	\centering
		\begin{tabular}{|c|}
			\hline
			\textit{jabłko} = [j a p ł k o] \\ 
			\hline \\
	
			\begin{tikzpicture}[node distance=1.7cm]
			
				\begin{scope}
				
				\def\x{0.65}
				\def\y{1.0}
				\def\z{2.5}
				
				\node [hmm0] (hmm1) {};
				\node [below] at (hmm1.south) {$j_1$};
				
				\node [hmm0, right = \x cm of hmm1] (hmm2) {};
				\node [below] at (hmm2.south) {$j_2$};
				
				\node [hmm0, right = \x cm of hmm2] (hmm3) {};
				\node [below] at (hmm3.south) {$j_3$};
				
				
				\node [hmm0, right = \y cm of hmm3] (hmm4) {};
				\node [below] at (hmm4.south) {$a_1$};
				
				\node [hmm0, right = \x cm of hmm4] (hmm5) {};
				\node [below] at (hmm5.south) {$a_2$};
				
				\node [hmm0, right = \x cm of hmm5] (hmm6) {};
				\node [below] at (hmm6.south) {$a_3$};
				
				
				\node [hmm0, right = \y cm of hmm6] (hmm7) {};
				\node [below] at (hmm7.south) {$p_1$};
				
				\node [hmm0, right = \x cm of hmm7] (hmm8) {};
				\node [below] at (hmm8.south) {$p_2$};
				
				\node [hmm0, right = \x cm of hmm8] (hmm9) {};
				\node [below] at (hmm9.south) {$p_3$};
				
				
				
				
				\node [hmm0, below = \z cm of hmm9] (hmm10) {};
				\node [below] at (hmm10.south) {$ł_1$};
				
				\node [hmm0, left = \x cm of hmm10] (hmm11) {};
				\node [below] at (hmm11.south) {$ł_2$};
				
				\node [hmm0, left = \x cm of hmm11] (hmm12) {};
				\node [below] at (hmm12.south) {$ł_3$};
				
				
				\node [hmm0, left = \y cm of hmm12] (hmm13) {};
				\node [below] at (hmm13.south) {$k_1$};
				
				\node [hmm0, left = \x cm of hmm13] (hmm14) {};
				\node [below] at (hmm14.south) {$k_2$};
				
				\node [hmm0, left = \x cm of hmm14] (hmm15) {};
				\node [below] at (hmm15.south) {$k_3$};
				
				
				\node [hmm0, left = \y cm of hmm15] (hmm16) {};
				\node [below] at (hmm16.south) {$o_1$};
				
				\node [hmm0, left = \x cm of hmm16] (hmm17) {};
				\node [below] at (hmm17.south) {$o_2$};
				
				\node [hmm0, left = \x cm of hmm17] (hmm18) {};
				\node [below] at (hmm18.south) {$o_3$};
				
				
				
				\draw[thick,->,shorten >=1pt] (hmm1) to [out=0,in=180] (hmm2);
				\draw[thick,->,shorten >=1pt] (hmm2) to [out=0,in=180] (hmm3);
				
				\draw[thick,->,shorten >=1pt] (hmm3) to [out=0,in=180] (hmm4);
				
				\draw[thick,->,shorten >=1pt] (hmm4) to [out=0,in=180] (hmm5);
				\draw[thick,->,shorten >=1pt] (hmm5) to [out=0,in=180] (hmm6);
				
				\draw[thick,->,shorten >=1pt] (hmm6) to [out=0,in=180] (hmm7);
				
				\draw[thick,->,shorten >=1pt] (hmm7) to [out=0,in=180] (hmm8);
				\draw[thick,->,shorten >=1pt] (hmm8) to [out=0,in=180] (hmm9);
				
				\draw[thick,->,shorten >=1pt] (hmm9) to [out=0,in=0,looseness=1.1] (hmm10);
				
				\draw[thick,->,shorten >=1pt] (hmm10) to [out=180,in=0] (hmm11);
				\draw[thick,->,shorten >=1pt] (hmm11) to [out=180,in=0] (hmm12);
				
				\draw[thick,->,shorten >=1pt] (hmm12) to [out=180,in=0] (hmm13);
				
				\draw[thick,->,shorten >=1pt] (hmm13) to [out=180,in=0] (hmm14);
				\draw[thick,->,shorten >=1pt] (hmm14) to [out=180,in=0] (hmm15);
				
				\draw[thick,->,shorten >=1pt] (hmm15) to [out=180,in=0] (hmm16);
				
				\draw[thick,->,shorten >=1pt] (hmm16) to [out=180,in=0] (hmm17);
				\draw[thick,->,shorten >=1pt] (hmm17) to [out=180,in=0] (hmm18);
				
		
				\draw[thick,->] (hmm1.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm2.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm3.70) arc (-60:245:4mm);
				
				\draw[thick,->] (hmm4.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm5.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm6.70) arc (-60:245:4mm);
				
				\draw[thick,->] (hmm7.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm8.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm9.70) arc (-60:245:4mm);
				
				
				
				\draw[thick,->] (hmm10.110) arc (240:-65:4mm);
				\draw[thick,->] (hmm11.110) arc (240:-65:4mm);
				\draw[thick,->] (hmm12.110) arc (240:-65:4mm);
				
				\draw[thick,->] (hmm13.110) arc (240:-65:4mm);
				\draw[thick,->] (hmm14.110) arc (240:-65:4mm);
				\draw[thick,->] (hmm15.110) arc (240:-65:4mm);
				
				\draw[thick,->] (hmm16.110) arc (240:-65:4mm);
				\draw[thick,->] (hmm17.110) arc (240:-65:4mm);
				\draw[thick,->] (hmm18.110) arc (240:-65:4mm);
				
				\draw[thick,<-,shorten <=1pt] (hmm1) -- +(180:1cm);
				\draw[thick,->,shorten <=1pt] (hmm18) -- +(180:1cm);
				\end{scope}			
			\end{tikzpicture} \\
			
			\hline
		\end{tabular}
    	\caption{Automat dla słowa \textit{jabłko}}
    	\label{fig:AutomatExample}
    	
    \end{figure}

    Przy generowaniu automatu dla wielu słów, w najprostszej implementacji, dla każdego słowa ze słownika, można stworzyć osobny ciąg stanów. Rysunek \ref{fig:Graph_simple} pokazuje przykład prostego grafu przejść dla słów: \textit{kot, kos, kasa, masa, ma}. Widać na nim dwa dodatkowe stany \textit{start} oraz \textit{stop}, z którymi nie wiąże się emisja żadnej obserwacji. Funkcją tych stanów jest połączenie wielu łańcuchów odpowiadających słowom w jeden graf, są  one równoważne stanom \textit{$q_0$} oraz \textit{$q_F$} z definicji \ref{equation:HMM_def}. Dodane jest również jeszcze jedno techniczne przejście, ze stanu \textit{stop} do stanu \textit{start}, jego zadaniem jest umożliwienie przejścia na początek automatu w celu rozpoznawania kolejnego słowa. \\
    Opisany powyżej automat dobrze oddaje koncepcję ukrytego modelu Markowa, który jest wykorzystywany przy rozpoznawaniu mowy, jednak jest bardzo nieefektywny. Nieefektywność wynika z wielokrotnego powtarzania tych samych obliczeń, dla słów o wspólnym prefiksie. Naturalną optymalizacją jest więc połączenie wspólnych prefiksów. Rysunek \ref{fig:Graph_complex} ilustruje zoptymalizowany automat \ref{fig:Graph_simple}. Widać, że dzięki optymalizacji otrzymujemy znacznie mniejszy graf o bardziej zwięzłej strukturze. 
    
	\begin{figure}
	   	\centering
	   	\vspace{-2.5cm}
	   	\begin{tabular}{|c|}
	   		\hline
	   		\textit{kot} = [k o t],  \textit{kos} = [k o s],  \textit{kasa} = [k a s a], \textit{masa} = [m a s a], \textit{ma} = [m a]\\ 
	   		\hline \\
	   		
	   		\begin{tikzpicture}[node distance=1.13cm]
	   		
	   		\begin{scope} 		
	   		\def \h{1.5}
	   		\def \t{3.2}
	   		\def \d{0.7}
	   		
  			\node [hmm] (hmm1) {$k_1$};
  			\node [hmm, above of=hmm1] (hmm2) {$k_2$};
  			\node [hmm, above of=hmm2] (hmm3) {$k_2$};
  			
  			\node [hmm, above = \d cm of hmm3] (hmm4) {$o_1$};
  			\node [hmm, above of=hmm4] (hmm5) {$o_2$};
  			\node [hmm, above of=hmm5] (hmm6) {$o_3$};
  			
  			\node [hmm, above = \d cm of hmm6] (hmm7) {$t_1$};
  			\node [hmm, above of=hmm7] (hmm8) {$t_2$};
  			\node [hmm, above of=hmm8] (hmm9) {$t_3$};
  			
  			
  			
  			\node [hmm, left = \h cm of hmm1] (hmm10) {$k_1$};
  			\node [hmm, above of=hmm10] (hmm11) {$k_2$};
  			\node [hmm, above of=hmm11] (hmm12) {$k_3$};
  			
  			\node [hmm, above = \d cm of hmm12] (hmm13) {$o_1$};
  			\node [hmm, above of=hmm13] (hmm14) {$o_2$};
  			\node [hmm, above of=hmm14] (hmm15) {$o_3$};
  			
  			\node [hmm, above = \d cm of hmm15] (hmm16) {$s_1$};
  			\node [hmm, above of=hmm16] (hmm17) {$s_2$};
  			\node [hmm, above of=hmm17] (hmm18) {$s_3$};
  			
  			
  			
  			\node [hmm, right = \h cm of hmm1] (hmm19) {$k_1$};
  			\node [hmm, above of=hmm19] (hmm20) {$k_2$};
  			\node [hmm, above of=hmm20] (hmm21) {$k_3$};
  			
  			\node [hmm, above = \d cm of hmm21] (hmm22) {$a_1$};
  			\node [hmm, above of=hmm22] (hmm23) {$a_2$};
  			\node [hmm, above of=hmm23] (hmm24) {$a_3$};
  			
  			\node [hmm, above = \d cm of hmm24] (hmm25) {$s_1$};
  			\node [hmm, above of=hmm25] (hmm26) {$s_2$};
  			\node [hmm, above of=hmm26] (hmm27) {$s_3$};
  			
  			\node [hmm, above = \d cm of hmm27] (hmm28) {$a_1$};
  			\node [hmm, above of=hmm28] (hmm29) {$a_2$};
  			\node [hmm, above of=hmm29] (hmm30) {$a_3$};
  			
  			
  			\node [hmm, right = \h cm of hmm19] (hmm31) {$m_1$};
  			\node [hmm, above of=hmm31] (hmm32) {$m_2$};
  			\node [hmm, above of=hmm32] (hmm33) {$m_3$};
  			
  			\node [hmm, above = \d cm of hmm33] (hmm34) {$a_1$};
  			\node [hmm, above of=hmm34] (hmm35) {$a_2$};
  			\node [hmm, above of=hmm35] (hmm36) {$a_3$};
  			
  			\node [hmm, above = \d cm of hmm36] (hmm37) {$s_1$};
  			\node [hmm, above of=hmm37] (hmm38) {$s_2$};
  			\node [hmm, above of=hmm38] (hmm39) {$s_3$};
  			
  			\node [hmm, above = \d cm of hmm39] (hmm40) {$a_1$};
  			\node [hmm, above of=hmm40] (hmm41) {$a_2$};
  			\node [hmm, above of=hmm41] (hmm42) {$a_3$};
  			
  			
  			\node [hmm, right = \h cm of hmm31] (hmm43) {$m_1$};
  			\node [hmm, above of=hmm43] (hmm44) {$m_2$};
  			\node [hmm, above of=hmm44] (hmm45) {$m_3$};
  			
  			\node [hmm, above = \d cm of hmm45] (hmm46) {$a_1$};
  			\node [hmm, above of=hmm46] (hmm47) {$a_2$};
  			\node [hmm, above of=hmm47] (hmm48) {$a_3$};
  			
  			
  			\node [hmm, below = 1.3 cm of hmm19] (start) {$start$};
  			\node [hmm, above = 1.3 cm of hmm30] (stop) {$stop$};
  			
  			\coordinate [right = \t cm of hmm42] (P1);
  			\coordinate [right of = hmm43] (P2);
  			\coordinate [right = 2.0 cm of hmm42] (P3);
  			\coordinate [left = 2.0 cm of hmm30] (P4);
  			\coordinate [left = 2.4 cm of P4] (P5);
  			%\coordinate [below of=hmm25] (P4);
  			
  			\draw[thick,->,shorten >=1pt] (hmm1) to [out=90,in=-90] (hmm2);
  			\draw[thick,->,shorten >=1pt] (hmm2) to [out=90,in=-90] (hmm3);
  			\draw[thick,->,shorten >=1pt] (hmm3) to [out=90,in=-90] (hmm4);
  			\draw[thick,->,shorten >=1pt] (hmm4) to [out=90,in=-90] (hmm5);
  			\draw[thick,->,shorten >=1pt] (hmm5) to [out=90,in=-90] (hmm6);
  			\draw[thick,->,shorten >=1pt] (hmm6) to [out=90,in=-90] (hmm7);
  			\draw[thick,->,shorten >=1pt] (hmm7) to [out=90,in=-90] (hmm8);
  			\draw[thick,->,shorten >=1pt] (hmm8) to [out=90,in=-90] (hmm9);
  			
  			\draw[thick,->,shorten >=1pt] (hmm10) to [out=90,in=-90] (hmm11);
  			\draw[thick,->,shorten >=1pt] (hmm11) to [out=90,in=-90] (hmm12);
  			\draw[thick,->,shorten >=1pt] (hmm12) to [out=90,in=-90] (hmm13);
  			\draw[thick,->,shorten >=1pt] (hmm13) to [out=90,in=-90] (hmm14);
  			\draw[thick,->,shorten >=1pt] (hmm14) to [out=90,in=-90] (hmm15);
  			\draw[thick,->,shorten >=1pt] (hmm15) to [out=90,in=-90] (hmm16);
  			\draw[thick,->,shorten >=1pt] (hmm16) to [out=90,in=-90] (hmm17);
  			\draw[thick,->,shorten >=1pt] (hmm17) to [out=0,in=-90] (hmm18);
  			
  			\draw[thick,->,shorten >=1pt] (hmm19) to [out=90,in=-90] (hmm20);
  			\draw[thick,->,shorten >=1pt] (hmm20) to [out=90,in=-90] (hmm21);
			\draw[thick,->,shorten >=1pt] (hmm21) to [out=90,in=-90] (hmm22);
			\draw[thick,->,shorten >=1pt] (hmm22) to [out=90,in=-90] (hmm23);
			\draw[thick,->,shorten >=1pt] (hmm23) to [out=90,in=-90] (hmm24);
			\draw[thick,->,shorten >=1pt] (hmm24) to [out=90,in=-90] (hmm25);
			\draw[thick,->,shorten >=1pt] (hmm25) to [out=90,in=-90] (hmm26);
			\draw[thick,->,shorten >=1pt] (hmm26) to [out=90,in=-90] (hmm27);
			\draw[thick,->,shorten >=1pt] (hmm27) to [out=90,in=-90] (hmm28);
			\draw[thick,->,shorten >=1pt] (hmm28) to [out=90,in=-90] (hmm29);
			\draw[thick,->,shorten >=1pt] (hmm29) to [out=90,in=-90] (hmm30);
	
			\draw[thick,->,shorten >=1pt] (hmm31) to [out=90,in=-90] (hmm32);
			\draw[thick,->,shorten >=1pt] (hmm32) to [out=90,in=-90] (hmm33);
			\draw[thick,->,shorten >=1pt] (hmm33) to [out=90,in=-90] (hmm34);
			\draw[thick,->,shorten >=1pt] (hmm34) to [out=90,in=-90] (hmm35);
			\draw[thick,->,shorten >=1pt] (hmm35) to [out=90,in=-90] (hmm36);
			\draw[thick,->,shorten >=1pt] (hmm36) to [out=90,in=-90] (hmm37);
			\draw[thick,->,shorten >=1pt] (hmm37) to [out=90,in=-90] (hmm38);
			\draw[thick,->,shorten >=1pt] (hmm38) to [out=90,in=-90] (hmm39);
			\draw[thick,->,shorten >=1pt] (hmm39) to [out=90,in=-90] (hmm40);
			\draw[thick,->,shorten >=1pt] (hmm40) to [out=90,in=-90] (hmm41);
			\draw[thick,->,shorten >=1pt] (hmm41) to [out=90,in=-90] (hmm42);
			
			\draw[thick,->,shorten >=1pt] (hmm43) to [out=90,in=-90] (hmm44);
			\draw[thick,->,shorten >=1pt] (hmm44) to [out=90,in=-90] (hmm45);
			\draw[thick,->,shorten >=1pt] (hmm45) to [out=90,in=-90] (hmm46);
			\draw[thick,->,shorten >=1pt] (hmm46) to [out=90,in=-90] (hmm47);				
			\draw[thick,->,shorten >=1pt] (hmm47) to [out=90,in=-90] (hmm48);
	  				  			
  			\draw[thick,->] (hmm1.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm2.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm3.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm4.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm5.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm6.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm7.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm8.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm9.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm10.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm11.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm12.160) arc (30:335:4mm);
		    \draw[thick,->] (hmm13.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm14.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm15.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm16.160) arc (30:335:4mm);
	        \draw[thick,->] (hmm17.160) arc (30:335:4mm);
		    \draw[thick,->] (hmm19.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm20.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm21.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm22.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm23.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm24.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm25.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm26.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm27.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm28.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm29.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm30.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm31.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm32.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm33.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm34.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm35.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm36.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm37.160) arc (30:335:4mm);
		    \draw[thick,->] (hmm38.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm39.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm40.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm41.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm42.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm43.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm44.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm45.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm46.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm47.160) arc (30:335:4mm);
  			\draw[thick,->] (hmm48.160) arc (30:335:4mm);
  			
  			\draw[thick,->,shorten >=1pt] (start) to [out=135,in=270] (hmm10);
  			\draw[thick,->,shorten >=1pt] (start) to [out=100,in=270] (hmm1);
  			\draw[thick,->,shorten >=1pt] (start) to [out=90,in=270] (hmm19);
			\draw[thick,->,shorten >=1pt] (start) to [out=70,in=270] (hmm31);
			\draw[thick,->,shorten >=1pt] (start) to [out=45,in=270] (hmm43);
  			
  			\draw[thick,->,shorten >=1pt] (hmm18) to [out=90,in=270] (P5);
  			\draw[thick,->,shorten >=1pt] (P5) to [out=90,in=225] (stop);
  			\draw[thick,->,shorten >=1pt] (hmm9) to [out=90,in=270] (P4);
  			\draw[thick,->,shorten >=1pt] (P4) to [out=90,in=250] (stop);
  			\draw[thick,->,shorten >=1pt] (hmm30) to [out=90,in=270] (stop);
  			\draw[thick,->,shorten >=1pt] (hmm42) to [out=90,in=300] (stop);
  			
  			\draw[thick,->,shorten >=1pt] (stop) to [out=45,in=90] (P1);
  			\draw[thick,->,shorten >=1pt] (P1) to [out=270,in=90] (P2);
  			\draw[thick,->,shorten >=1pt] (P2) to [out=270,in=315] (start);
  			
  			\draw[thick,->,shorten >=1pt] (hmm48) to [out=90,in=270] (P3);
  			\draw[thick,->,shorten >=1pt] (P3) to [out=90,in=-20] (stop);
  			%\draw[thick,->,shorten >=1pt] (P3) to [out=0,in=240] (stop);
	   		
	   		\end{scope}			
	   		\end{tikzpicture} \\
	   		
	   		\hline
	   	\end{tabular}
	   	\caption{Niezoptymalizowany automat dla słów: \textit{kot}, \textit{kos}, \textit{kasa}, \textit{masa} oraz \textit{ma}}
	   	\label{fig:Graph_simple}
	   	
	\end{figure}

	\begin{figure}
		\centering
		
		\begin{tabular}{|c|}
			\hline
			\textit{kot} = [k o t],  \textit{kos} = [k o s],  \textit{kasa} = [k a s a], \textit{masa} = [m a s a], \textit{ma} = [m a]\\ 
			\hline \\
			
			\begin{tikzpicture}[node distance=1.13cm]
			
			\begin{scope}  		
			
			\def \h{1.5}
			\def \d{0.7}
			
			\node [hmm] (hmm1) {$k_1$};
			\node [hmm, above of=hmm1] (hmm2) {$k_2$};
			\node [hmm, above of=hmm2] (hmm3) {$k_3$};
			
			\node [hmm, above = \d cm of hmm3] (hmm13) {$a_1$};
			\node [hmm, above of=hmm13] (hmm14) {$a_2$};
			\node [hmm, above of=hmm14] (hmm15) {$a_3$};
			
			\node [hmm, above = \d cm of hmm15] (hmm16) {$s_1$};
			\node [hmm, above of=hmm16] (hmm17) {$s_2$};
			\node [hmm, above of=hmm17] (hmm18) {$s_3$};
			
			\node [hmm, above = \d cm of hmm18] (hmm19) {$a_1$};
			\node [hmm, above of=hmm19] (hmm20) {$a_2$};
			\node [hmm, above of=hmm20] (hmm21) {$a_3$};
			
			
			\node [hmm, left = \h cm of hmm13] (hmm4) {$o_1$};
			\node [hmm, above of=hmm4] (hmm5) {$o_2$};
			\node [hmm, above of=hmm5] (hmm6) {$o_3$};
			
			\node [hmm, above = \d cm of hmm6] (hmm7) {$t_1$};
			\node [hmm, above of=hmm7] (hmm8) {$t_2$};
			\node [hmm, above of=hmm8] (hmm9) {$t_3$};
			
			\node [hmm, left = \h cm of hmm7] (hmm10) {$s_1$};
			\node [hmm, above of=hmm10] (hmm11) {$s_2$};
			\node [hmm, above of=hmm11] (hmm12) {$s_3$};
			
			
			\node [hmm, right = \h cm of hmm1] (hmm31) {$m_1$};
			\node [hmm, above of=hmm31] (hmm32) {$m_2$};
			\node [hmm, above of=hmm32] (hmm33) {$m_3$};
			
			\node [hmm, above = \d cm of hmm33] (hmm34) {$a_1$};
			\node [hmm, above of=hmm34] (hmm35) {$a_2$};
			\node [hmm, above of=hmm35] (hmm36) {$a_3$};
			
			\node [hmm, above = \d cm of hmm36] (hmm37) {$s_1$};
			\node [hmm, above of=hmm37] (hmm38) {$s_2$};
			\node [hmm, above of=hmm38] (hmm39) {$s_3$};
			
			\node [hmm, above = \d cm of hmm39] (hmm40) {$a_1$};
			\node [hmm, above of=hmm40] (hmm41) {$a_2$};
			\node [hmm, above of=hmm41] (hmm42) {$a_3$};
			
			\node [hmm, below = 1.3 cm of hmm1] (start) {$start$};
			\node [hmm, above = 1.3 cm of hmm21] (stop) {$stop$};
			
			\coordinate [right = \h cm of hmm42] (P1);
			\coordinate [right = \h cm of hmm31] (P2);
			\coordinate [left = 1.9 cm of hmm21] (P3);
			\coordinate [left = 2.3 cm of P3] (P4);
			\coordinate [right of = hmm37] (P5);
			\coordinate [right of = hmm42] (P6);
			
			\draw[thick,->,shorten >=1pt] (hmm1) to [out=90,in=270] (hmm2);
			\draw[thick,->,shorten >=1pt] (hmm2) to [out=90,in=270] (hmm3);
			\draw[thick,->,shorten >=1pt] (hmm3) to [out=105,in=285] (hmm4);
			\draw[thick,->,shorten >=1pt] (hmm4) to [out=90,in=270] (hmm5);
			\draw[thick,->,shorten >=1pt] (hmm5) to [out=90,in=270] (hmm6);
			\draw[thick,->,shorten >=1pt] (hmm6) to [out=90,in=270] (hmm7);
			\draw[thick,->,shorten >=1pt] (hmm7) to [out=90,in=270] (hmm8);
			\draw[thick,->,shorten >=1pt] (hmm8) to [out=90,in=270] (hmm9);
			
			\draw[thick,->,shorten >=1pt] (hmm6) to [out=105,in=285] (hmm10);
			\draw[thick,->,shorten >=1pt] (hmm10) to [out=90,in=270] (hmm11);
			\draw[thick,->,shorten >=1pt] (hmm11) to [out=90,in=270] (hmm12);
			
			\draw[thick,->,shorten >=1pt] (hmm3) to [out=90,in=270] (hmm13);
			\draw[thick,->,shorten >=1pt] (hmm13) to [out=90,in=270] (hmm14);
			\draw[thick,->,shorten >=1pt] (hmm14) to [out=90,in=270] (hmm15);
			\draw[thick,->,shorten >=1pt] (hmm15) to [out=90,in=270] (hmm16);
			\draw[thick,->,shorten >=1pt] (hmm16) to [out=90,in=270] (hmm17);
			\draw[thick,->,shorten >=1pt] (hmm17) to [out=90,in=270] (hmm18);
			\draw[thick,->,shorten >=1pt] (hmm18) to [out=90,in=270] (hmm19);
			\draw[thick,->,shorten >=1pt] (hmm19) to [out=90,in=270] (hmm20);
			\draw[thick,->,shorten >=1pt] (hmm20) to [out=90,in=270] (hmm21);
			
			\draw[thick,->,shorten >=1pt] (hmm31) to [out=90,in=270] (hmm32);
			\draw[thick,->,shorten >=1pt] (hmm32) to [out=90,in=270] (hmm33);
			\draw[thick,->,shorten >=1pt] (hmm33) to [out=90,in=270] (hmm34);
			\draw[thick,->,shorten >=1pt] (hmm34) to [out=90,in=270] (hmm35);
			\draw[thick,->,shorten >=1pt] (hmm35) to [out=90,in=270] (hmm36);
			\draw[thick,->,shorten >=1pt] (hmm36) to [out=90,in=270] (hmm37);
			\draw[thick,->,shorten >=1pt] (hmm37) to [out=90,in=270] (hmm38);
			\draw[thick,->,shorten >=1pt] (hmm38) to [out=90,in=270] (hmm39);
			\draw[thick,->,shorten >=1pt] (hmm39) to [out=90,in=270] (hmm40);
			\draw[thick,->,shorten >=1pt] (hmm40) to [out=90,in=270] (hmm41);
			\draw[thick,->,shorten >=1pt] (hmm41) to [out=90,in=270] (hmm42);
			
			\draw[thick,->] (hmm1.160) arc (30:335:4mm);
			\draw[thick,->] (hmm2.160) arc (30:335:4mm);
			\draw[thick,->] (hmm3.160) arc (30:335:4mm);
			\draw[thick,->] (hmm4.160) arc (30:335:4mm);
			\draw[thick,->] (hmm5.160) arc (30:335:4mm);
   \draw[thick,->] (hmm6.160) arc (30:335:4mm);
			\draw[thick,->] (hmm7.160) arc (30:335:4mm);
			\draw[thick,->] (hmm8.160) arc (30:335:4mm);
			\draw[thick,->] (hmm9.160) arc (30:335:4mm);
			\draw[thick,->] (hmm10.160) arc (30:335:4mm);
			\draw[thick,->] (hmm11.160) arc (30:335:4mm);
			\draw[thick,->] (hmm12.160) arc (30:335:4mm);
			\draw[thick,->] (hmm13.160) arc (30:335:4mm);
			\draw[thick,->] (hmm14.160) arc (30:335:4mm);
			\draw[thick,->] (hmm15.160) arc (30:335:4mm);
			\draw[thick,->] (hmm16.160) arc (30:335:4mm);
			\draw[thick,->] (hmm17.160) arc (30:335:4mm);
			\draw[thick,->] (hmm18.160) arc (30:335:4mm);
   \draw[thick,->] (hmm19.160) arc (30:335:4mm);
			\draw[thick,->] (hmm20.160) arc (30:335:4mm);
			\draw[thick,->] (hmm21.160) arc (30:335:4mm);
			\draw[thick,->] (hmm31.160) arc (30:335:4mm);
			\draw[thick,->] (hmm32.160) arc (30:335:4mm);
			\draw[thick,->] (hmm33.160) arc (30:335:4mm);
			\draw[thick,->] (hmm34.160) arc (30:335:4mm);
			\draw[thick,->] (hmm35.160) arc (30:335:4mm);
			\draw[thick,->] (hmm36.160) arc (30:335:4mm);
			\draw[thick,->] (hmm37.160) arc (30:335:4mm);
			\draw[thick,->] (hmm38.160) arc (30:335:4mm);
			\draw[thick,->] (hmm39.160) arc (30:335:4mm);
			\draw[thick,->] (hmm40.160) arc (30:335:4mm);
			\draw[thick,->] (hmm41.160) arc (30:335:4mm);
			\draw[thick,->] (hmm42.160) arc (30:335:4mm);
			
			
			\draw[thick,->,shorten >=1pt] (start) to [out=90,in=270] (hmm1);
			\draw[thick,->,shorten >=1pt] (start) to [out=70,in=250] (hmm31);
			
			\draw[thick,->,shorten >=1pt] (hmm9) to [out=90,in=270] (P3);
			\draw[thick,->,shorten >=1pt] (P3) to [out=90,in=250] (stop);
			\draw[thick,->,shorten >=1pt] (hmm12) to [out=90,in=270] (P4);
			\draw[thick,->,shorten >=1pt] (P4) to [out=90,in=225] (stop);
			\draw[thick,->,shorten >=1pt] (hmm21) to [out=90,in=270] (stop);
			
			\draw[thick,->,shorten >=1pt] (hmm36) to [out=70,in=270] (P5);
			\draw[thick,->,shorten >=1pt] (P5) to [out=90,in=270] (P6);
			\draw[thick,->,shorten >=1pt] (P6) to [out=90,in=320] (stop);
			
			\draw[thick,->,shorten >=1pt] (hmm42) to [out=90,in=300] (stop);
			
			\draw[thick,->,shorten >=1pt] (stop) to [out=45,in=90] (P1);
			\draw[thick,->,shorten >=1pt] (P1) to [out=270,in=90] (P2);
			\draw[thick,->,shorten >=1pt] (P2) to [out=270,in=315] (start);
			
			\end{scope}			
			\end{tikzpicture} \\
			
			\hline
		\end{tabular}
		\caption{Zoptymalizowany automat dla słów: \textit{kot}, \textit{kos}, \textit{kasa}, \textit{masa} oraz \textit{ma}}
		\label{fig:Graph_complex}
		
	\end{figure}
    
	   \subsection{ Rozpoznawanie}
		   \label{sec:recognition}
		   Mając zamodelowany słownik, za pomocą ukrytych modeli Markowa, zgodnie z opisem w rozdziale \ref{sec:hmm_in_asr}, można przejść do rozpoznawania mowy. Proces rozpoznawania jest w rzeczywistości wyszukiwaniem najbardziej prawdopodobnej ścieżki, zwanej \textit{hipotezą}. Ze ścieżka wiąże się sekwencja wypowiedzianych słów. Ten etap rozpoznawania mowy odpowiada blokowi \refBlock{lancuch_markova} z rysunku \ref{fig:ARM_schemat}. W rozdziale \ref{sec:Viterbi} opisano algorytm \ref{alg:viterbi}, który wylicza szukaną ścieżkę. Chcąc go wykorzystać do rozpoznawania, należy zmodyfikować algorytm o uwzględnianie prawdopodobieństwa słów z modelu językowego, które wyznaczamy za pomocą n-gramów, tak jak opisano w rozdziale \ref{sec:ngramm}. Ponadto należy dodać fragment kodu zapamiętującego słowa przez, które prowadzi ścieżka w celu ich późniejszego wypisania.\\
		   Model językowy uwzględnia się poprzez pomnożenie prawdopodobieństwa ścieżki przez prawdopodobieństwo rozpoznanego słowa, przy przechodzeniu do stany \textit{stop}.\\
		   W rzeczywistości wiele systemów rozpoznawania mowy, w szczególności \textit{Julius}, wykorzystują algorytm \textit{token passing} (\ref{alg:token_passing}), który jest wariancją algorytmu Viterbiego. Czas działania algorytmu \ref{fig:viterbi} to $O(T\cdot |Q|^2)$, co przy słowniku zawierającym kilkaset tysięcy słów przekłada się na bardzo wolny czas działania. W algorytmie \textit{token passing} uwzględnia się jedynie stany do których jest bezpośrednie przejście, czuli dla stanu $q_i$ rozpatrzony będzie jedynie zbiór $\{q_j |  A_{i,j}>0 \}$. Takie podejście znacząco przyśpiesza procedurę wyznaczania ścieżki. \\
		   Ponadto w algorytmie \ref{alg:token_passing} można zastosować heurystyczną metodę przyśpieszania, polegającą na przycinaniu zbiory $Q2$ do zadanej wielkości, zwanej \textit{szerokością wiązki}. Podczas przycinania pozostawia się jedynie $n$ par o największym prawdopodobieństwie, gdzie $n$ jest empirycznie wyznaczanym parametrem. W praktyce, znaczące przyśpieszenie przy jednoczesnym znikomym spadku skuteczności rozpoznawania otrzymuje się dla $n \in [500, 8000]$.
		    
		    \begin{figure}
		    	\begin{algorithmic}[1]
		    		
		    		\REQUIRE {tablica obserwacji $O$ o rozmiarze T}
		    		\REQUIRE {macierz przejścia $A$ o rozmiarze $|Q| \times |Q|$}
		    		\REQUIRE {zbiór krawędzi $E$, które występują w automacie}
		    		
		    		\STATE $Q1 = \{(start, 1)\}$; 
		    		\STATE $Q2 = \phi$;		    
		    		
		    		\FOR{$t=1$ to $T$}
			    		\STATE $Q2 = \phi$
			    		\FOR{$(q1, p1)$ in $Q1$}
				    		\FOR{$\{q2: (q1, q2) \in E \} $}
					    		\STATE $p_{q_2} = \bigg( p1\cdot a_{q,w}\cdot b_q(O_t) \label{line:b_q} \bigg)$;				    	
					    		\STATE $Q2 = Q2 \cup \{(q2, p_{q_2})\}$
				    		\ENDFOR
			    		\ENDFOR
			    		\STATE $swap(Q1, Q2)$;
		    		\ENDFOR
		    		
		    		\RETURN $\argmax_{(q,p)\in Q1}(p)$;
		    	\end{algorithmic}
	    	
		    	Zbiory $Q1$, $Q2$ przechowują pary $(stan, parwdopodobieństwo)$ 
		    	
		    	\caption{Algorytmy token passing}
		    	\label{alg:token_passing}
		    \end{figure}				   	
       

\subsubsection{Parametry dekodowania}
	\label{sec:meta_params}
	 	Wykorzystany w eksperymentach dekoder \textit{Julius} udostępnia szereg parametrów konfiguracyjnych wpływających na skuteczność i szybkość rozpoznawania. Przykładowo, jednym z nich jest szerokość wiązki sterująca algorytmem \textit{token passing} opisanym w rozdziale \ref{sec:recognition}. W przeprowadzonych eksperymentach była ona ustawiona na $6500$. Innymi parametrami jest \textit{waga modelu językowego} oraz \textit{kara za wstawienie słowa}, szczegółowy opis wszystkich dostępnym parametrów znaleźć można w \cite{juliusbook}. Oba wspomniane parametry wpływają na ocenę hipotezy podczas procesu rozpoznawania. Załóżmy, że ich wartość to odpowiednio $a$ dla wagi oraz $b$ dla kary. Niech hipoteza $h$ składa się ze słów $w_1, w_2, \cdots w_n$, którym zostały w procesie dekodowania przyporządkowane odpowiednio fragmenty obserwacji $o_1, o_2, \cdots o_n$, oczywiście $O=\overline{o_1,o_2,\cdots o_n}$, wtedy całkowita ocena $h$ wyrażona jest wzorem \ref{eqn:h_score}. Manipulując karą za wstawianie nowych słów, można zapobiec efektowi wypisywania wielu krótkich słów, zamiast właściwego długiego słowa, oraz dopisywaniu krótkich słów w miejscu ciszy. Efekt ten jest związany z łatwości dopasowanie słów o małej liczbie fonemów.  Wartości wagi oraz kary mogą być dobrane drogą eksperymentalną.
	
		\begin{equation}
			P_{acc}(h) = \prod_{i=1}^n P_{acc}(o_i|w_i)
		\end{equation}
		\begin{equation}
			P_{lng}(h) = P_{lng}(w_1)P_{lng}(w_2|w_1)\prod_{i=3}^nP(w_i|w_{i-1},w_{i-2})
		\end{equation}
		\begin{equation}
			P(h) = P_{acc}(h) \cdot \bigg(P_{lng}(h)\bigg)^a \cdot b^n
		\label{eqn:h_score}
		\end{equation}
	
\section{N-gramowy model językowy}
	\label{sec:ngramm}
	\textit{N-gramowy model językowy}, odpowiadający blokowi \refBlock{n_gramy} z rysunku \ref{fig:ARM_schemat}, zawiera prawdopodobieństwa słów ze słownika \refBlock{slownik}. Model o stopniu $n$ opisuje szansę wystąpienia słowa $w$ poprzedzonego $n-1$ słowami, co można zapisać jako: $P(w|w_{n-1},w_{n-2},\cdots,w_1)$. Oczywiście N-gramowy model zawiera w sobie również modele o niższy stopniu, które są wykorzystywane na początku zdania. Modele językowe buduje się przez zliczanie wystąpień kombinacji słów w pewnym korpusie treningowym. Wraz ze wzrostem $n$ wykładniczo rośnie ilość tekstu potrzebna do zbudowania modelu. W konsekwencji, przy ograniczonej liczbie wypowiedzi, niektóre kombinacje z warunkowego członu prawdopodobieństwa  $P(w|w_{n-1},w_{n-2},\cdots,w_1)$ nie występują. Aby umożliwić rozpoznanie takiego słowa $w$, poprzedzonego słowami $w_{n-1},w_{n-2},\cdots,w_1$ stosuje się techniki \textit{wygładzania} (ang: \textit{smoothing}) takie jak przykładowo \textit{Add-on} czy \textit{Katz smoothing}. Ideą tych algorytmów jest przypisanie niewystępującym kombinacjom niezerowego prawdopodobieństwa. Wykorzystane w eksperymentach modele językowe miały stopień $3$.

\chapter{Neuronowy system rozpoznający mowę}
\section{Rozpoznawanie mowy z wykorzystaniem sieci neuronowych}
	\label{sec:ASR_NN}
	W prezentowanym podejściu do rozpoznawania mowy z wykorzystaniem sieci neuronowych, sieć jest wykorzystywana do wyznaczania prawdopodobieństw stanów na podstawie wektora cech, co odpowiada blokwi \refBlock{klasyfikator} z rysunku \ref{fig:ARM_schemat}).
	
	\subsection{Splotowe sieci neuronowe}
		
		Splotowe sieci neuronowe, będące rozwinięciem klasycznych sieci, zostały pierwotnie zaprojektowane na potrzeby rozpoznawania obrazów. Zyskały szerokie uznanie po prezentacji sieci, opisanej w artykule \cite{alex_net}. Sieć składa się zarówno ze standardowych warstw, w których wyjście jest opisane wzorem \ref{eqn:nn_std}, jak i splotowych.
		W klasycznej warstwie wejściem jest wektor $Input$, natomiast wyjściem wektor $Output$ przemnożony przez macierz $W$, która powstaje w wyniku treningu sieci i przechowuje zdobytą wiedzę. Dodatkowo na każdy z osobna element wektora wyjściowego, nakłada się nieliniową funkcję $f$ zwaną \textit{funkcją aktywacji}. Przykładowe funkcje aktywacji to $\max(0, x)$, $\max(0.0x, x)$ lub $\tanh(x)$. Więcej funkcji aktywacji wraz ze wzorami można znaleźć w dokumentacji biblioteki wykorzystanej w eksperymentach\footnote{http://lasagne.readthedocs.io/en/latest/modules/nonlinearities.html}. 
		
		\begin{equation}
			Output = f(Input \cdot W)
			\label{eqn:nn_std}
		\end{equation}
		
		Warstwa splotowa składa z zadanej parametrem $n$ liczby filtrów, o ustalonym kształci $a\times b$. Podczas treningu wyznaczane są współczynniki filtrów. Wejściem warstwy jest macierzy trójwymiarowa o wymiarach $x \times y \times k$, natomiast wyjściem jest macierz trójwymiarowa o wymiarach $x - a + 1 \times y - b + 1 \times n$. Podobnie jak przy klasycznych warstwach, tutaj również na wyjście nakłada się funkcje aktywacji $f$. Pojedyncze wyjście $Output_{p,q,m}$ opisane jest wzorem \ref{eqn:nn_conv}, gdzie $Input[p:p+a, q:q+b, i]$ jest podmacierzą, $W_m$ współczynnikami $m$-tego filtra, natomiast $\ast$ operacją splotu.
		
		\begin{equation}
		Output_{p,q,m} = f\bigg(\sum_{i=1}^k Input[p:p+a, q:q+b, i] \ast W_m\bigg)
		\label{eqn:nn_conv}
		\end{equation}
		
		Sieć powstaje poprzez łączenie ze sobą wielu warstw, tak aby wyjście jednej było wejściem kolejnej warstwy. Trening sieci wykonywany jest za pomocą algorytmu \textit{back propagation} opisanego w artykule \cite{pack_propagation}. Podczas treningu pokazuje się sieci kolejno wektory wejściowe i modyfikuje współczynniki $W$, tak aby zminimalizować błąd wyjścia całej sieci.
	
	\subsection{Sieć jako estymator prawdopodobieństw stanów }
	\label{sec:nn_estymator}
		Sieć neuronowa wykorzystywana jako klasyfikator, dostaje na wejściu $k$-elementowy wektor cech $v \in R^k$, natomiast na wyjściu zwraca $n$-elementowy wektor $w \in R_+^n$, gdzie $n$ jest liczbą rozpoznawanych klas. W idealnym klasyfikatorze $w \in \{0,1\}^n$ i dokładnie jeden element ma wartość $1$, a pozostałe $0$. W praktyce, klasyfikatory nie udzielają tak idealnych odpowiedzi, dlatego uznajemy, że klasyfikator wybiera klasę o największej wartości w wektorze wyjściowym. Aby nadać wyjściu klasyfikatora charakter prawdopodobieństwa, należy nałożyć na niego funkcję \textit{softmax} opisaną równaniem \ref{eqn:softmax}. Po złożeniu z funkcją \ref{eqn:softmax} klasyfikator będzie wyznaczał $P(w_i|v)$, czyli prawdopodobieństwo $i$-tej klasy pod warunkiem wektora wejściowego $v$.
		
		\begin{equation}
			\sigma(w)_k=\frac{\mathrm{e}^{w_k}}{\sum_{i=1}^{n} \mathrm{e}^{w_i}}
			\text{,   gdzie $n$ to szerokości wektora $w$}
			\label{eqn:softmax}
		\end{equation}
		
		Zgodnie z równaniem \ref{equation:ASR_definicja2} do rozpoznawani mowy potrzeba $P(O|W)$, co oznacza, że klasyfikator musi wyznaczać $P(v|w_i)$. Korzystając ze wzoru Bayes i obserwacji, że $P(v)$ nie ma wpływu na rozpoznawanie, gdyż jest takie samo dla wszystkich hipotez, otrzymujemy:
		
		\begin{equation}
			P(v|w_i) = \frac{P(w_i|v) \cdot P(w_i)}{P(v)}\simeq P(w_i|v) \cdot P(w_i)
			\label{eqn:softmax2}
		\end{equation}
		
		 W proponowanym podejściu zastosowania sieci neuronowej przy rozpoznawaniu mowy, wejściem sieci jest $k$ $123$-elementowych wektorów cech, wyznaczanych zgodnie z opisem w rozdziale \ref{sec:Feature_vec_mfsc}. Parametr $k$ określa liczbę kolejnych ramek dostarczanych sieci w wektorze wejściowym. Poprzez \textit{szerokość kontekstu} rozumieć będziemy liczbę ramek na prawo i lewo od rozpoznawanej obserwacji, zatem $k = 2 \cdot kontekst + 1$. Zwiększanie kontekstu wiąże się ze zmniejszeniem liczby obserwacji, gdyż pierwsza obserwacja może wystąpić dopiero po liczbie ramek równej $kontekst + 1$. Rysunek \ref{fig:nn_context} pokazuje, jak kolejne ramki są agregowane przy tworzeniu obserwacji. W przykładzie z rysunku szerokość kontekstu wynosi $2$, zatem każda obserwacja jest tworzona z $5$ kolejnych ramek. Wyjściem sieci jest wektor o szerokości równej liczbie stanów modelu Markowa. Uwzględniając równanie \ref{eqn:softmax2}, wartość funkcji $b_q(o_t)$ z \ref{line:b_q}. linii algorytmu \ref{alg:token_passing} odpowiada wyrażeniu:
		 
		 \begin{equation}
			 b_q(O_t) = NN(O_t) \cdot P(w_q)			 
		 \end{equation}
		 ,gdzie NN to sieć neuronowa rozumiana jako funkcja.
		
	\subsection{Prawdopodobieństwo stanów \textit{apriori} }
		W rozdziale \ref{sec:nn_estymator} opisano, jak wykorzystać sieć neuronową do wyznaczania prawdopodobieństw stanów, jednak wymaga to wcześniejszego wyznaczenia prawdopodobieństw stanów apriori, czyli $P(w_i)$ ze wzoru \ref{eqn:softmax2}. 
		$P(w_i)$ może być łatwo wyznaczone podczas przygotowywani danych do uczenia sieci neuronowej. Na etapie generowania wektorów wyjściowych do treningu, możliwe jest zliczanie wystąpień stanu $q_i$.
		Następnie wystarczy dla każdego stanu obliczyć:
		
		\begin{equation}
			P(w_k) = \frac{c_k}{\sum_{i=1}^{|Q|} c_i}
			\text{,   gdzie $c_i$ to liczba wystąpień stanu $q_i$}
		\end{equation}
		
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[node distance=0.8cm]
			
			\tikzstyle{frame} = [rectangle, draw, text centered, rounded corners, minimum height=0.7em, minimum width=4em, rotate=90,anchor=north]
			
			\begin{scope}
			\node [frame] (fr1) {$t_1$};
			\node [frame, below of=fr1] (fr2) {$t_2$};
			\node [frame, below of=fr2] (fr3) {$t_3$};
			\node [frame, below of=fr3] (fr4) {$t_4$};
			\node [frame, below of=fr4] (fr5) {$t_5$};
			\node [frame, below of=fr5] (fr6) {$t_6$};
			\node [frame, below of=fr6] (fr7) {$t_7$};
			\node [frame, below of=fr7] (fr8) {$t_8$} node [black,midway,yshift=-2.3cm, xshift=5.8cm] {\footnotesize $\ldots$};
			\node [frame, below of=fr8] (fr9) {$t_9$};
			\node [frame, below of=fr9] (fr10) {$t_{10}$};
			\node [frame, below of=fr10] (fr11) {$t_{11}$};
			\node [frame, below of=fr11] (fr12) {$t_{12}$};
			\node [frame, below of=fr12] (fr13) {$t_{13}$};
			\node [frame, below of=fr13] (fr14) {$t_{14}$};
			\node [frame, below of=fr14] (fr15) {$t_{15}$};
			
			
			
			
			\draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=5pt},yshift=10pt]
			(fr1.north west) -- (fr5.south west) node [black,midway,yshift=-1.5cm] {\footnotesize $o_3$};
			\draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=12pt},yshift=10pt]
			(fr2.north west) -- (fr6.south west) node [black,midway,yshift=-1.5cm] {\footnotesize $o_4$};
			\draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=19pt},yshift=10pt]
			(fr3.north west) -- (fr7.south west) node [black,midway,yshift=-1.5cm] {\footnotesize $o_5$};
			\draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=26pt},yshift=10pt]
			(fr4.north west) -- (fr8.south west) node [black,midway,yshift=-1.5cm] {\footnotesize $o_6$};
			
			\draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=5pt},yshift=10pt]
			(fr8.north west) -- (fr12.south west) node [black,midway,yshift=-1.5cm] {\footnotesize $o_{10}$};
			\draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=12pt},yshift=10pt]
			(fr9.north west) -- (fr13.south west) node [black,midway,yshift=-1.5cm] {\footnotesize $o_{11}$};
			\draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=19pt},yshift=10pt]
			(fr10.north west) -- (fr14.south west) node [black,midway,yshift=-1.5cm] {\footnotesize $o_{12}$};
			\draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=26pt},yshift=10pt]
			(fr11.north west) -- (fr15.south west) node [black,midway,yshift=-1.5cm] {\footnotesize $o_{13}$};
		
			\end{scope}
			
			\end{tikzpicture}
			\caption{Tworzenie obserwacji dla sieci neuronowej przy kontekście równym $2$.}
			\label{fig:nn_context}
			
		\end{figure}
		
	\subsection{Architektura sieci i układ danych}
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
		
		\begin{scope}
		
		\node at (0.0,3.25){warstwa wejściowa};
		\node at (3.6,3.25){warstwy splotowe};
		\node at (7.5,3.25){warstwy pełne};
		
		\def\w{1.5}		
		
		\def\x{0.0}	
		\def\y{0.5}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \w};
		\draw[fill=black,fill=gray!20,draw=black] (\x,\y) -- (\x,\Y) -- (\X,\Y) -- (\X,\y) -- (\x,\y);
		
		\def\x{0.25}	
		\def\y{0.25}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \w};
		\draw[fill=black,fill=gray!20,draw=black] (\x,\y) -- (\x,\Y) -- (\X,\Y) -- (\X,\y) -- (\x,\y);
		
		\def\x{0.5}	
		\def\y{0.0}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \w};
		\draw[fill=black,fill=gray!20,draw=black] (\x,\y) -- (\x,\Y) -- (\X,\Y) -- (\X,\y) -- (\x,\y);
		
		\def\w{0.75}
		
		\def\x{3.0}	
		\def\y{1.25}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \w};
		\draw[fill=black,fill=gray!20,draw=black] (\x,\y) -- (\x,\Y) -- (\X,\Y) -- (\X,\y) -- (\x,\y);
		
		\def\x{3.25}	
		\def\y{1.0}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \w};
		\draw[fill=black,fill=gray!20,draw=black] (\x,\y) -- (\x,\Y) -- (\X,\Y) -- (\X,\y) -- (\x,\y);
		
		\def\x{3.5}	
		\def\y{0.75}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \w};
		\draw[fill=black,fill=gray!20,draw=black] (\x,\y) -- (\x,\Y) -- (\X,\Y) -- (\X,\y) -- (\x,\y);
		
		\def\x{3.75}	
		\def\y{0.5}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \w};
		\draw[fill=black,fill=gray!20,draw=black] (\x,\y) -- (\x,\Y) -- (\X,\Y) -- (\X,\y) -- (\x,\y);
		
		\def\x{4.0}	
		\def\y{0.25}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \w};
		\draw[fill=black,fill=gray!20,draw=black] (\x,\y) -- (\x,\Y) -- (\X,\Y) -- (\X,\y) -- (\x,\y);
		
		\def\x{4.25}	
		\def\y{0.0}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \w};
		\draw[fill=black,fill=gray!20,draw=black] (\x,\y) -- (\x,\Y) -- (\X,\Y) -- (\X,\y) -- (\x,\y);
			
		\draw (1.1,1.1) -- (1.3,1.1) -- (1.3,1.3) -- (1.1,1.3) -- (1.1,1.1);	
		\draw (1.3,1.3) -- (3.7,1.2);
		\draw (1.3,1.1) -- (3.7,1.2);
		
		\def\w{0.5}	
		\def\h{2.0}	
		\def\x{6.5}	
		\def\y{0.0}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \h};
		\draw[fill=black,fill=gray!20,draw=black] (\x,\y) -- (\x,\Y) -- (\X,\Y) -- (\X,\y) -- (\x,\y);
		
		\draw (3.75,2.0) -- (6.75,1.0);
		\draw (5.0,0.0) -- (6.75,1.0);
		
		\def\w{0.5}	
		\def\h{1.5}	
		\def\x{8.5}	
		\def\y{0.25}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \h};
		\draw[fill=black,fill=gray!20,draw=black] (\x,\y) -- (\x,\Y) -- (\X,\Y) -- (\X,\y) -- (\x,\y);
		
		\draw (7.0,2.0) -- (8.75,1.0);
		\draw (7.0,0.0) -- (8.75,1.0);
		
		\end{scope}
		
		\end{tikzpicture}
		\label{fig:cnn_architecture_1}
		\caption{Architektura splotowej sieci neuronowej.}
	\end{figure}

	Do wyznaczanie prawdopodobieństw stanów modelu Markowa wykorzystano splotową sieć neuronową. Sieć składa się z odpowiednio sformatowanej warstwy wejściowej, warstw splotowych oraz klasycznych, pełnych warstw na samym końcu. Rysunek \ref{fig:cnn_architecture_1} wizualizuje ogólną architekturę wykorzystanych sieci. Szczegółowy opis konfiguracji warstw znajduje się w rozdziale \ref{sec:experiments}. Jako funkcję błędu zastosowano \textit{entropię skrośna}. W celu zapobieżenia efektowi przeuczania dodano \textit{dropout} ze współczynnikiem odrzucania równym $0.5$ do pierwszej warstwy klasycznej oraz \textit{regularyzacje} parametrów ze współczynnikiem $10^{-5}$. Początkowy współczynnik uczenia wynosił $5 \cdot 10^{-3}$ i w każdej epoce, w której nie nastąpiła poprawo na zbiorze walidacyjnym, był mnożony przez $0.94$, trening trwał $160$ epok.
	
	Wejście sieci jest podzielone na trzy warstwy odpowiadające zerowej ($\epsilon, w$), pierwszej ($\epsilon', w'$) i drugiej pochodnej $\epsilon'', w''$), co odzwierciedla charakter wektorów cech wyznaczanych zgodnie z opisem w rozdziale \ref{sec:Feature_vec_mfsc}. Dzięki oddzieleniu pochodnych warstwami, dane pochodzące z różnych pochodnych nie są przetwarzane przez jeden filtr. Szerokości jednej warstwy wejściowej odpowiada liczbie uwzględnionych, przy ekstrakcji cech, filtrów MEL-owych wraz z całkowitą energią, łącznie $M+1$ liczb. Wysokości jest równa liczbie ramek z jakich składna się pojedyncza obserwacja, czyli $2\cdot kontekst + 1$. Na rysunku \ref{fig:cnn_architecture_input} zwizualizowano podział wektora wejściowego. 
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
		
		\begin{scope}
		
		\def\w{5.0}		
		\def\h{2.0}	
		
		\def\x{0.0}	
		\def\y{1.0}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \h};
		\draw[fill=black,fill=gray!20,draw=black] (\x,\y) -- (\x,\Y) -- (\X,\Y) -- (\X,\y) -- (\x,\y);
		
		\draw [decorate,decoration={brace,amplitude=10pt,raise=26pt},xshift=8pt]
		(\x, \y) -- (\x, \Y) node [black,midway,xshift=-1.5cm, rotate=90] {\footnotesize $2\cdot kontekst + 1$};
		
		\def\x{0.5}	
		\def\y{0.5}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \h};
		\draw[fill=black,fill=gray!20,draw=black] (\x,\y) -- (\x,\Y) -- (\X,\Y) -- (\X,\y) -- (\x,\y);
		
		\def\x{1.0}	
		\def\y{0.0}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \h};
		\draw[fill=black,fill=gray!20,draw=black] (\x,\y) -- (\x,\Y) -- (\X,\Y) -- (\X,\y) -- (\x,\y);
		
		\draw (\x,\Y) -- (\X,\Y)  node [black,midway,below] {$\epsilon_{t-1}, w_{t-1}$};
		\pgfmathsetmacro{\H}{\y + 2 * \h / 3};
		\draw [gray!80] (\x,\H) -- (\X,\H)  node [black,midway,below] {$\epsilon_{t}, w_{t}$};
		\pgfmathsetmacro{\H}{\y + \h / 3};
		\draw [gray!80] (\x,\H) -- (\X,\H)  node [black,midway,below] {$\epsilon_{t+1}, w_{t+1}$};
		
		\draw [decorate,decoration={brace,amplitude=10pt,mirror,raise=26pt},yshift=10pt]
		(\x, \y) -- (\X, \y) node [black,midway,yshift=-1.5cm] {\footnotesize liczba filtrów (M) + całkowita energia};
		
		\end{scope}
		
		\end{tikzpicture}
		\label{fig:cnn_architecture_input}
		\caption{Wejście splotowej sieci neuronowej.}
	\end{figure}

\section{ Zastosowanie modelowania fonemów z ograniczonym kontekstem w ASR }	
	Wykorzystując ukryte modele Markowa, tak jak opisano w rozdziale \ref{sec:ASR_HMM}, wymaga się policzenia, dla każdej ramki, prawdopodobieństwa obserwacji pod warunkiem każdego ze zdefiniowanych stanów. Wykorzystując tri-fony przy rozpoznawaniu mowy, trzeba zdefiniować $O(\sigma^3)$ stanów. Jednak tylko niektóre z tri-fonów o wspólnym środkowym fonemie różnią się w istotny sposób. Oczywiście wraz ze wzrostem liczby fonemów, rośnie liczba obliczeń, które trzeba wykonać, co ma negatywny wpływ na szybkość rozpoznawania. Ponadto, przy wykorzystaniu sieci neuronowych zgodnie z podejściem opisanym w rozdziale \ref{sec:ASR_NN} trzeba wytrenować więcej parametrów, co z kolei utrudnia lub wręcz uniemożliwia proces uczenia. Aby ograniczyć negatywny wpływ dużej liczby fonemów, zastosowano technikę opisaną w rozdziale \ref{sec:tri-fone_mapping}. Dzięki niej możliwe jest znaczne ograniczenie liczby stanów, a w konsekwencji szerokości warstwy wyjściowej w sieci neuronowej.
	
	\subsection{ Definicja modelu z ograniczonym kontekstem }
		\label{sec:tri-fone_mapping}
		Korzystając ze spostrzeżenia, że tylko niektóre tri-fony o wspólnym środkowym fonemie istotnie się różnią, można zastosować mechanizm współdzielenia modeli przez wiele tri-fonów, tworząc tym samym \textit{klasy abstrakcji}. Grupa tri-fonów należeć będzie do tej samej klasy abstrakcji, jeśli opisana jest takimi samymi parametrami, w skład których wchodzą stany tworzące fonem oraz prawdopodobieństwa przejść pomiędzy nimi. Z przyczyn implementacyjno-techniczny każda klasa abstrakcji będzie miała swojego reprezentanta, zwanego \textit{tri-fonem fizycznym}. Rysunek \ref{fig:virtual_phisical_tri-phones} pokazuje przykładowe klasy abstrakcji. Oznaczenia tri-fonów z rysunku są zgodne z konwencją przedstawioną w rozdziale \ref{sec:trifones_definition}. Schemat przedstawia trzy klasy abstrakcji 
		\begin{itemize}
			\item $(d-b+e) \equiv (a-b+c) \equiv (c-b+d)$
			\item $(k-l+m)$ 
			\item $(x-y+z) \equiv (w-y+t) \equiv (u-y+t)$
		\end{itemize} 
		Każda z nich jest zaznaczona ramką. Reprezentantem jest tri-fon z którego wychodzą bezpośrednie przejścia do stanów. Stany z których zbudowani są reprezentanci klas abstrakcji mogą być współdzielone. Oba mechanizmy, współdzielenie pojedynczych stanów oraz całych konfiguracji, umożliwia bardzo skuteczne ograniczenie liczby parametrów, które trzeba wyliczyć podczas procesu rozpoznawania i treningu modelu akustycznego. W tabeli \ref{tab:acustic_models} umieszczono zestawienie modeli akustycznych, wytrenowanych na potrzeby eksperymentu, w kolumnie \textit{Liczba stanów} wpisano liczbę stanów, do jakiej ograniczono modele tri-fonowe, korzystając z opisanego mechanizmu. \\
		Poprzez \textit{Model z ograniczonym kontekstem} będziemy rozumieć akustyczny model tri-fonowy, w którym ograniczono liczbę stanów modelu Markowa poprzez oba opisane powyżej sposoby.
		
		
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[node distance=1.2cm]
			
			\begin{scope}
						
			\def\x{0.8}
			\def\w{\x * 5}
			\def\z{\x * 2}
			\def\y{1.4}
			\def\j{2.0}
			
			\draw[dotted] (-1,-4) rectangle (3.2,1);
			\draw[dotted] (4,-4) rectangle (6.3,1);
			\draw[dotted] (6.75,-4) rectangle (10.85,1);
			
			\node [hmm] (p1) {d-b+e};
			\node [hmm, right = \x cm of p1] (p2) {a-b+c};
			\node [hmm, below = \y cm of p2] (p3) {c-b+d};
			
			\node [hmm, right = \z cm of p3] (p4) {k-l+m};
			
			\node [hmm, right = \w cm of p2] (p5) {x-y+z};
			\node [hmm, right = \x cm of p5] (p6) {w-y+t};
			\node [hmm, below = \y cm of p5] (p7) {u-y+t};
			

			\draw [->] (p1) [out=270, in=140] to  (p3);
			\draw [->] (p2) [out=280, in=80] to  (p3);
			
			\draw [->] (p6) [out=270, in=40] to  (p7);
			\draw [->] (p5) [out=260, in=100] to  (p7);
			
			\node [hmm, below = \j cm of p3] (s1) {s1};
			\node [hmm, right = \x cm of s1] (s2) {s2};
			\node [hmm, right = \x cm of s2] (s3) {s3};
			\node [hmm, right = \x cm of s3] (s4) {s4};
			\node [hmm, right = \x cm of s4] (s5) {s5};
			
			\draw [->] (p3) [out=240, in=100] to (s1);
			\draw [->] (p3) [out=270, in=90]  to (s2);
			\draw [->] (p3) [out=300, in=130] to (s5);
			
			\draw [->] (p4) [out=240, in=80]  to (s1);
			\draw [->] (p4) [out=270, in=90]  to (s3);
			\draw [->] (p4) [out=300, in=100] to (s5);
			
			\draw [->] (p7) [out=240, in=50]  to (s1);
			\draw [->] (p7) [out=270, in=90]  to (s4);
			\draw [->] (p7) [out=300, in=80] to (s5);
			\end{scope}
			
			\end{tikzpicture}
			\caption{Współdzielenie modeli przez klasy abstrakcji.}
			\label{fig:virtual_phisical_tri-phones}
			
		\end{figure}
	
	\subsection{Metody ograniczania kontekstu}
	\label{sec:state_red}
	Wykorzystany do budowy modeli akustycznych pakiet \textit{HTK} oferuje dwie metody ograniczania liczby stanów. Pierwsza metoda o nazwie \textit{Data driven clustering} zakłada, że na wejściu dostaje się zbiór tri-fonów. Każdy z tri-fonów ma przypisane trzy stany, zgodnie z opisem w rozdziale \ref{sec:Phones}. Następnie porównuje się zróżnicowanie w obrębie pierwszych, drugich i trzecich stanów. Jeśli dwa stany są podobne, łączy się je. W efekcie, wraz z każdą iteracją algorytmu, otrzymuje się mniejszy zbiór stanów wykorzystywanych przez zbiór tri-fonów. Jako miarę podobieństwa pomiędzy stanami wykorzystuje się odległość Euklidesową średnich z rozkładów normalnych modelujących stan. Wadą tej metody jest niemożność obsłużenia tri-fonów, które nie wystąpiły w zbiorze treningowym.
	
	Drugą metodą z pakietu \textit{HTK}, która została wykorzystana podczas eksperymentów, jest \textit{Tree- based clustering}. Twórcy \textit{Tree- based clustering} opisali swój algorytm w artykule \cite{tree_based_clustring}. W przeciwieństwie do \textit{Data driven clustering}, ta metoda obsługuje również tri-fony, które nie pojawiły się w zbiorze treningowym. Metoda ta wymaga wczytania zbioru reguł, opisujących zależności pomiędzy fonemami. Reguły powinny odzwierciedlać fizyczne podobieństwa w wymowie fonemów. Ponadto, każda z reguł powinna dzielić cały zbiór zdefiniowanych fonemów na dwa podzbiory. Przykładowymi regułami są:
	\begin{itemize}
		\item Czy fonem x jest nosowy?
		\item Czy fonem x jest spółgłoską zwartą?
		\item Czy fonem x jest dźwięczny?
		\item Czy fonem x jest spółgłoska miękką?
		\item Czy fonem x jest spółgłoską twardą?
		\item Czy fonem x jest głoską ustną? 
		\item Czy fonem x jest spółgłoską sonorną? 
	\end{itemize}
	 Zbiór wykorzystanych reguł został zbudowany na podstawie zaleceń z \textit{HTK Book}\cite{htkbook} oraz ogólnodostępnych dostępnych zasad polskiej fonetyki. Na podstawie reguł odbywa się dzielenie tri-fonów. Po wczytaniu zbioru tri-fonów współdzielących lewy lub prawy stan, wszystkie zdefiniowane reguły są uruchamiane. W efekcie dostaje się zbiór hipotetycznych podziałów, z których wybierany jest jeden. Aby zdecydować, który podział jest najlepszy, dla każdego z nich, liczy się prawdopodobieństwo zbiory treningowego. Reguła maksymalizująca prawdopodobieństwo jest wybierana. \\
	Algorytm \textit{Tree-based clustering} został zapisany w pseudokodzie \ref{alg:tree_based_clustering}. Kod uwzględnia tylko podziały lewego stanu spośród trójki definiującej tri-fon. Linie od \ref{line:iter_begin} do \ref{line:iter_end} opisują jedną iterację, w wyniku której wybierana jest najkorzystniejsza reguła. Wraz z każdą iteracją, zwiększa się liczba stanów w zbiorze $Q$, a jednocześnie zmniejsza się liczba tri-fonów współdzielących ten sam stan. Najbardziej zewnętrzna pętla wykonywana jest dopóki istnieje jakiś  podział spełniający wymagane kryteria. Pętla w linii \ref{line:loop_Q} przechodzi po wszystkich stanach, które są aktualnie zdefiniowane w modelu akustycznym. Następnie wyliczany jest zbiór $P$ zawierający tri-fony współdzielące pierwszy stan. W kolejnej pętli z linii \ref{line:loop_R} przechodzi się przez zdefiniowane reguły podziału. Przy pomocy wybranej reguły $r$ zbiór $P$ dzieli się na dwa podzbiory $P_l, P_r$. W liniach \ref{line:f_l} oraz \ref{line:f_r} wybierane są ramki, które podczas rozpoznawania zostały przyporządkowane do stanu $q_l$ i $q_r$. Warunek w linii \ref{line:if_n} sprawdza, czy oba zbiory są odpowiednio liczne. Następnie korzystając z wyznaczonych zbiorów trenuje się parametry rozkładów opisujących dwa nowe stany $q_l, q_r$. Ostatecznie wylicza się w linii \ref{line:r_prob} prawdopodobieństwo zbiorów $f_l, f_r$, które stanowią ocenę reguły podziału. Wyliczona ocena jest dodawana do zbioru $X$. Po przejściu przez wszystkie stany z pętli w linii \ref{line:loop_Q} wybiera się regułę o najlepszej ocenie. Reguła ta jest następnie wykorzystania do rozbudowy modelu akustycznego o nowy stan w liniach \ref{line:q_ext} oraz \ref{line:p_eqt}. Na końcu działania, w linii \ref{line:return}, algorytm zwraca rozbudowany zbiór stanów $Q$ oraz definicji tr-fonów $Phones$.\\
	Na rysunku \ref{fig:trr-based_clustering_example} znajduje się przykład działania opisanego algorytmu. Ilustruje on, jak podziały wpływają na współdzielenie stanów. Początkowo wszystkie trzy tri-fony dzielą te same stany. Odpowiada to sytuacji, gdy mamy tylko jedną klasę abstrakcji. W pierwszej iteracji wybrana została reguła dla pierwszego stanu, dzieląca fonemy na dwie grupy $\{a,w\},\{n\}$. W efekcie stan $s1$ został podzielony na dwa $s0,s1$. W kolejnej iteracji wybrano regułę dla stanu trzeciego dzielącą fonemy na grupy $\{z\},\{b\}$. Stan $s3$ został podzielony na dwa nowe $s3,s4$. W wyniku działania algorytmu powstały trzy klasy abstrakcji, każda z nich ma inny zbiór stanów, które ją modelują. \\
	Proces podziału w algorytmie \textit{tree-based clustering} jest iterowany tak długo, do póki nie osiągnie się kryterium stopu. Jednym z możliwych kryteriów jest \textit{minimalna liczba obserwacji}. W pseudokodzie \ref{alg:tree_based_clustering} warunek z linii \ref{line:if_n} jest związany właśnie tym kryterium. Stosując ten warunek wymaga się, aby przy podziale na dwa nowe stany, oba z nich miały co najmniej $n$ obserwacji, gdzie $n$ jest zadanym parametrem. Drugim kryterium jest \textit{poprawa prawdopodobieństwa obserwacji}, zapisana w linii \ref{line:if_p}. Wymaga się przy nim, aby po podziale prawdopodobieństwo obserwacji przypisanych do dzielonego stanu zwiększyło się o podany współczynnik $\lambda$. Wymóg ten jest opisany wzorem \ref{eqn:p_progress}, ze względów numerycznych stosuje się tu skalę logarytmiczną. Oznaczenia we wzorze są takie same, jak w pseudokodzie. Prawdopodobieństwo $P(o|q)$ jest wyliczane zgodnie z opisem w rozdziale \ref{sec:gmm_estimation}.
	 
	 \begin{equation}
	 	\log{P(f_l | q_l)} + \log{P(f_r | q_r)} - \log{P(f_l | q)} - \log{P(f_r | q)} > \lambda
	 	\label{eqn:p_progress}
	 \end{equation}
	 
	Ustawiając wysoką liczbę wymaganych obserwacji oraz duży współczynnik poprawy można zdławić proces dzielenia się tri-fonów i otrzymać model o małej liczbie stanów. Analogicznie przesuwając parametry w druga stronę można zbudować bardzo duży model. Warto zauważyć, że w przypadku tego algorytmu, nie ma sensu poddawać podziałowi środkowego stanu.  \\
	Rysunek \ref{fig:tree-based_clustering-tree} pokazuje zagłębianie się reguł nałożonych na pierwszy stan wraz z kolejnymi iteracjami. Początkowo wybierana jest reguła pytająca o spółgłoski zwarte, która dzieli tri-fony na dwie grupy. Następnie obie grupy dzielone są regułą odpytująca o fonemy nosowe. \\
	Dodatkowo po wykonaniu podziałów stanów algorytmem \textit{Tree-based clustering}, można zastosować algorytm podobny do opisanego \textit{Data driven clustering} w celu połączenia podobnych stanów pochodzących z tri-fonów o różnym fonemie centralnym. Proces ten nazywa się \textit{Parameter Tying}.
	
	\begin{figure}
		\begin{algorithmic}[1]
			
			\REQUIRE {Zbiór reguł $Rules$}
			\REQUIRE {Zbiór stanów $Q$}
			\REQUIRE {Zbiór tri-fonów $Phones$}
			\REQUIRE {Zbiór ramek $F$}
			
			\WHILE{$TRUE$}
				\STATE $X = \emptyset \label{line:iter_begin}$
				\FOR{$q \in Q$ \label{line:loop_Q}}
					\STATE $P = \Big\{\langle q_1, q_2, q_3\rangle \in Phones: q_1 = q, q_2 \in Q, q_3 \in Q\Big\}$
					\FOR {$r \in Rules \label{line:loop_R}$}
						\STATE $\langle P_l, P_r \rangle = r(P)$			
						\STATE $q_l = P_{l,1}$
						\STATE $q_r = P_{r,1}$
						\STATE $f_l = F(q_l)$ \label{line:f_l}
						\STATE $f_r = F(q_r)$ \label{line:f_r}
						\IF {$|f_l| > n \AND |f_r| > n \label{line:if_n}$}
							\STATE $q_l.params = estimate(f_l)$
							\STATE $q_r.params = estimate(f_r)$
							\IF {$\log{P(f_l | q_l)} + \log{P(f_r | q_r)} - \log{P(f_l | q)} - \log{P(f_r | q)} > \lambda$ \label{line:if_p}}
								\STATE $prob = P(f_l | q_l) \cdot P(f_r | q_r)$
								\STATE $X = X \cup \{\langle prob, r, q, r_l, q_r\rangle\}$ \label{line:r_prob}
							\ENDIF
						\ENDIF
					\ENDFOR
				\ENDFOR \label{line:iter_end}
				
				\IF{$X \neq \emptyset$}
					
					\STATE $\langle prob, r, q, q_l, q_r\rangle = \argmax_{\langle prob, r, q, r_l, q_r\rangle\in X}(prob)$;
					\STATE $Q = Q \setminus {q} \cup \{q_l, q_r\} \label{line:q_ext}$
					\STATE $Phones = rebuild(Phones, Q)\label{line:p_eqt}$
				\ELSE
					\RETURN{ $\langle Q, Phones \rangle \label{line:return}$}
				\ENDIF
			\ENDWHILE
		\end{algorithmic}
		
		Zbiory $X$ przechowują pary $(parwdopodobieństwo, reguła, dzielony stan)$ 
		
		\caption{Algorytmy tree-based clustering}
		\label{alg:tree_based_clustering}
	\end{figure}

	\begin{figure}
		\centering
		\begin{tikzpicture}[node distance=1.2cm]
		
		\begin{scope}
		
		\tikzstyle{state}=[circle,thick,draw=gray!75,fill=gray!20,minimum size=1mm];
		\tikzstyle{node}=[circle,thick,draw=gray!100,minimum size=1mm];
		
		\def\xl{-2.5}
		\def\xa{-0.85}
		\def\xb{0}
		\def\xc{0.85}
		
		\def\ya{-0.85}
		\def\yb{0}
		\def\yc{0.85}
		\def\yd{-3.0}
		\def\ye{-4.6}
		\def\yf{-6.3}
		
		\def\r{1.5}
		
		\draw [dashed] (-1.2, 1.8) -- (-1.2, -1.8);
		\draw [dashed] (-0.5, 1.8) -- (-0.5, -1.8);
		
		
		\node [state] (s1_1) at (\xa, \ya) {};
		\node [state] (s1_2) at (\xb, \ya) {};
		\node [state] (s1_3) at (\xc, \ya) {};
		\node at (\xl, \ya) {$a-o+z$};
		
		\path [line] (s1_1) -> (s1_2);
		\path [line] (s1_2) -> (s1_3);
		\draw[thick,->] (s1_1.110) arc (240:-65:\r mm);
		\draw[thick,->] (s1_2.110) arc (240:-65:\r mm);
		\draw[thick,->] (s1_3.110) arc (240:-65:\r mm);
		
		\node [state] (s2_1) at (\xa, \yb) {};
		\node [state] (s2_2) at (\xb, \yb) {};
		\node [state] (s2_3) at (\xc, \yb) {};
		\node at (\xl, \yb) {$r-o+b$};
		
		\path [line] (s2_1) -> (s2_2);
		\path [line] (s2_2) -> (s2_3);
		\draw[thick,->] (s2_1.110) arc (240:-65:\r mm);
		\draw[thick,->] (s2_2.110) arc (240:-65:\r mm);
		\draw[thick,->] (s2_3.110) arc (240:-65:\r mm);
		
		\node [state] (s3_1) at (\xa, \yc) {};
		\node [state] (s3_2) at (\xb, \yc) {};
		\node [state] (s3_3) at (\xc, \yc) {};
		\node at (\xl, \yc) {$w-o+z$};
		
		\path [line] (s3_1) -> (s3_2);
		\path [line] (s3_2) -> (s3_3);
		\draw[thick,->] (s3_1.110) arc (240:-65:\r mm);
		\draw[thick,->] (s3_2.110) arc (240:-65:\r mm);
		\draw[thick,->] (s3_3.110) arc (240:-65:\r mm);
		
		\path [line] (\xa, -1.5) -> (\xa, -2.4);
		
		//--------------- POZIOM DRZEWA 0 ----------------------
		
		\node [node] (r0_1) at (\xa, \yd) {}; \node [right of= r0_1, xshift=60] {Czy fonem jest spółgłoską zwartą?};
		
		//--------------- POZIOM DRZEWA 1 ----------------------
		
		\node [node] (r1_1) at (-2.2, \ye) {}; \node [left of= r1_1, xshift=-35] {Czy fonem jest nosowy?};
		\node [node] (r1_2) at (0.4, \ye) {}; \node [right of= r1_2, xshift=35] {Czy fonem jest nosowy?};
		
		\draw  (r0_1) -- (r1_1) node [left, midway] {tak};
		\draw  (r0_1) -- (r1_2) node [right, midway] {nie};
		
		//--------------- POZIOM DRZEWA 2 ----------------------
				
		\node [node] (r2_1) at (-3.0, \yf) {};
		\node [node] (r2_2) at (-1.6, \yf) {};
		
		\draw  (r1_1) -- (r2_1) node [left, midway] {tak};
		\draw  (r1_1) -- (r2_2) node [right, midway] {nie};
		
		\node [node] (r2_3) at (-0.4, \yf) {};
		\node [node] (r2_4) at (1.0, \yf) {};
		
		\draw  (r1_2) -- (r2_3) node [left, midway] {tak};
		\draw  (r1_2) -- (r2_4) node [right, midway] {nie};
		
		//--------------- POZIOM DRZEWA 3 ----------------------
		
		\end{scope}
		
		\end{tikzpicture}
		\caption{Tree-based clustering - reguły}
		\label{fig:tree-based_clustering-tree}
		
	\end{figure}
	
	\begin{figure}
		\centering
		\begin{tikzpicture}[node distance=1.2cm]
		
		\begin{scope}
		
		\tikzstyle{state}=[circle,thick,draw=gray!75,fill=gray!20,minimum size=1mm];
		\tikzstyle{node}=[circle,thick,draw=gray!100,minimum size=1mm];
		
		\def\ya{0}
		\def\yb{-4.2}
		\def\yc{-8.6}
		
		\def\d{0.5}
		\def\s{0.8}
		\def\h{0.4}
		
		\def\hl{1.5}
		\def\hs{0.2}
		
		\def\r{1.5}
		
		//-------------------  KROK 1 ---------------------
		
		\node [state] (s1_1) at (0, \ya) {};
		\node [state, right = \d cm of s1_1] (s1_2) {};
		\node [state, right = \d cm of s1_2] (s1_3) {};
		\node [above = \h cm of s1_2] {$a-o+b$};
		
		\path [line] (s1_1) -> (s1_2);
		\path [line] (s1_2) -> (s1_3);
		\draw[thick,->] (s1_1.110) arc (240:-65:\r mm);
		\draw[thick,->] (s1_2.110) arc (240:-65:\r mm);
		\draw[thick,->] (s1_3.110) arc (240:-65:\r mm);
		
		\node [state, right = \s cm of s1_3] (s2_1) {};
		\node [state, right = \d cm of s2_1] (s2_2) {};
		\node [state, right = \d cm of s2_2] (s2_3) {};
		\node [above = \h cm of s2_2] {$n-o+b$};
		
		\path [line] (s2_1) -> (s2_2);
		\path [line] (s2_2) -> (s2_3);
		\draw[thick,->] (s2_1.110) arc (240:-65:\r mm);
		\draw[thick,->] (s2_2.110) arc (240:-65:\r mm);
		\draw[thick,->] (s2_3.110) arc (240:-65:\r mm);
		
		\node [state, right = \s cm of s2_3] (s3_1) {};
		\node [state, right = \d cm of s3_1] (s3_2) {};
		\node [state, right = \d cm of s3_2] (s3_3) {};
		\node [above = \h cm of s3_2] {$w-o+z$};
		
		\path [line] (s3_1) -> (s3_2);
		\path [line] (s3_2) -> (s3_3);
		\draw[thick,->] (s3_1.110) arc (240:-65:\r mm);
		\draw[thick,->] (s3_2.110) arc (240:-65:\r mm);
		\draw[thick,->] (s3_3.110) arc (240:-65:\r mm);
		
		
		\node [state, below = \hl cm of s2_1] (s1) {}; \node [below = \hs cm of s1] {$s1$};
		\node [state, right = \d cm of s1] (s2) {}; \node [below = \hs cm of s2] {$s2$};
		\node [state, right = \d cm of s2] (s3) {}; \node [below = \hs cm of s3] {$s3$};
		
		\draw[thick,->,shorten >=1pt] (s1_1) to [out=-60,in=130] (s1);
		\draw[thick,->,shorten >=1pt] (s1_2) to [out=-60,in=130] (s2);
		\draw[thick,->,shorten >=1pt] (s1_3) to [out=-60,in=130] (s3);
		
		\draw[thick,->,shorten >=1pt] (s2_1) to [out=-90,in=90] (s1);
		\draw[thick,->,shorten >=1pt] (s2_2) to [out=-90,in=90] (s2);
		\draw[thick,->,shorten >=1pt] (s2_3) to [out=-90,in=90] (s3);
		
		\draw[thick,->,shorten >=1pt] (s3_1) to [out=-120,in=50] (s1);
		\draw[thick,->,shorten >=1pt] (s3_2) to [out=-120,in=50] (s2);
		\draw[thick,->,shorten >=1pt] (s3_3) to [out=-120,in=50] (s3);
		
		//-------------------  KROK 2 ---------------------
		
		\node [state] (s1_1) at (0, \yb) {};
		\node [state, right = \d cm of s1_1] (s1_2) {};
		\node [state, right = \d cm of s1_2] (s1_3) {};
		\node [above = \h cm of s1_2] {$a-o+b$};
		
		\path [line] (s1_1) -> (s1_2);
		\path [line] (s1_2) -> (s1_3);
		\draw[thick,->] (s1_1.110) arc (240:-65:\r mm);
		\draw[thick,->] (s1_2.110) arc (240:-65:\r mm);
		\draw[thick,->] (s1_3.110) arc (240:-65:\r mm);
		
		\node [state, right = \s cm of s1_3] (s2_1) {};
		\node [state, right = \d cm of s2_1] (s2_2) {};
		\node [state, right = \d cm of s2_2] (s2_3) {};
		\node [above = \h cm of s2_2] {$n-o+z$};
		
		\path [line] (s2_1) -> (s2_2);
		\path [line] (s2_2) -> (s2_3);
		\draw[thick,->] (s2_1.110) arc (240:-65:\r mm);
		\draw[thick,->] (s2_2.110) arc (240:-65:\r mm);
		\draw[thick,->] (s2_3.110) arc (240:-65:\r mm);
		
		\node [state, right = \s cm of s2_3] (s3_1) {};
		\node [state, right = \d cm of s3_1] (s3_2) {};
		\node [state, right = \d cm of s3_2] (s3_3) {};
		\node [above = \h cm of s3_2] {$w-o+z$};
		
		\path [line] (s3_1) -> (s3_2);
		\path [line] (s3_2) -> (s3_3);
		\draw[thick,->] (s3_1.110) arc (240:-65:\r mm);
		\draw[thick,->] (s3_2.110) arc (240:-65:\r mm);
		\draw[thick,->] (s3_3.110) arc (240:-65:\r mm);
		
		
		\node [state, below = \hl cm of s2_1] (s1) {}; \node [below = \hs cm of s1] {$s1$};
		\node [state, right = \d cm of s1] (s2) {}; \node [below = \hs cm of s2] {$s2$};
		\node [state, right = \d cm of s2] (s3) {}; \node [below = \hs cm of s3] {$s3$};
		\node [state, left = \d cm of s1] (s0) {}; \node [below = \hs cm of s0] {$s0$};
		
		\draw[thick,->,shorten >=1pt] (s1_1) to [out=-60,in=130] (s0);
		\draw[thick,->,shorten >=1pt] (s1_2) to [out=-60,in=130] (s2);
		\draw[thick,->,shorten >=1pt] (s1_3) to [out=-60,in=130] (s3);
		
		\draw[thick,->,shorten >=1pt] (s2_1) to [out=-90,in=90] (s1);
		\draw[thick,->,shorten >=1pt] (s2_2) to [out=-90,in=90] (s2);
		\draw[thick,->,shorten >=1pt] (s2_3) to [out=-90,in=90] (s3);
		
		\draw[thick,->,shorten >=1pt] (s3_1) to [out=-120,in=50] (s0);
		\draw[thick,->,shorten >=1pt] (s3_2) to [out=-120,in=50] (s2);
		\draw[thick,->,shorten >=1pt] (s3_3) to [out=-120,in=50] (s3);
		
		//-------------------  KROK 3 ---------------------
		
		\node [state] (s1_1) at (0, \yc) {};
		\node [state, right = \d cm of s1_1] (s1_2) {};
		\node [state, right = \d cm of s1_2] (s1_3) {};
		\node [above = \h cm of s1_2] {$a-o+b$};
		
		\path [line] (s1_1) -> (s1_2);
		\path [line] (s1_2) -> (s1_3);
		\draw[thick,->] (s1_1.110) arc (240:-65:\r mm);
		\draw[thick,->] (s1_2.110) arc (240:-65:\r mm);
		\draw[thick,->] (s1_3.110) arc (240:-65:\r mm);
		
		\node [state, right = \s cm of s1_3] (s2_1) {};
		\node [state, right = \d cm of s2_1] (s2_2) {};
		\node [state, right = \d cm of s2_2] (s2_3) {};
		\node [above = \h cm of s2_2] {$n-o+z$};
		
		\path [line] (s2_1) -> (s2_2);
		\path [line] (s2_2) -> (s2_3);
		\draw[thick,->] (s2_1.110) arc (240:-65:\r mm);
		\draw[thick,->] (s2_2.110) arc (240:-65:\r mm);
		\draw[thick,->] (s2_3.110) arc (240:-65:\r mm);
		
		\node [state, right = \s cm of s2_3] (s3_1) {};
		\node [state, right = \d cm of s3_1] (s3_2) {};
		\node [state, right = \d cm of s3_2] (s3_3) {};
		\node [above = \h cm of s3_2] {$w-o+z$};
		
		\path [line] (s3_1) -> (s3_2);
		\path [line] (s3_2) -> (s3_3);
		\draw[thick,->] (s3_1.110) arc (240:-65:\r mm);
		\draw[thick,->] (s3_2.110) arc (240:-65:\r mm);
		\draw[thick,->] (s3_3.110) arc (240:-65:\r mm);
		
		
		\node [state, below = \hl cm of s2_1] (s1) {}; \node [below = \hs cm of s1] {$s1$};
		\node [state, right = \d cm of s1] (s2) {}; \node [below = \hs cm of s2] {$s2$};
		\node [state, right = \d cm of s2] (s3) {}; \node [below = \hs cm of s3] {$s3$};
		\node [state, left = \d cm of s1] (s0) {}; \node [below = \hs cm of s0] {$s0$};
		\node [state, right = \d cm of s3] (s4) {}; \node [below = \hs cm of s4] {$s4$};
		
		\draw[thick,->,shorten >=1pt] (s1_1) to [out=-60,in=130] (s0);
		\draw[thick,->,shorten >=1pt] (s1_2) to [out=-60,in=130] (s2);
		\draw[thick,->,shorten >=1pt] (s1_3) to [out=-60,in=130] (s3);
		
		\draw[thick,->,shorten >=1pt] (s2_1) to [out=-90,in=90] (s1);
		\draw[thick,->,shorten >=1pt] (s2_2) to [out=-90,in=90] (s2);
		\draw[thick,->,shorten >=1pt] (s2_3) to [out=-90,in=90] (s4);
		
		\draw[thick,->,shorten >=1pt] (s3_1) to [out=-120,in=50] (s0);
		\draw[thick,->,shorten >=1pt] (s3_2) to [out=-120,in=50] (s2);
		\draw[thick,->,shorten >=1pt] (s3_3) to [out=-120,in=50] (s4);
		
		\end{scope}
		
		\end{tikzpicture}
		\caption{Tree-based clustering - dzielenie stanów}
		\label{fig:trr-based_clustering_example}
		
	\end{figure}


\chapter{Eksperymenty}
\label{sec:experiments}


	\begin{figure}
	\begin{tabular}{|l|c|c|c|c|} \hline
		
		Nazwa modelu & \vtop{\hbox{\strut Min liczba}\hbox{\strut obserwacji}} &
		\vtop{\hbox{\strut Próg}\hbox{\strut poprawy}}& Liczba stanów & Skuteczność \\ \hline
		
		M\_TRI\_AOPT   & 100   & 350 & 4192 & 83.44 \\
		M\_TRI         & 100   & 350 & 4192 & 81.58 \\
		M\_25000       & 25000 & 350 & 304  & 76.90 \\
		M\_29000       & 29000 & 350 & 269  & 77.75 \\
		M\_33000       & 33000 & 350 & 231  & 77.23 \\
		M\_37000       & 37000 & 350 & 214  & 77.13 \\
		M\_70000       & 70000 & 350 & 147  & 75.91 \\
		M\_1800000     & 18000 & 350 & 122  & 75.17 \\
		M\_UNI         &  N/A  & N/A & 120  & 76.86 \\
		\hline
	\end{tabular}
	\label{tab:acustic_models}
	\caption{Wytrenowane modele akustyczne}
	\end{figure}

	Wiele badań i publikacji pokazało, że stosując \textit{tri-fonowe modele akustyczne} można znacząco poprawić skuteczność rozpoznawania. Niestety przejście z modeli \textit{uni-fonowych} do \textit{tri-fonowych} wiąże się ze zwiększeniem stopnia skomplikowania całego systemu, procesu uczenia, jak i wydłużeniem samego czasu rozpoznawania. Ponadto, w przypadku zastosowania sieci neuronowej, duża liczba stanów modelu tri-fonowego wymaga jednoczesnej estymacji wielu prawdopodobieństw, co z kolei może utrudnić proces trenowania sieć. Kompromisem pomiędzy modelami czysto tri-fonowymi, które modelują wszystkie występujące trifony, a uni-fonowymi oferującymi przeciętną jakość rozpoznawania, są właśnie \textit{modele tri-fonowe z ograniczonym kontekstem} budowane zgodnie z koncepcją opisane w rozdziale \ref{sec:state_red}. Modele te łączą zwiększoną zdolność gromadzenia wiedzy klasycznych modeli tri-fonowych ze zmniejszoną liczbą parametrów, które trzeba wyuczyć w klasycznym modelu uni-fonowych. Ponadto w przeciągu kilku ostatnich lat, pojawiło się wiele publikacji na temat zastosowania sieci neuronowych, jako estymatorów prawdopodobieństwa stanów model Markowa >> DODAC CYTATY <<. Chcą sprawdzić możliwości połączenia obu wspomnianych podejść, \textit{modeli z ograniczonym kontekstem} oraz \textit{sieci neuronowych} estymujących prawdopodobieństwa stanów, zgodnie z opisem w rozdziale \ref{sec:ASR_NN}, przeprowadzono szereg eksperymentów, mających sprawdzić wpływ ograniczania kontekstu na skuteczność sieci neuronowej. Dodatkowo porównano skuteczność klasycznego podejścia opisanego w rozdziale \ref{sec:ASR_HMM} z proponowanym podejściem. Przeprowadzone eksperymenty dały możliwość zweryfikowania głównej tezy niniejszej pracy, zgodnie z która: \textbf{Trifonowe, neuronowe modele o ograniczonym kontekście umożliwiają poprawę skuteczności rozpoznawania, względem klasycznych Gaussowkich modeli akustycznych.}
	
	\section{ Opis danych}
	\label{sec:opis_danych}
	
	W celu przetestowania proponowanych metod wykorzystano studyjny korpus Clarin\footnote{http://mowa.clarin-pl.eu/korpusy/}. Składa się on z 56 godzin polskich nagrań o różnej tematyce, nagranych przez 554 różnych mówców. Każdy mówca nagrał 20 lub 30 wypowiedzi o długości od 6 do 22 sekund. Korpus nie jest zbalansowany pod względem płci. Wypowiedzi mają przypisane transkrypcje, jednak znajduje się w nich wiele błędów i artefaktów, takich jak zająknięcia (4. wypowiedz 191. mówcy), seplenienie (200. mówca) czy błędne odczytanie tekstu (18. wypowiedz 294. mówcy) skutkujące różnicą pomiędzy transkrypcją a faktycznie wypowiedzianymi słowami. Artefakty utrudniają lub wręcz uniemożliwiają wykorzystanie niektórych nagrań do uczenia modelu akustycznego. Dźwięk został nagrany z próbkowaniem 16 kHz i jakością 16 bitów na próbkę, a następnie zapisany w niekompresowanym formacie wav. Wypowiedzi zaczynają się i kończą ciszą. 
	\\
	W fazie wstępnego przetwarzania danych, nagrania zostały poddane następującym procesom:
	\begin{itemize}
		\item automatyczne wyrównanie poziomu nagrań to stałego poziomu -6dB (1/2 maksymalnego możliwego do osiągnięcia poziomu) dla najgłośniejszych miejsc całego nagrania
		\item odrzucenie fragmentów ciszy/szumów tła o długości przekraczającej 0.7 sek,
		\item odrzucenie tych nagrań dla których faktycznie wypowiedziana fraza różniła się od frazy zadeklarowanej w dostarczonej transkrypcji.
	\end{itemize}
	W celu wykonania ostatniego kroku, służącego automatycznemu oczyszczeniu nagrań, zbudowano model akustyczny i językowy na wszystkich nagraniach z korpusu Clarin. Tak otrzymane modele dają bardzo wysoką skuteczność rozpoznawania zbioru treningowego na podstawie którego je zbudowano. Następnie wykorzystując zbudowany model, rozpoznano nagrania z korpusu. Wypowiedzi o idealnym rozpoznaniu uznano za poprawne, natomiast te o niepoprawnym odrzucono. Odrzucone nagrania nie były wykorzystywane do budowy żadnych późniejszych modeli. 
	Ostatecznie, oczyszczony korpus został podzielony na trzy zbiory: treningowy, walidacyjny oraz testowy w stosunku 27:2:1. Zbiory są rozłączne pod względem mówców. 
	
	\section{Środowisko sprzętowo-programisytyczne }
	\label{sec:env}
	
	W celu przeprowadzenia eksperymentów wykorzystany został szereg ogólnodostępnych pakietów i bibliotek oraz komercyjny system do rozpoznawania mowy \textit{Magic Scribe} firmy \textit{Unikkon} z Wrocławia. W skład ogólnodostępnych, wykorzystanych pakietów wchodzi \textit{HTK} (\url{http://htk.eng.cam.ac.uk/}) oferujący zestaw narzędzi do budowy klasycznych modeli akustycznych oraz językowych. Kolejnym wykorzystanym narzędziem jest \textit{Julius} (\url{http://julius.osdn.jp/en_index.php}) będący dekoderem w pełni kompatybilnym z pakietem HTK. Julius jest jednym z kilku open-sourcowych dekoderów. Pomimo, że nie oferuje on najwyższej skuteczności rozpoznawani, o czym można przeczytać w artykule \cite{asr_toolkit_cmp}, zdecydowałem się na to narzędzie ponieważ mam doświadczenie w pracy z nim. Ponadto Julius uchodzi za szybki dekoder, działający w czasie zbliżonym do rzeczywistego, co też jest istotne z punktu widzenia użytkownika systemu rozpoznającego mowę. W artykule \cite{julius} twórcy \textit{juliusa} opisują swój system oraz zastosowane algorytmy i struktury danych umożliwiające osiągnięcie wysokiej szybkości rozpoznawania. Do budowy sieci neuronowych wykorzystano bibliotekę \textit{Lasagne} \url{http://lasagne.readthedocs.io/en/latest/}, napisaną w języku Python. Zapewnia ona wygodny interfejs do środowiska \textit{Theano}. Lasagne oferuje zestaw metod do budowy sieci składających się z warstw klasycznych, splotowych, dropout, max-pull itd. \textit{Theano} (\url{http://deeplearning.net/software/theano/}) jest z kolei biblioteką umożliwiającą wykonywanie współbieżnych obliczeń na karcie graficznej. Z wykorzystaniem tej biblioteki oraz GPU udało się znacząco przyśpieszyć trening sieci neuronowych w stosunku do treningu na CPU. Schemat zależności pomiędzy użytymi komponentami oraz budowanymi w trakcie eksperymentów modelami, znajduje się na rysunku \ref{fig:arch}. 
	
	Punktem wyjścia stworzonej architektury jest korpus językowy \textit{Clarin} przedstawiony w rozdziale \ref{sec:opis_danych}, na schemacie \ref{fig:arch} odpowiada mu blok \refBlock{clarin_wav}. Na podstawie transkrypcji zawartych w korpusie, zbudowano model językowy wraz ze słownikiem \refBlock{lang_model_clarin}, wykorzystując do tego pakiet HTK oraz system Magic Scribe z bloku \refBlock{htk_ms}. Do budowy modelu językowego i słownika \refBlock{lang_model_clarin} wykorzystano wszystkie wypowiedzi ze zbioru treningowego, walidacyjnego i testowego. Tak powstały model językowy został wykorzystany wraz z zewnętrznym modelem akustycznym \refBlock{model_acc_ms} do rozpoznania wypowiedzi z korpusu \refBlock{clarin_wav} i stworzenia plików \textit{mfc} oraz \textit{out} tworzących korpus \refBlock{clarin_mfc}. Każdej z oryginalnych wypowiedzi znajdujących się w korpusie \refBlock{clarin_wav} przypisano w zbiorze \refBlock{clarin_mfc}, plik \textit{mfc} zawierający kolejne wektory cech oraz plik \textit{out} z zapisanym mapowaniem tri-fonu wraz z numerem stanu na ramkę. Pliki \textit{out} wygenerował dekoder \refBlock{julius} podczas procesu rozpoznawania. Dzięki dobremu zewnętrznemu  modelowi akustycznemu \refBlock{model_acc_ms} oraz modelowi językowemu \refBlock{lang_model_clarin} uzyskano bardzo wysoką skuteczność rozpoznawania wynoszącą $99.3\%$, dzięki czemu dopasowanie ramek do stanów w korpusie \refBlock{clarin_mfc} jest bardzo precyzyjne. Wysoka skuteczność jest w znacznej mierze efektem budowy słownika \refBlock{lang_model_clarin} na zbiorze, który został nim później rozpoznany. Słownik z modelu \refBlock{lang_model_clarin} zawiera jedynie słowa, które mają się pojawić w wyniku rozpoznawania. Na podstawie korpusu \refBlock{clarin_mfc} przygotowano narzędziem \refBlock{data_processor} dane do treningu sieci neuronowej. Efektem działania narzędzia \refBlock{data_processor} jest plik z mapowaniem \textit{wektor cech} $\longrightarrow$ \textit{stan modelu Markowa}. Podczas tego etapu dokonana została normalizacja wektorów cech dla każdej frazy zgodnie z opisem w rozdziale \ref{sec:Feature_vec_mfcc}. Wykorzystując stworzone mapowanie oraz wspomnianą powyżej bibliotekę \textit{Lasagne} i \textit{Theano} w kroku \refBlock{net_build} wytrenowano szereg sieci neuronowych \refBlock{net}. Ostatecznie uruchomiono rozpoznawanie \textit{zbioru testowego} dekoderem \refBlock{julius3} korzystającym z sieci neuronowych \refBlock{net}. W efekcie otrzymano skuteczności rozpoznawania \refBlock{results2}. Należy dodać, że dekoder \refBlock{julius3} jest oryginalnym dekoderem \textit{Julius} rozbudowanym o własnoręcznie dopisany moduł pobierający prawdopodobieństwa stanów modelu Markowa z sieci neuronowej, zamiast z klasycznego modelu Gaussowskiego. Równocześnie korzystając z pakietu HTK i systemu Magic Scribe (\refBlock{htk_ms}) wyuczono szereg klasycznych modeli akustycznych \refBlock{model_acc4}, modele te zostały wytrenowane jedynie na \textit{zbiorze treningowym} z korpusu \refBlock{clarin_wav}. Następnie wykorzystując te modele dokonano testów skuteczności rozpoznawania \textit{zbioru testowego} z korpusu \refBlock{clarin_wav} dekoderem \refBlock{julius2}. W efekcie otrzymano skuteczność \refBlock{results1}.
	
	Jako miarę skuteczności zastosowano metrykę \textit{WER} (\textit{Word Error Rate}), zdefiniowaną wzorem \ref{eqn:wer}, gdzie $S$ jest liczbą zamienionych słów, $D$ liczbą usuniętych słów, $I$ liczbą dodanych słów, natomiast $N$ całkowita liczbą słów z referencyjnego zdania. Jest to metryka analogiczna do \textit{odległości Levenshteina}, ale jest zdefiniowana na poziomie całych słów, a nie liter.
	\begin{equation}
		WER = 1 - \frac{S+D+I}{N}
		\label{eqn:wer}
	\end{equation}
	
	
	\begin{figure}[H]
		\centering
		\hspace*{-1.7cm}
		\begin{tikzpicture}
			
			\begin{scope}
			\tikzstyle{arch_part} = [rectangle, draw, text width=8em, text centered, rounded corners, minimum height=3em]
			
			\def\l{-2.5}
			\def\r{6.7}
			\def\c{2}
			\def\x{10.5}
			\def\j{4.0}
			
			\draw  [rounded corners, text centered] (0,0) rectangle (4,1.2);
			
			\path [line, dashed] (1.0, 0.75) -- (1.0, 1.5) node[above, xshift=-10pt] {Zbiór treningowy};
			\path [line, dashed] (2.5, 0.75) -- (2.5, 2.0) node[above, xshift=-20pt] {Zbiór testowy};
			\path [line, dashed] (3.5, 0.75) -- (3.5, 2.5) node[above, xshift=-20pt] {Zbiór walidacyjny};
			
			\draw [gray!80, dashed] (2,0.4) -- (2,1.2)  node [black,below, yshift=-20pt] {\labelBlock{clarin_wav} CLARIN : wav, txt};
			\draw [gray!80, dashed] (3,0.4) -- (3,1.2)  node [black,midway,below] {};
			
			\node [arch_part] (htk) at (\l, 0.6) {\labelBlock{htk_ms}HTK, Magic Scribe};
			\path [line] (0,0.6) -- (htk);
			
			\node [arch_part] (model_lang) at (\l, -3) {\labelBlock{lang_model_clarin}Model językowy ze slownikiem};
			\path [line] (htk) -- (model_lang);
			
			\node [arch_part] (model_acc) at (\c, 5) {\labelBlock{model_acc4}Model akustyczny};
			\path [line] (htk) |- (model_acc);
			
			\node [arch_part] (julius2) at (\r, 2.5) {\labelBlock{julius2}Julius};
			\path [line] (4,0.6) -| (julius2);
			\path [line] (model_acc) -| (julius2);
			
			\node [arch_part] (results) at (\x, 2.5) {\labelBlock{results1}Skuteczność modelu klasycznego};
			\path [line] (julius2) -- (results);
						
			\node [arch_part] (julius) at (\c, -3) {\labelBlock{julius} Julius};
			\path [line] (model_lang) -- (julius);
			\path [line] (2, 0) -- (julius);
			\path [line] (julius) -- (2, -4.8);
			
			\draw  [rounded corners, text centered] (0,-6) rectangle (4,-4.8);
			\draw [gray!80, dashed] (2,-5.6) -- (2,-4.8)  node [black,below, yshift=-20pt] {\labelBlock{clarin_mfc}CLARIN : mfc, out};
			\draw [gray!80, dashed] (3,-5.6) -- (3,-4.8)  node [black,midway,below] {};
			
			\node [arch_part] (data_processor) at (\l, -5.4) {\labelBlock{data_processor}Przetwarzanie danych};
			\path [line] (0, -5.4) -- (data_processor);
			
			\node [arch_part] (siec_builder) at (\l, -7.5) {\labelBlock{net_build}Budowa sieci};
			\path [line] (data_processor) -- (siec_builder);
			
			\node [arch_part] (siec) at (\c, -7.5) {\labelBlock{net}Sieć neuronowa};
			\path [line] (siec_builder) -- (siec);
			
			\node [arch_part] (julius3) at (\r, -3) {\labelBlock{julius3}Julius + modul sieci neuronowej};
			\path [line] (siec) -| (julius3);
			\path [line] (4,0.6) -| (julius3);
			
			\node [arch_part] (model_acc2) at (\x, -3) {\labelBlock{results2}Skuteczność modelu neuronowego};
			\path [line] (julius3) -- (model_acc2);
			
			\node [arch_part] (model_lang2) at (\x, 0.6) {\labelBlock{lang_model2}Zewnętrzny model językowy};
			\path [line] (model_lang2) -| (7.5, 1.9);
			\path [line] (model_lang2) -| (7.5, -2.2);
			
			\node [arch_part] (model_acc3) at (4.5, -1.0) {\labelBlock{model_acc_ms}Zewnętrzny model akustyczny};
			\path [line] (model_acc3) -| (2.5, -2.4);
			
			
			\end{scope}
		
		\end{tikzpicture}
		\caption{Zależności pomiędzy wykorzystanymi komponentami.}
		\label{fig:arch}	
	\end{figure}

	\section{Budowa modeli klasycznych}
		Klasyczne modele Gaussowkie, zaznaczone w bloku \refBlock{model_acc4} na schemacie \ref{fig:arch}, zostały zbudowane z wykorzystaniem pakietu \textit{HTK} oraz opakowującego go systemu \textit{Magic Scribe}. Do treningu użyto zbioru treningowego z korpusu \textit{Clarin}. Model akustyczny uczony był zgodnie z zaleceniami opisanymi w \textit{HTK Book}\cite{htkbook}, procedurę uczenia zapisano w pseudokodzie \ref{alg:acc_model_training}. Pierwszym krokiem procedury jest zainicjalizowanie parametrów prostego modelu uni-fonowego, w którym mikstury gaussowskie, zgodne z opisem w rozdziale \ref{sec:gmm_estimation}, składają się z jednego komponentu. Następnie w pierwszej pętli wykonuje się po trzy iteracje re-estymacji parametrów, która przebiega zgodnie z algorytmem Bauma-Welcha opisanym w rozdziale \ref{sec:Baum_Welch} oraz \ref{sec:gmm_estimation}. Podczas re-estymacji przechodzi się przez wszystkie dane treningowe ze zbioru $S$. Podczas przechodzenia wyliczany jest \textit{Force alignment}, a następnie na jego podstawie estymowane są nowe parametry. \textit{Force alignment} polega na dopasowaniu ramek do zadanego ciągu stanów. Ciąg ten odpowiada transkrypcji, którą dostaje się podczas treningu razem z nagraniem. W praktyce \textit{Force alignment} polega na wymuszeniu przejścia, w fazie E algorytmu \textit{Buma-Welcha}, przez automat odpowiadający transkrypcji nagrania. Schemat \ref{fig:AutomatExampleForFA} pokazuje przykład automatu dla zdania \textit{Ola ma kota} wymuszającego przejście przez odpowiednie stany. Automat ten jest jednym łańcuchem i w przeciwieństwie do automatu ze schematu \ref{fig:Graph_complex}, wykorzystywanego przez dekoder, uniemożliwia swobodny wybór kolejnych słów. Kolejne re-estymacje powodują zbieganie modelu do wytrenowanej postaci. Po trzech iteracjach, kiedy model ma już pewną wiedzę, rozbudowuje się mikstury Gaussowskie o nowe komponenty. Po zakończeniu pierwszej pętli następuje przekształcenie modelu uni-fonowego w tri-fonowy. Początkowo tri-fony o wspólnym centralnym fonemie współdzielą te same stany. Dopiero w następnym kroku wykonuje się algorytm \textit{Tree-based clustering} z rozdziału \ref{sec:state_red}, w wyniku którego zwiększa się liczba stanów modelu akustycznego. W kolejnych krokach ponownie re-estymuje się parametry i rozbudowuje mikstury Gaussowskie. Łącznie wykonuje się $36$ iteracji re-estymacji parametrów. Na koniec zwracany jest wytrenowany model. Wszystkie modele tri-fonowe z tabeli \ref{tab:acustic_models} zostały zbudowane zgodnie z procedurą \ref{alg:acc_model_training}. Wyjątek stanowi \textit{M\_UNI}, które jest modelem unifonowym, budowanym według uproszczonej procedury. \textit{M\_TRI} oraz \textit{M\_TRI\_AOPT} to te same modele, ale $M\_TRI\_AOPT$ odpowiada wynikowi otrzymanemu dla najlepszych, znalezionych parametrów dekodowania, które wynoszą 20 -15 20 -15. Opis parametrów dekodowania znajduje się w rozdziale \ref{sec:meta_params}.
		
		\begin{figure}[H]
			\centering
			\begin{tabular}{|c|}
				\hline
				
				\begin{tikzpicture}[node distance=1.7cm]
				
				\begin{scope}
				
				\def\x{0.65}
				\def\y{1.0}
				\def\z{1.5}
				
				\node [hmm0, left = \x cm of hmm1] (start) {};
				\node [below] at (start.south) {start};
				
				\node [hmm0] (hmm1) {};
				\node [below] at (hmm1.south) {$o_1$};
				
				\node [hmm0, right = \x cm of hmm1] (hmm2) {};
				\node [below] at (hmm2.south) {$o_2$};
				
				\node [hmm0, right = \x cm of hmm2] (hmm3) {};
				\node [below] at (hmm3.south) {$o_3$};
				
				
				\node [hmm0, right = \y cm of hmm3] (hmm4) {};
				\node [below] at (hmm4.south) {$l_1$};
				
				\node [hmm0, right = \x cm of hmm4] (hmm5) {};
				\node [below] at (hmm5.south) {$l_2$};
				
				\node [hmm0, right = \x cm of hmm5] (hmm6) {};
				\node [below] at (hmm6.south) {$l_3$};
				
				
				\node [hmm0, right = \y cm of hmm6] (hmm7) {};
				\node [below] at (hmm7.south) {$a_1$};
				
				\node [hmm0, right = \x cm of hmm7] (hmm8) {};
				\node [below] at (hmm8.south) {$a_2$};
				
				\node [hmm0, right = \x cm of hmm8] (hmm9) {};
				\node [below] at (hmm9.south) {$a_3$};
				
				
				
				
				\node [hmm0, below = \z cm of hmm9] (hmm10) {};
				\node [below] at (hmm10.south) {$m_1$};
				
				\node [hmm0, left = \x cm of hmm10] (hmm11) {};
				\node [below] at (hmm11.south) {$m_2$};
				
				\node [hmm0, left = \x cm of hmm11] (hmm12) {};
				\node [below] at (hmm12.south) {$m_3$};
				
				
				\node [hmm0, left = \y cm of hmm12] (hmm13) {};
				\node [below] at (hmm13.south) {$a_1$};
				
				\node [hmm0, left = \x cm of hmm13] (hmm14) {};
				\node [below] at (hmm14.south) {$a_2$};
				
				\node [hmm0, left = \x cm of hmm14] (hmm15) {};
				\node [below] at (hmm15.south) {$a_3$};
				
				
				\node [hmm0, left = \y cm of hmm15] (hmm16) {};
				\node [below] at (hmm16.south) {$k_1$};
				
				\node [hmm0, left = \x cm of hmm16] (hmm17) {};
				\node [below] at (hmm17.south) {$k_2$};
				
				\node [hmm0, left = \x cm of hmm17] (hmm18) {};
				\node [below] at (hmm18.south) {$k_3$};
				
				
				
				
				\node [hmm0, below = \z cm of hmm18] (hmm19) {};
				\node [below] at (hmm19.south) {$o_1$};
				
				\node [hmm0, right = \x cm of hmm19] (hmm20) {};
				\node [below] at (hmm20.south) {$o_2$};
				
				\node [hmm0, right = \x cm of hmm20] (hmm21) {};
				\node [below] at (hmm21.south) {$o_3$};
				
				
				\node [hmm0, right = \y cm of hmm21] (hmm22) {};
				\node [below] at (hmm22.south) {$t_1$};
				
				\node [hmm0, right = \x cm of hmm22] (hmm23) {};
				\node [below] at (hmm23.south) {$t_2$};
				
				\node [hmm0, right = \x cm of hmm23] (hmm24) {};
				\node [below] at (hmm24.south) {$t_3$};
				
				
				\node [hmm0, right = \y cm of hmm24] (hmm25) {};
				\node [below] at (hmm25.south) {$a_1$};
				
				\node [hmm0, right = \x cm of hmm25] (hmm26) {};
				\node [below] at (hmm26.south) {$a_2$};
				
				\node [hmm0, right = \x cm of hmm26] (hmm27) {};
				\node [below] at (hmm27.south) {$a_3$};
				
				\node [hmm0, right = \x cm of hmm27] (stop) {};
				\node [below] at (stop.south) {stop};
				
				
				
				
				\draw[thick,->,shorten >=1pt] (hmm1) to [out=0,in=180] (hmm2);
				\draw[thick,->,shorten >=1pt] (hmm2) to [out=0,in=180] (hmm3);
				
				\draw[thick,->,shorten >=1pt] (hmm3) to [out=0,in=180] (hmm4);
				
				\draw[thick,->,shorten >=1pt] (hmm4) to [out=0,in=180] (hmm5);
				\draw[thick,->,shorten >=1pt] (hmm5) to [out=0,in=180] (hmm6);
				
				\draw[thick,->,shorten >=1pt] (hmm6) to [out=0,in=180] (hmm7);
				
				\draw[thick,->,shorten >=1pt] (hmm7) to [out=0,in=180] (hmm8);
				\draw[thick,->,shorten >=1pt] (hmm8) to [out=0,in=180] (hmm9);
				
				\draw[thick,->,shorten >=1pt] (hmm9) to [out=0,in=0,looseness=1.1] (hmm10);
				
				\draw[thick,->,shorten >=1pt] (hmm10) to [out=180,in=0] (hmm11);
				\draw[thick,->,shorten >=1pt] (hmm11) to [out=180,in=0] (hmm12);
				
				\draw[thick,->,shorten >=1pt] (hmm12) to [out=180,in=0] (hmm13);
				
				\draw[thick,->,shorten >=1pt] (hmm13) to [out=180,in=0] (hmm14);
				\draw[thick,->,shorten >=1pt] (hmm14) to [out=180,in=0] (hmm15);
				
				\draw[thick,->,shorten >=1pt] (hmm15) to [out=180,in=0] (hmm16);
				
				\draw[thick,->,shorten >=1pt] (hmm16) to [out=180,in=0] (hmm17);
				\draw[thick,->,shorten >=1pt] (hmm17) to [out=180,in=0] (hmm18);
				
				\draw[thick,->,shorten >=1pt] (hmm18) to [out=180,in=180,looseness=1.1] (hmm19);
				
				\draw[thick,->,shorten >=1pt] (hmm19) to [out=0,in=180] (hmm20);
				\draw[thick,->,shorten >=1pt] (hmm20) to [out=0,in=180] (hmm21);
				
				\draw[thick,->,shorten >=1pt] (hmm21) to [out=0,in=180] (hmm22);
				
				\draw[thick,->,shorten >=1pt] (hmm22) to [out=0,in=180] (hmm23);
				\draw[thick,->,shorten >=1pt] (hmm23) to [out=0,in=180] (hmm24);
				
				\draw[thick,->,shorten >=1pt] (hmm24) to [out=0,in=180] (hmm25);
				
				\draw[thick,->,shorten >=1pt] (hmm25) to [out=0,in=180] (hmm26);
				\draw[thick,->,shorten >=1pt] (hmm26) to [out=0,in=180] (hmm27);
				
				
				
				\draw[thick,->] (hmm1.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm2.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm3.70) arc (-60:245:4mm);
				
				\draw[thick,->] (hmm4.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm5.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm6.70) arc (-60:245:4mm);
				
				\draw[thick,->] (hmm7.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm8.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm9.70) arc (-60:245:4mm);
				
				
				
				\draw[thick,->] (hmm10.110) arc (240:-65:4mm);
				\draw[thick,->] (hmm11.110) arc (240:-65:4mm);
				\draw[thick,->] (hmm12.110) arc (240:-65:4mm);
				
				\draw[thick,->] (hmm13.110) arc (240:-65:4mm);
				\draw[thick,->] (hmm14.110) arc (240:-65:4mm);
				\draw[thick,->] (hmm15.110) arc (240:-65:4mm);
				
				\draw[thick,->] (hmm16.110) arc (240:-65:4mm);
				\draw[thick,->] (hmm17.110) arc (240:-65:4mm);
				\draw[thick,->] (hmm18.110) arc (240:-65:4mm);
				
				
				\draw[thick,->] (hmm19.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm20.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm21.70) arc (-60:245:4mm);
				
				\draw[thick,->] (hmm22.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm23.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm24.70) arc (-60:245:4mm);
				
				\draw[thick,->] (hmm25.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm26.70) arc (-60:245:4mm);
				\draw[thick,->] (hmm27.70) arc (-60:245:4mm);
				
				\draw[thick,->,shorten >=1pt] (start) to [out=0,in=180] (hmm1);
				\draw[thick,->,shorten >=1pt] (hmm27) to [out=0,in=180] (stop);
				\end{scope}			
				\end{tikzpicture} \\
				
				\hline
			\end{tabular}
			\caption{Automat dla wypowiedzi treningowej z transkrypcją \textit{Ola ma kota}, stworzony na potrzeby \textit{Force alignmentu}}
			\label{fig:AutomatExampleForFA}
			
		\end{figure}
		
		\begin{figure}
			\begin{algorithmic}[1]
				
				\REQUIRE {$S =$ zbiór par $(wypowiedz, transkrypcja)$}
				
				\STATE $M_{acc} = Init(S)$				
				\FOR{$i = 1 \text{ to }3$}
					\STATE $M_{acc} = ReEstimate(M_{acc}, S)$
					\STATE $M_{acc} = ReEstimate(M_{acc}, S)$
					\STATE $M_{acc} = ReEstimate(M_{acc}, S)$
					\STATE $M_{acc} = ExtendGMM(M_{acc})$
				\ENDFOR			
				\STATE $M_{acc} = CreateTriFones(M_{acc})$
				\STATE $M_{acc} = TreeBasedClustering(M_{acc})$
				\FOR{$i = 4 \text{ to }12$}
					\STATE $M_{acc} = ReEstimate(M_{acc}, S)$
					\STATE $M_{acc} = ReEstimate(M_{acc}, S)$
					\STATE $M_{acc} = ReEstimate(M_{acc}, S)$
					\STATE $M_{acc} = ExtendGMM(M_{acc})$
				\ENDFOR			
				\STATE $M_{acc} = ReEstimate(M_{acc}, S)$
				\STATE $M_{acc} = ReEstimate(M_{acc}, S)$
				\STATE $M_{acc} = ReEstimate(M_{acc}, S)$
				\RETURN $M_{acc}$;
			\end{algorithmic}		
			
			\caption{Procedura uczenia tri-fonowych modeli akustycznych.}
			\label{alg:acc_model_training}
		\end{figure}

	\section{ Szczegóły budowy modeli neuronowych}
		\label{sec:nn_model_details}
		Wspomniane w rozdziale \ref{sec:env} modele neuronowe \refBlock{net} ze schematu \ref{fig:arch} składają się z dwóch części. Pierwszą jest zbiór stanów $Q$ oraz macierz $A$, opisująca prawdopodobieństwa przejść pomiędzy stanami. Drugą częścią jest natomiast sama sieć neuronowa, odpowiadająca rodzinie funkcji $B$ wyznaczającej prawdopodobieństwa obserwacji. Każdy model neuronowy jest budowany w oparciu o wcześniej wytrenowany klasyczny model Gaussowski. Z klasycznego modelu brane sa, beż żadnych modyfikacji, stany $Q$ oraz macierz $A$. Następnie w oparciu o zbiór $Q$ definiuje się liczbę neuronów w wrastwie wyjściowej sieci. Sieć ma pełnić rolę estymatora prawdopodobieństw obserwacji, zgodnie z opisem w rozdziale \ref{sec:ASR_NN}. Podczas uczenia sieci, pokazuje się jej pary wartości \textit{wektor cech} $\longrightarrow$ \textit{stan modelu Markowa}, które tworzą zbiór treningowy sieci, odpowiadający blokowi \refBlock{clarin_mfc} ze schematu \ref{fig:arch}. Każdy model neuronowy musi mieć swój zbiór treningowy, gdyż jest on uzależniony od modelu klasycznego, na podstawie, którego uczona jest sieć. Wynika to z faktu, że każdy model Gaussowski ma inne mapowanie tri-fonów na stany, co definiuje zbiór treningowy sieci. Opis mapowania tri-fonów na stany oraz jego wpływ na model akustyczny został opisany w rozdziale \ref{sec:tri-fone_mapping}.
	
	\section{ Optymalizacja parametrów procesu dekodowania }
		\label{sec:metadata_impact}
		Celem pierwszego eksperymentu było znalezienie wartości \textit{wagi modelu językowego} oraz \textit{kary za wstawienie słowa}, które dają dobrą skuteczność rozpoznawania mowy. Wpływ tych parametrów na proces dekodowania został opisany w rozdziale \ref{sec:meta_params}. W trakcie eksperymentu sprawdzono skuteczność rozpoznawania na zbiorze testowym z korpusu \textit{Clarin} dla różnych wartości \textit{wagi} oraz \textit{kary}. Eksperymentowi poddano sieci neuronowe wykorzystane w testach z rozdziału \ref{sec:q-num_impact}. Wyniki eksperymentu zostały zamieszczone w tabeli \ref{tab:metadata_impact}. Bazując na otrzymanych rezultatach uznano parametry $12, -10, 15, -10$ za optymalne. Dały one najlepszą skuteczność w ujęciu wszystkich pomiarów, w szczególności dla sieci o $231$ oraz $147$ stanach.
		
			\begin{figure}
				\centering
				\vspace{-2.5cm}
				\begin{tabular}{|c|c|c|c|c|} \hline
					Nazwa modelu & Liczba stanów & \vtop{\hbox{\strut Parametry}\hbox{\strut pierwszego}\hbox{\strut przejścia}}  & 
					\vtop{\hbox{\strut Parametry}\hbox{\strut drugiego}\hbox{\strut przejścia}} & Skuteczność [\%] \\
				
					\hline
					\multirow{7}{*}{Model 240424} &
					\multirow{7}{*}{-}            &
					13 -8 & 13 -9 & 81.09 \\	
					&& \textbf{12 -7} & \textbf{15 -9} & \textbf{81.55}\\
					&& 12 -7 & 12 -7 & 81.32 \\
					&& 11 -7 & 11 -7 &  81.03 \\
					&& 10 -7 & 10 -7 & 80.72 \\
					&& 7 -8  & 7 -8  & 78.19 \\
					&& 5 -8  & 5 -8  & 73.37 \\
					\hline
					\multirow{8}{*}{Model 268154} &
					\multirow{7}{*}{122}            &
					\textbf{13 -8} & \textbf{13 -9} & \textbf{81.69} \\
					&& 12 -7 & 15 -9 & 81.48 \\
					&& 12 -7 & 12 -7 & 81.61 \\
					&& 11 -7 & 11 -7 & 81.50 \\
					&& 10 -7 & 10 -7 & 81.23 \\
					&& 7 -8  & 7 -8  & 78.47 \\
					&& 5 -8  & 5 -8  & 73.9  \\
					&& 18 -7 & 16 -12& 79.04 \\
					\hline					
					\multirow{7}{*}{Model 268151} &	
					\multirow{7}{*}{214}            &	  
					13 -8 & 13 -9 & 81.44 \\
					&& \textbf{12 -7}    & \textbf{15 -9} & \textbf{83.39} \\
					&& 12 -7.5 & 15 -9.5 & 83.33 \\
					&& 12 -7 16& -9.5    & 83.04 \\
					&& 12 -7 12& -7      & 81.09 \\ 
					&& 11 -7 11& -7      &  80.61 \\
					&& 18 -7 16& -12     &  79.76 \\
					\hline					
					\multirow{5}{*}{Model 268148} &	
					\multirow{7}{*}{304}            &	  
					13 -8 & 13 -9 & 54.30 \\
					&& \textbf{12 -7} & \textbf{15 -9} & \textbf{57.42} \\
					&& 12 -7 & 12 -7 & 52.58 \\
					&& 7 -8  & 7 -8  & 39.58 \\
					&& 18 -7 & 16 -12& 55.14 \\
					\hline
					\multirow{9}{*}{Model 268159} &
					\multirow{7}{*}{231}            &
					18 -7 & 16 -12 & 79.90 \\
					&& 12 -7 & 15 -9  & 83.37 \\
					&& 12 -7 & 15 -9  & 83.40 \\
					&& \textbf{12 -10}& \textbf{15 -10} & \textbf{83.48} \\
					&& 13 -10& 15 -10 & 83.37 \\
					&& 7 -8  & 7 -8   & 74.63 \\
					&& 12 -7 & 12 -7  & 80.68 \\
					&& 13 -11& 15 -11 & 83.34 \\
					&& 14 -12& 15 -12 & 82.19 \\
					\hline
					\multirow{7}{*}{Model 268158} &
					\multirow{7}{*}{147}            &
					18 -7 & 16 -12 & 79.64 \\
					&& 12 -7 & 15 -9  & 83.18 \\
					&& \textbf{12 -10}& \textbf{15 -10} & \textbf{83.24} \\
					&& 7 -8  & 7 -8   & 78.59 \\
					&& 11 -10& 11 -10 &  81.12 \\
					&& 11 -9 & 11 -9  &  81.32 \\
					&& 12 -9 & 12 -9  &  81.55 \\
					\hline					
					
				\end{tabular}
			\label{tab:metadata_impact}
			\caption{Wpływ parametrów dekodowania na skuteczność rozpoznawania }
		\end{figure}
		
	\section{Wpływ liczby stanów na skuteczność rozpoznawania }
		\label{sec:q-num_impact}
	
		W celu zweryfikowania głównej tezy niniejszej pracy przeprowadzono porównanie modeli o różnym stopniu zdławienia, dokonanym zgodnie z opisem w rozdziale \ref{sec:state_red}. Manipulując liczba obserwacji wymaganych do podziału tri-fonu zbudowano serię klasycznych modeli akustycznych o różnej liczbie stanów. Następnie dla każdego z nich wytrenowano sieć neuronową zgodnie z filozofią opisaną w rozdziale \ref{sec:ASR_NN}. Wszystkie wykorzystane w tym eksperymencie siec neuronowe miały taką samą architekturę. Składały się z jednej warstwy splotowej zbudowanej z $64$ filtrów, filtry były trzy kanałowe i miały wymiary $11\times6$, co było związane w szerokością kontekstu i odpowiada $kontekst \cdot 2 + 1\times6$. Następnie dodano jedną klasyczną warstwę ukrytą, składającą się z $2400$ neuronów oraz warstwę wyjściową, której szerokość była równa liczbie stanów danego modelu akustycznego. Warstwa środkowa miała dodatkowo dropout w współczynniku $0.5$. Funkcją aktywacji było \textit{rectify} opisane wzorem $\max(x,0)$, ostatnia warstwa miała funkcję \textit{softmax}, aby nadać wyjściu sieci charakter prawdopodobieństwa. Każda ze zbudowanych sieci neuronowych jest kompatybilna z jednym modelem klasycznym, zgodnie z opisem w rozdziale \ref{sec:nn_model_details}. Schemat \ref{fig:arch_q_num_impact} obrazuje architekturę sieci. Powstałe pary modeli akustycznych przebadano na testowym podzbiorze korpusu językowego \textit{Clarin}. Podczas procesu dekodowania wykorzystano wartości \textit{wagi} oraz \textit{kary} równe optymalnym parametrom z eksperymentu opisanego w rozdziale \ref{sec:metadata_impact}. Otrzymane wyniki zamieszczono w tabeli \ref{fig:tab_q_num_impact}. Wykres \ref{fig:fig_q_num_impact} obrazuje, w sposób graficzny, wpływ liczby stanów na skuteczność rozpoznawania dla modeli neuronowych oraz klasycznych.
		
		Przeprowadzony eksperyment pokazał, że rzeczywiście zwiększenie liczby wyjść sieci neuronowej pozytywnie wpływa na skuteczność rozpoznawania. Zaskakujące jednak okazało się, że dla modeli o ponad 231 neuronach obserwuje się gwałtowny spadek skuteczności. Niemniej jednak, model o 231 neuronach uzyskał  $83.48\%$ skuteczności, co daje $1.9\%$ punktu procentowego poprawy względem najlepszego standardowego modelu (\textit{M\_TRI}) i $0.05\%$ punktu procentowego poprawy względem modelu o zoptymalizowanych parametrach dekodowania (\textit{M\_TRI\_AOPT}). 
		
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}
			
			\begin{scope}
			
			\tikzstyle{conv_layer} = [fill=black,fill=gray!20,draw=black, text centered]
			
			\node at (0.5,2.4){$64$ filtry};
			\node at (3.4,2.9){warstwa pełna + dropout $0.5$};
			\node at (6.8,2.4){warstwa pełna + softmax};
			
			\def\w{1.5}		
			
			\def\x{0.0}	
			\def\y{0.5}				
			\pgfmathsetmacro{\X}{\x + \w};
			\pgfmathsetmacro{\Y}{\y + \w};
			\draw [fill=black,fill=gray!20,draw=black] (\x,\y) rectangle (\X,\Y);

			
			\def\x{0.25}	
			\def\y{0.25}				
			\pgfmathsetmacro{\X}{\x + \w};
			\pgfmathsetmacro{\Y}{\y + \w};
			\draw [fill=black,fill=gray!20,draw=black] (\x,\y) rectangle (\X,\Y);
			
			\def\x{0.5}	
			\def\y{0.0}				
			\pgfmathsetmacro{\X}{\x + \w};
			\pgfmathsetmacro{\Y}{\y + \w};
			\draw [fill=black,fill=gray!20,draw=black] (\x,\y) rectangle (\X,\Y) node [black,midway] {$11\times6$};
			
			
			\def\w{0.6}	
			\def\h{2.0}	
			\def\x{3.5}	
			\def\y{0.0}				
			\pgfmathsetmacro{\X}{\x + \w};
			\pgfmathsetmacro{\Y}{\y + \h};
			\draw [fill=black,fill=gray!20,draw=black] (\x,\y) rectangle (\X,\Y) node [black,midway, rotate=90] {$2400$};
			
			\draw (1.5,2.0) -- (3.5,1.0);
			\draw (2.0,0.0) -- (3.5,1.0);
			
			\def\w{0.6}	
			\def\h{1.5}	
			\def\x{5.5}	
			\def\y{0.25}				
			\pgfmathsetmacro{\X}{\x + \w};
			\pgfmathsetmacro{\Y}{\y + \h};
			\draw [fill=black,fill=gray!20,draw=black] (\x,\y) rectangle (\X,\Y) node [black,midway, rotate=90] {$|Q|$};
			
			\draw (4.2,2.0) -- (5.5,1.0);
			\draw (4.2,0.0) -- (5.5,1.0);
			
			\end{scope}
			
			\end{tikzpicture}
			\label{fig:arch_q_num_impact}
			\caption{Architektura sieci przy badaniu wpływu liczby stanów na skuteczność rozpoznawania mowy.}
		\end{figure}
		
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}			
			\begin{axis}[axis lines = left,	xlabel = Liczba stanów, ylabel = {1-WER [\%]},xmin=100,xmax=320,ymin=45,ymax=100,]
	
			\addplot[color=blue,mark=square,]
			coordinates {
				(122,81.55)(147,81.69)(214,83.34)(231,83.48)(269,72.07)(304, 57.42)
			};
			\addlegendentry{Modele neuronowe}
		
			\addplot[color=red,mark=square,]
			coordinates {
				(122,75.17)(147,75.91)(214,77.13)(231,77.23)(269,77.75)(304, 78.06)
			};
			\addlegendentry{Modele klasyczne}
					
			\end{axis}			
			\end{tikzpicture}
			\label{fig:fig_q_num_impact}
			\caption{Wpływ liczby stanów na skuteczność rozpoznawania.}
		\end{figure}
		
		
		
			
		\begin{figure}[H]
			\centering
			\begin{tabular}{|c|c|c|c|c|} \hline
				Liczba stanów & \vtop{\hbox{\strut Min liczba}\hbox{\strut obserwacji}} & Typ & \vtop{\hbox{\strut Skuteczność}\hbox{\strut modelu GMM}} & \vtop{\hbox{\strut Skuteczność}\hbox{\strut modelu CNN}} \\
				\hline
				120 & - &  UNI & 71.86 & 81.55 \\
				122 & 180000 & TRI & 75.17 & 81.69 \\				
				147 & 70000  & TRI & 75.91 & 83.24 \\
				214 & 37000  & TRI & 77.13 & 83.39 \\
				231 & 33000  & TRI & 77.23 & 83.48 \\
				269 & 29000  & TRI & 77.75 & 72.07 \\
				304 & 25000  & TRI & 78.06 & 57.42 \\
				\hline
				
			\end{tabular}
			\label{fig:tab_q_num_impact}
			\caption{Wpływ liczby stanów na skuteczność rozpoznawania mowy.}
	\end{figure}
		
	\section{ Wpływ głębokości/architektury sieci na skuteczność rozpoznawania }
		Celem tego eksperymentu jest sprawdzenie, jak architektura sieci, w tym liczba neuronów wpływa na skuteczność rozpoznawania. W pierwszej części eksperymentu zbudowano szereg modeli neuronowych bazujących na jednym modelu Gaussowskim o $231$ stanach. Modele miały taki sam kontekst równy $5$ oraz tę samą liczbę i kształt filtrów. Sieci miały taką sama architekturę jak w rozdziale \ref{sec:q-num_impact}. Jedynym parametrem, jaki uległ zmianie, była liczba neuronów w środkowej, pełnej warstwie. 		
		
		Otrzymane wyniki zamieszczono w tabeli \ref{tab:n_impact} i zilustrowano wykresem \ref{fig:n_impact}. Przeprowadzony test pokazał, że liczba neuronów w warstwie ukrytej ma wpływ na skuteczność. Optymalną wartością wydaje się $1200$ neuronów, dalsze rozszerzanie warstwy jedynie utrudnia trening sieci.
		
		
		\begin{figure}
			\centering
			\begin{tabular}{|c|c|c|c|c|} \hline
				\vtop{\hbox{\strut Liczba neuronów}\hbox{\strut w warstwie ukrytej}}  & 
				\vtop{\hbox{\strut Liczba }\hbox{\strut filtrów}} & \vtop{\hbox{\strut Skuteczność}\hbox{\strut corr}} & \vtop{\hbox{\strut Skuteczność}\hbox{\strut acc}} \\
				\hline
				2400 & 32 & 88.36 & 83.02 \\
				1200 & 32 & 88.54 & 83.10 \\
				600 & 32 & 87.52 & 82.01 \\
				300 & 32 & 86.90 & 81.36 \\
				\hline
				
			\end{tabular}
			\caption{\label{tab:n_impact}Wpływ liczby neuronów na skuteczność rozpoznawania.}
		\end{figure}
	
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}			
			\begin{axis}[axis lines = left,	xlabel = {Liczba neuronów} , ylabel = {1-WER [\%]},xmin=0,xmax=2500,ymin=80,ymax=85,]
			
			\addplot[color=red,mark=square,]
			coordinates {
				(300,81.36)(600,82.01)(1200,83.10)(2400, 83.02)
			};
			
			\end{axis}			
			\end{tikzpicture}
			\label{fig:n_impact}
			\caption{Wpływ liczby neuronów warstwy ukrytej na skuteczność rozpoznawania.}
		\end{figure}
	
	W drugiej części zbudowano dwie kolejne sieci o innej architekturze. Pierwsza z nich była uproszczona, nie zawierała ukrytej warstwy pełnej, miała 24 filtry. Druga sieć miła dodatkową warstwę splotową, obie warstwy splotowe miały po 24 filtry, warstwa pełna miała 600 neuronów. Zmniejszenie szerokości warstwo podyktowane było czasem treningu. Schemat \ref{fig:arch_exp} przedstawia obie architektury.
	W przeprowadzonym eksperymencie uproszczona siec uzyskała $77.38 \%$ skuteczności, natomiast rozbudowana $82.35 \%$. Oba wyniki są gorsze od najlepszej wartości z pierwszego eksperymentu, wynoszącej $83.10\%$, co sugeruje że przyjęta w poprzednich eksperymentach architektura ze schematu \ref{fig:arch_q_num_impact}, wydaje się być optymalną. Sieci o mniejszej liczbie warstw nie dysponują wystarczająca pojemnością wiedzy, a modele bardziej rozbudowane są zbyt trudne do wyuczenia. 
	
		\begin{figure}[H]
		\centering
		\begin{tikzpicture}
		
		\begin{scope}
		
		\tikzstyle{conv_layer} = [fill=black,fill=gray!20,draw=black, text centered]

		//------------------------ siec 1 -----------------------------------
				
		\node at (-3.75,2.4){$24$ filtry};
		
		\def\w{1.2}		
		
		\def\x{-4.5}	
		\def\y{0.5}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \w};
		\draw [fill=black,fill=gray!20,draw=black] (\x,\y) rectangle (\X,\Y);
		
		
		\def\x{-4.25}	
		\def\y{0.25}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \w};
		\draw [fill=black,fill=gray!20,draw=black] (\x,\y) rectangle (\X,\Y);
		
		\def\x{-4.0}	
		\def\y{0.0}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \w};
		\draw [fill=black,fill=gray!20,draw=black] (\x,\y) rectangle (\X,\Y) node [black,midway] {$11\times6$};
		
		\def\w{0.6}	
		\def\h{1.2}	
		\def\x{-1.75}	
		\def\y{0.25}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \h};
		\draw [fill=black,fill=gray!20,draw=black] (\x,\y) rectangle (\X,\Y) node [black,midway, rotate=90] {$231$};
		
		\draw (-3.3,1.7) -- (-1.75,1.0);
		\draw (-2.8,0.0) -- (-1.75,1.0);
		
		//------------------------ siec 2 -----------------------------------
		//splot 1
		\def\w{1.2}	
		
		\node at (0.5,2.4){$24$ filtry};
		
		\def\x{0.0}	
		\def\y{0.5}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \w};
		\draw [fill=black,fill=gray!20,draw=black] (\x,\y) rectangle (\X,\Y);
		
		
		\def\x{0.25}	
		\def\y{0.25}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \w};
		\draw [fill=black,fill=gray!20,draw=black] (\x,\y) rectangle (\X,\Y);
		
		\def\x{0.5}	
		\def\y{0.0}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \w};
		\draw [fill=black,fill=gray!20,draw=black] (\x,\y) rectangle (\X,\Y) node [black,midway] {$6\times5$};
		
		//splot2
		\node at (2.5,2.4){$24$ filtry};
		
		\draw (1.2,1.7) -- (2.25,1.0);
		\draw (1.7,0.0) -- (2.25,1.0);
		
		\def\x{2.25}	
		\def\y{0.5}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \w};
		\draw [fill=black,fill=gray!20,draw=black] (\x,\y) rectangle (\X,\Y);
		
		
		\def\x{2.5}	
		\def\y{0.25}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \w};
		\draw [fill=black,fill=gray!20,draw=black] (\x,\y) rectangle (\X,\Y);
		
		\def\x{2.75}	
		\def\y{0.0}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \w};
		\draw [fill=black,fill=gray!20,draw=black] (\x,\y) rectangle (\X,\Y) node [black,midway] {$6\times5$};
		
		//pelna srodkowa	
		
		\draw (3.45,1.7) -- (5.0,1.0);
		\draw (3.95,0.0) -- (5.0,1.0);
		
		\def\w{0.6}	
		\def\h{1.7}	
		\def\x{5.0}	
		\def\y{0.0}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \h};
		\draw [fill=black,fill=gray!20,draw=black] (\x,\y) rectangle (\X,\Y) node [black,midway, rotate=90] {$600$};

		
		\def\w{0.6}	
		\def\h{1.2}	
		\def\x{6.5}	
		\def\y{0.25}				
		\pgfmathsetmacro{\X}{\x + \w};
		\pgfmathsetmacro{\Y}{\y + \h};
		\draw [fill=black,fill=gray!20,draw=black] (\x,\y) rectangle (\X,\Y) node [black,midway, rotate=90] {$231$};
		
		\draw (5.6,1.7) -- (6.5,1.0);
		\draw (5.6,0.0) -- (6.5,1.0);
		
		\end{scope}
		
		\end{tikzpicture}
		\label{fig:arch_exp}
		\caption{Architektura sieci przy badaniu wpływu architektury na skuteczność rozpoznawania mowy.}
	\end{figure}
	
	
	\section{ Wpływ szerokości kontekstu na skuteczność rozpoznawania }
		Typowa szybkość mówienia wynosi $12-14$ głosek na sekundę, w konsekwencji przeciętna głoska zajmuje $7-8$ ramek. W związku z tym, uzasadnione wydaje się przekazywanie do wektora cech wielu kolejnych ramek. W tym eksperymencie sprawdzono wpływ liczby ramek, z których budowany jest wektor cech na skuteczność rozpoznawania mowy z wykorzystaniem sieci neuronowych. Architektura wykorzystanej siec była podobna do architektury z eksperyment opisanego w rozdziale \ref{sec:q-num_impact}. Jednak teraz liczba neuronów w warstwie wyjściowej była ustalona, gdyż bazowano na jednym modelu akustycznym o $231$ stanach. Liczba filtrów pozostała taka sama i wyniosła $64$. Zmieniał się natomiast kształt filtrów, który wynosił $kontekst \cdot 2 + 1 \times 6$. Ponadto zamiast funkcji nieliniowości \textit{rectify} wykorzystano funkcję \textit{leaky rectify} opisana wzorem $\max(x, 0.01x)$. Wykres \ref{fig:fig_ctx_impact} przedstawia wyniki przeprowadzonych eksperymentów. 
		
		Na podstawie otrzymanych wyników ciężko jednoznacznie określić wpływ szerokości kontekstu na skuteczność rozpoznawania. Najlepszy wynik otrzymano dla kontekstu $5$, co odpowiada $11$ ramkom przekazywanym do wektora cech.
		
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}			
			\begin{axis}[axis lines = left,	xlabel = Szerokość kontekstu, ylabel = {1-WER [\%]},xmin=0,xmax=10,ymin=82.7,ymax=84.5,]
			
			\addplot[color=blue,mark=square,]
			coordinates {
				(1,83.83)(3,83.64)(5,83.89 )(7,83.57 )(9, 83.57)
			};
			\addlegendentry{Modele neuronowe}		
			
			\end{axis}			
			\end{tikzpicture}
			\label{fig:fig_ctx_impact}
			\caption{Wpływ szerokości kontekstu na skuteczność rozpoznawania mowy.}
		\end{figure}

	\section{ Inne czynniki wpływające na skuteczność rozpoznawania }
		\label{sec:impact_other}
		Oprócz parametrów wspomnianych we wcześniejszych rozdziałach, sprawdzono jaki wpływ na skuteczność rozpoznawania ma funkcja aktywacji oraz pomieszanie danych uczących. 
		
		W eksperymencie opisanym w rozdziale \ref{sec:q-num_impact} wykorzystano funkcję aktywacji opisaną wzorem $\max(x,0)$. Celem tego eksperymentu jest sprawdzenie czy funkcja \textit{leaky rectify} opisana wzorem $\max(x, 0.01x)$ umożliwia wytrenowanie skuteczniejszej sieci. W założeniach czynniki $0.01x$ daje sieci informację z neuronów o negatywnym pobudzeniu. Jest to różnica w stosunku do funkcji \textit{rectify}, która zwraca wtedy zawsze $0$ i nie propaguje żadnej informacji. 
		
		W wcześniejszych eksperymentach dane treningowe były ułożone w uporządkowany sposób. Wektory cech pochodzące od jednego mówcy następowały po sobie. Podobnie, wektory dla kolejnych ramek pochodzących z jednej wypowiedzi, też  ułożone było kolejno po sobie. Celem eksperymentu jest sprawdzenie czy po przemieszaniu wektorów cech można uzyskać lepszą skuteczność. Efektem przemieszania, kolejne, podobne wektory nie są razem pokazywane sieci podczas treningu.
		
		Oba czynniki testowane były przy takich samych architekturach, ale innych liczbach neuronów. Tabele \ref{fig:tab_other_impact} pokazuje wyniki eksperymentu. Okazało się, że zarówno stosowanie funkcji \textit{leaky rectify}, jak i mieszanie danych treningowych poprawia skuteczność. Funkcja \textit{leaky rectify} dała względną poprawę o $2.97\%$, natomiast mieszanie danych dało $3.2\%$ poprawy. Na skutek pozytywnych wyników eksperymentu, zbudowano jeszcze jedną sieć, łączącą w sobie oba badane elementy. Powstały model agreguje wszystkie obserwacji uzyskane z przeprowadzonych testach i stanowi najlepsza uzyskaną konfigurację. Ma $64$ filtry, $2400$ neuronów w warstwie ukrytej, funkcję aktywacji \textit{leaky rectify} oraz architekturę z rysunku \ref{fig:arch_q_num_impact}. Sieć uzyskała $84.71\%$ skuteczności, co stanowi najlepszy wynik spośród wszystkich modeli, zarówno neuronowych, jak i Gaussowkich. Jest to wynik lepszy o $ 1.27$ punkt procentowego ($7.67\%$ względnej poprawy) względem najlepszego modelu klasycznego \textit{M\_TRI\_AOPT}.
		\begin{figure}[H]
			\centering
			\begin{tabular}{|l|c|c|c|c|} \hline
				\vtop{\hbox{\strut Funkcja}\hbox{\strut nieliniowości}} & \vtop{\hbox{\strut Dane}\hbox{\strut przemieszane}} & \vtop{\hbox{\strut Liczba}\hbox{\strut filtrów}} & \vtop{\hbox{\strut Liczba}\hbox{\strut neuronów}} & Skuteczność \\
				\hline
				recify        &  nie & 64 & 2400 & 83.50  \\
				leaky rectify &  nie & 64 & 2400 & 83.89 \\
				\hline
				leaky rectify &  nie & 32 & 1200 & 83.10 \\
				leaky rectify &  tak & 32 & 1200 & 83.64\\
				\hline
			    leaky rectify &  tak & 64 & 2400 & 84.71\\
				\hline				
			\end{tabular}
			\label{fig:tab_other_impact}
			\caption{Wpływ liczby stanów na skuteczność rozpoznawania mowy.}
		\end{figure}
	
\section{ Podsumowanie }
	Na podstawie przeprowadzonych eksperymentów, można jednoznacznie potwierdzić tezę niniejszej pracy magisterskiej. \textbf{Wykorzystując splotowe sieci neuronowe rozpoznające \textit{tri-fony}, jako estymator prawdopodobieństw stanów modelu Markowa, można uzyskać poprawę skuteczności, względem analogicznego podejścia wykorzystującego \textit{uni-fony}.} W eksperymencie \ref{sec:q-num_impact} uzyskano $1.93\%$ punktu procentowego poprawy względem sieci neuronowej uni-fonowej, co odpowiada $10.46\%$ względnej poprawy. Ponadto, w eksperymencie \ref{sec:impact_other} udało się zbudować sieć uzyskującą $1.27$ punktu procentowego poprawy ($7.67\%$ względnej poprawy) względem najlepszego klasycznego modelu \textit{M\_TRI\_AOPT}. Nieoczekiwany okazał się gwałtowny spadek skuteczności dla modeli neuronowych o więcej niż $231$ stanach. Efekt ten można uzasadnić zbyt duża liczbą neuronów w warstwie wyjściowej, reagujących na bardzo podobne wektory wejściowe, czego konsekwencją jest nieumiejętność rozróżnienia przez sieć, podobnych tri-fonów. W przeprowadzonych eksperymentach nie uzyskano poprawy, przy zwiększaniu liczby warstw względem proponowanej architektury ze schematu \ref{fig:arch_q_num_impact}.
	
	Wyniki przeprowadzonych eksperymentów dały pozytywne rezultaty zachęcająca do dalszych badań. W przyszłości warto by było skupić się na budowie sieci uczonej na znacznie większym korpusie. Ponadto mają więcej danych uczących można powtórzyć próbę budowy sieci o głębszej architekturze lub sieci o większej liczbie wyjść.
   
   
	\nocite{*}
	\bibliographystyle{plain}
	\bibliography{bibliografia}
\end{document}